{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saving_model_for_android_app-victor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8STnhomg3TuJ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSMe4hxv0dbo",
        "colab_type": "code",
        "outputId": "108711be-c96d-465e-a7ca-6dd8b0ab7985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.manual_seed(10)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import transforms, datasets\n",
        "import pickle\n",
        "import time\n",
        "from google.colab import drive\n",
        "from torchsummary import summary\n",
        "drive.mount('/content/drive')\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import dill as dill"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting torch_optimizer\n",
            "  Downloading https://files.pythonhosted.org/packages/33/d3/4ff0ce01ccbedf3a3c86e7882f428097a37a9b9a55eca73548e657da5518/torch_optimizer-0.0.1a12-py3-none-any.whl\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEz-jr7DsEPX",
        "colab_type": "text"
      },
      "source": [
        "# Mean - std calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KwnhkFf4Dqn",
        "colab_type": "code",
        "outputId": "4256ad20-6927-4d8e-fbad-bcf82b20e587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(128),\n",
        "    #transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "Train_neu_dataset = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/train',transform=data_transform)\n",
        "Train_dataset_loader = torch.utils.data.DataLoader(Train_neu_dataset,batch_size=20,num_workers=4,shuffle=False)#shuffle=True for training add\n",
        "\n",
        "\n",
        "mean = 0.\n",
        "meansq = 0.\n",
        "for data,labels in Train_dataset_loader:\n",
        "    mean = data.mean()\n",
        "    meansq = (data**2).mean()\n",
        "\n",
        "std = torch.sqrt(meansq - mean**2)\n",
        "print(\"mean: \" + str(mean))\n",
        "print(\"std: \" + str(std))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: tensor(0.6191)\n",
            "std: tensor(0.1090)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulep8e6A5uZG",
        "colab_type": "code",
        "outputId": "4099f33e-b0f6-4119-9944-d90d0de439f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mean = 0.0\n",
        "meansq = 0.0\n",
        "count = 0\n",
        "\n",
        "for index, data_set in enumerate(Train_dataset_loader):\n",
        "    data,labels = data_set\n",
        "    mean = data.sum()\n",
        "    meansq = meansq + (data**2).sum()\n",
        "    count += np.prod(data.shape)\n",
        "\n",
        "total_mean = mean/count\n",
        "total_var = (meansq/count) - (total_mean**2)\n",
        "total_std = torch.sqrt(total_var)\n",
        "print(\"mean: \" + str(total_mean))\n",
        "print(\"std: \" + str(total_std))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: tensor(0.0046)\n",
            "std: tensor(0.5687)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOmYIfxk6Bup",
        "colab_type": "code",
        "outputId": "20f2fa41-c0f9-4ecc-c58f-504ca94d6334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mean = 0.0\n",
        "for images, _ in Train_dataset_loader:\n",
        "    batch_samples = images.size(0) \n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    mean += images.mean(2).sum(0)\n",
        "mean = mean / len(Train_dataset_loader.dataset)\n",
        "print('mean for the dataset',mean)\n",
        "var = 0.0\n",
        "for images, _ in Train_dataset_loader:\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
        "std = torch.sqrt(var / (len(Train_dataset_loader.dataset)*128*128))\n",
        "print('standard deviation for the dataset',std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean for the dataset tensor([0.5241, 0.4949, 0.5741])\n",
            "standard deviation for the dataset tensor([0.1864, 0.2078, 0.2274])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rxPqN6VJjlR",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErX713gisDi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "\n",
        "def visualize(Train_dataset_loader,classes):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  # functions to show an image\n",
        "  def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "  # get some random training images\n",
        "  dataiter = iter(Train_dataset_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  # show images\n",
        "  imshow(torchvision.utils.make_grid(images))\n",
        "  print(' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
        "\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    #optimizer = optim.RAdam(net.parameters(), lr=learning_rate)# with RAdam\n",
        "    return(loss, optimizer)\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "\n",
        "    data_transform_train = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomAffine(3, translate=(0.1,0.9), scale= None, shear=None,resample=False, fillcolor=0),\n",
        "        transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.02, hue=0.01),\n",
        "        transforms.RandomPerspective(distortion_scale=0.01, p=0.5, interpolation=3, fill=0),\n",
        "        #transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265]),\n",
        "        AddGaussianNoise(0.1, 0.2)\n",
        "    ])\n",
        "    Train_neu_dataset = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/train',transform=data_transform_train)\n",
        "    Train_dataset_loader = torch.utils.data.DataLoader(Train_neu_dataset,batch_size=batch_size,num_workers=4,shuffle=True)#shuffle=True for training add\n",
        "\n",
        "    data_transform_valid = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265])\n",
        "    ])\n",
        "\n",
        "\n",
        "    valid_neu_dataset = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/val',transform=data_transform_valid)\n",
        "    valid_dataset_loader = torch.utils.data.DataLoader(valid_neu_dataset,batch_size=5, shuffle=True,num_workers=4)\n",
        "\n",
        "\n",
        "    classes = ('im_Dyskeratotic','im_Koilocytotic','im_Metaplastic','im_Parabasal','im_Superficial-Intermediate')\n",
        "\n",
        "    visualize(Train_dataset_loader,classes)\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    n_batches = len(Train_dataset_loader)\n",
        "    \n",
        "    #Create our loss and optimizer functions\n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "   \n",
        "    #Time for printing\n",
        "    training_start_time = time.time()\n",
        "    train_losses, valid_losses = [], []\n",
        "    #Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        print_every = n_batches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        accuracy = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(Train_dataset_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss_size.item()\n",
        "            total_train_loss += loss_size.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:\n",
        "                \n",
        "                print(\"Epoch {}, Batch_no {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, i ,int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                running_loss = 0\n",
        "                start_time = time.time()\n",
        "        print('Training accuracy: %d %%' % (100 * accuracy / total))\n",
        "        scheduler.step()    \n",
        "        #At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        valid_accuracy = 0\n",
        "        total = 0\n",
        "        net = net.eval()\n",
        "        for inputs, labels in valid_dataset_loader:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.item()\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            valid_accuracy += (predicted == labels).sum().item()\n",
        "        train_losses.append(total_train_loss/len(Train_dataset_loader))\n",
        "        valid_losses.append(total_test_loss/len(valid_dataset_loader))\n",
        "        print('Validation accuracy: %d %%' % (100 * valid_accuracy / total))\n",
        "        print(\"Validation loss = {:.2f}\".format(total_test_loss / len(valid_dataset_loader)))\n",
        "        net = net.train()\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        #transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265])\n",
        "    ])\n",
        "    Test_neu_dataset = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/test',transform=data_transform)\n",
        "    Test_dataset_loader = torch.utils.data.DataLoader(Test_neu_dataset,batch_size=10, shuffle=True,num_workers=4)\n",
        "\n",
        "    confusion_matrix = torch.zeros(5, 5) # no_classes*no_classes\n",
        "\n",
        "    def test_label_predictions(net, device, Test_dataset_loader):\n",
        "      net.eval()\n",
        "      net = net.to(\"cpu\")\n",
        "      # @victor: torch.save and save mehtod to pth file\n",
        "      torch.save(net.state_dict(), './drive/My Drive/Papsmear_dataset/testnet_normalized.pth')\n",
        "      \n",
        "      net = net.to(device, )\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for data, classes in Test_dataset_loader:\n",
        "          data, classes = data.to(device), classes.to(device)\n",
        "          output = net(data)\n",
        "          _, preds = torch.max(output, 1)\n",
        "          total += classes.size(0)\n",
        "          correct += (preds == classes).sum().item()\n",
        "          for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "            confusion_matrix[t.long(), p.long()] += 1\n",
        "      print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
        "      return confusion_matrix,correct,total\n",
        "    # net = net.eval()\n",
        "    confusion_matrix,correct, total = test_label_predictions(net, device, Test_dataset_loader)\n",
        "    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
        "    print('Confusion matrix:')\n",
        "    print(confusion_matrix)\n",
        "    \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    return train_losses,valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5KQKCaLb1yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "class simple_testnet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    self.num_classes = num_classes\n",
        "    super(simple_testnet, self).__init__()\n",
        "\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer: conv1\n",
        "            nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1), #starting size:224*224*3\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer: conv2\n",
        "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1), #after conv1, size:112*112*4\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer: conv3\n",
        "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1), #after conv2, size:56*56*4\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer: conv4\n",
        "            nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1), #after conv3, size:28*28*4\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer: conv5\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1), #after conv4, size:14*14*8\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #output after conv5, size:7*7*16\n",
        "        )\n",
        "    \n",
        "    self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=16 * 7 * 7, out_features=8*7*7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(8 * 7 * 7, self.num_classes)\n",
        "        )\n",
        "    \n",
        "    # Defining the forward pass    \n",
        "  def forward(self, x):\n",
        "      x = self.cnn_layers(x)\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.linear_layers(x)\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjyShFdq_u8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Creates a MobileNetV3 Model as defined in:\n",
        "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
        "Searching for MobileNetV3\n",
        "arXiv preprint arXiv:1905.02244.\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
        "                h_sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        h_swish()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        h_swish()\n",
        "    )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "\n",
        "        if inp == hidden_dim:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
        "                # Squeeze-and-Excite\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # Squeeze-and-Excite\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV3(nn.Module):\n",
        "    def __init__(self, cfgs, mode, num_classes=5, width_mult=1.):\n",
        "        super(MobileNetV3, self).__init__()\n",
        "        # setting of inverted residual blocks\n",
        "        self.cfgs = cfgs\n",
        "        assert mode in ['large', 'small']\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(16 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = InvertedResidual\n",
        "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            exp_size = _make_divisible(input_channel * t, 8)\n",
        "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
        "            input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        output_channel = {'large': 1280, 'small': 1024}\n",
        "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(exp_size, output_channel),\n",
        "            h_swish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(output_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def mobilenetv3_large(**kwargs):\n",
        "        \"\"\"\n",
        "        Constructs a MobileNetV3-Large model\n",
        "        \"\"\"\n",
        "        cfgs = [\n",
        "            # k, t, c, SE, HS, s \n",
        "            [3,   1,  16, 0, 0, 1],\n",
        "            [3,   4,  24, 0, 0, 2],\n",
        "            [3,   3,  24, 0, 0, 1],\n",
        "            [5,   3,  40, 1, 0, 2],\n",
        "            [5,   3,  40, 1, 0, 1],\n",
        "            [5,   3,  40, 1, 0, 1],\n",
        "            [3,   6,  80, 0, 1, 2],\n",
        "            [3, 2.5,  80, 0, 1, 1],\n",
        "            [3, 2.3,  80, 0, 1, 1],\n",
        "            [3, 2.3,  80, 0, 1, 1],\n",
        "            [3,   6, 112, 1, 1, 1],\n",
        "            [3,   6, 112, 1, 1, 1],\n",
        "            [5,   6, 160, 1, 1, 2],\n",
        "            [5,   6, 160, 1, 1, 1],\n",
        "            [5,   6, 160, 1, 1, 1]\n",
        "        ]\n",
        "        return MobileNetV3(cfgs, mode='large', **kwargs)\n",
        "\n",
        "\n",
        "    def mobilenetv3_small(**kwargs):\n",
        "        \"\"\"\n",
        "        Constructs a MobileNetV3-Small model\n",
        "        \"\"\"\n",
        "        cfgs = [\n",
        "            # k, t, c, SE, HS, s \n",
        "            [3,    1,  16, 1, 0, 2],\n",
        "            [3,  4.5,  24, 0, 0, 2],\n",
        "            [3, 3.67,  24, 0, 0, 1],\n",
        "            [5,    4,  40, 1, 1, 2],\n",
        "            [5,    6,  40, 1, 1, 1],\n",
        "            [5,    6,  40, 1, 1, 1],\n",
        "            [5,    3,  48, 1, 1, 1],\n",
        "            [5,    3,  48, 1, 1, 1],\n",
        "            [5,    6,  96, 1, 1, 2],\n",
        "            [5,    6,  96, 1, 1, 1],\n",
        "            [5,    6,  96, 1, 1, 1],\n",
        "        ]\n",
        "\n",
        "        return MobileNetV3(cfgs, mode='small', **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PEr-jN1CZWZ",
        "colab_type": "text"
      },
      "source": [
        "Training cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yqNMl1nBhCo",
        "colab_type": "code",
        "outputId": "07da048d-5136-4763-f7d7-c99d42055d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#network = MobileNetV3.mobilenetv3_large()        \n",
        "network = simple_testnet(num_classes=5)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "network.to(device)\n",
        "train_list,test_list = trainNet(network, batch_size=16, n_epochs=210, learning_rate=0.0009862520003992714)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "im_Parabasal im_Koilocytotic im_Parabasal im_Parabasal im_Parabasal\n",
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 16\n",
            "epochs= 210\n",
            "learning_rate= 0.0009862520003992715\n",
            "==============================\n",
            "Epoch 1, Batch_no 12, 10% \t train_loss: 1.86 took: 3.01s\n",
            "Epoch 1, Batch_no 25, 20% \t train_loss: 1.71 took: 2.52s\n",
            "Epoch 1, Batch_no 38, 30% \t train_loss: 1.61 took: 2.18s\n",
            "Epoch 1, Batch_no 51, 41% \t train_loss: 1.60 took: 2.45s\n",
            "Epoch 1, Batch_no 64, 51% \t train_loss: 1.64 took: 2.45s\n",
            "Epoch 1, Batch_no 77, 61% \t train_loss: 1.52 took: 3.13s\n",
            "Epoch 1, Batch_no 90, 72% \t train_loss: 1.35 took: 2.19s\n",
            "Epoch 1, Batch_no 103, 82% \t train_loss: 1.49 took: 1.99s\n",
            "Epoch 1, Batch_no 116, 92% \t train_loss: 1.43 took: 2.34s\n",
            "Training accuracy: 35 %\n",
            "Validation accuracy: 48 %\n",
            "Validation loss = 1.33\n",
            "Epoch 2, Batch_no 12, 10% \t train_loss: 1.58 took: 3.05s\n",
            "Epoch 2, Batch_no 25, 20% \t train_loss: 1.47 took: 2.58s\n",
            "Epoch 2, Batch_no 38, 30% \t train_loss: 1.34 took: 2.65s\n",
            "Epoch 2, Batch_no 51, 41% \t train_loss: 1.40 took: 2.64s\n",
            "Epoch 2, Batch_no 64, 51% \t train_loss: 1.31 took: 2.48s\n",
            "Epoch 2, Batch_no 77, 61% \t train_loss: 1.28 took: 2.25s\n",
            "Epoch 2, Batch_no 90, 72% \t train_loss: 1.34 took: 2.24s\n",
            "Epoch 2, Batch_no 103, 82% \t train_loss: 1.31 took: 2.73s\n",
            "Epoch 2, Batch_no 116, 92% \t train_loss: 1.29 took: 2.97s\n",
            "Training accuracy: 46 %\n",
            "Validation accuracy: 53 %\n",
            "Validation loss = 1.26\n",
            "Epoch 3, Batch_no 12, 10% \t train_loss: 1.36 took: 3.11s\n",
            "Epoch 3, Batch_no 25, 20% \t train_loss: 1.20 took: 2.41s\n",
            "Epoch 3, Batch_no 38, 30% \t train_loss: 1.35 took: 2.61s\n",
            "Epoch 3, Batch_no 51, 41% \t train_loss: 1.24 took: 2.27s\n",
            "Epoch 3, Batch_no 64, 51% \t train_loss: 1.21 took: 2.84s\n",
            "Epoch 3, Batch_no 77, 61% \t train_loss: 1.28 took: 2.14s\n",
            "Epoch 3, Batch_no 90, 72% \t train_loss: 1.22 took: 2.76s\n",
            "Epoch 3, Batch_no 103, 82% \t train_loss: 1.18 took: 2.43s\n",
            "Epoch 3, Batch_no 116, 92% \t train_loss: 1.16 took: 2.44s\n",
            "Training accuracy: 52 %\n",
            "Validation accuracy: 54 %\n",
            "Validation loss = 1.25\n",
            "Epoch 4, Batch_no 12, 10% \t train_loss: 1.10 took: 3.17s\n",
            "Epoch 4, Batch_no 25, 20% \t train_loss: 1.16 took: 2.35s\n",
            "Epoch 4, Batch_no 38, 30% \t train_loss: 1.09 took: 2.14s\n",
            "Epoch 4, Batch_no 51, 41% \t train_loss: 1.25 took: 2.52s\n",
            "Epoch 4, Batch_no 64, 51% \t train_loss: 1.14 took: 2.14s\n",
            "Epoch 4, Batch_no 77, 61% \t train_loss: 1.25 took: 2.78s\n",
            "Epoch 4, Batch_no 90, 72% \t train_loss: 1.32 took: 3.33s\n",
            "Epoch 4, Batch_no 103, 82% \t train_loss: 1.31 took: 2.36s\n",
            "Epoch 4, Batch_no 116, 92% \t train_loss: 1.23 took: 2.20s\n",
            "Training accuracy: 53 %\n",
            "Validation accuracy: 53 %\n",
            "Validation loss = 1.37\n",
            "Epoch 5, Batch_no 12, 10% \t train_loss: 1.13 took: 3.06s\n",
            "Epoch 5, Batch_no 25, 20% \t train_loss: 1.21 took: 2.84s\n",
            "Epoch 5, Batch_no 38, 30% \t train_loss: 1.08 took: 2.62s\n",
            "Epoch 5, Batch_no 51, 41% \t train_loss: 1.05 took: 2.46s\n",
            "Epoch 5, Batch_no 64, 51% \t train_loss: 1.20 took: 2.95s\n",
            "Epoch 5, Batch_no 77, 61% \t train_loss: 1.19 took: 2.30s\n",
            "Epoch 5, Batch_no 90, 72% \t train_loss: 1.11 took: 2.15s\n",
            "Epoch 5, Batch_no 103, 82% \t train_loss: 1.09 took: 2.20s\n",
            "Epoch 5, Batch_no 116, 92% \t train_loss: 1.20 took: 2.84s\n",
            "Training accuracy: 56 %\n",
            "Validation accuracy: 57 %\n",
            "Validation loss = 1.28\n",
            "Epoch 6, Batch_no 12, 10% \t train_loss: 1.08 took: 3.14s\n",
            "Epoch 6, Batch_no 25, 20% \t train_loss: 1.02 took: 2.48s\n",
            "Epoch 6, Batch_no 38, 30% \t train_loss: 1.07 took: 2.96s\n",
            "Epoch 6, Batch_no 51, 41% \t train_loss: 1.16 took: 2.46s\n",
            "Epoch 6, Batch_no 64, 51% \t train_loss: 1.14 took: 2.20s\n",
            "Epoch 6, Batch_no 77, 61% \t train_loss: 1.09 took: 3.11s\n",
            "Epoch 6, Batch_no 90, 72% \t train_loss: 1.08 took: 1.96s\n",
            "Epoch 6, Batch_no 103, 82% \t train_loss: 1.15 took: 2.47s\n",
            "Epoch 6, Batch_no 116, 92% \t train_loss: 1.23 took: 2.69s\n",
            "Training accuracy: 58 %\n",
            "Validation accuracy: 57 %\n",
            "Validation loss = 1.24\n",
            "Epoch 7, Batch_no 12, 10% \t train_loss: 1.18 took: 3.49s\n",
            "Epoch 7, Batch_no 25, 20% \t train_loss: 1.16 took: 2.58s\n",
            "Epoch 7, Batch_no 38, 30% \t train_loss: 1.05 took: 2.38s\n",
            "Epoch 7, Batch_no 51, 41% \t train_loss: 1.09 took: 2.45s\n",
            "Epoch 7, Batch_no 64, 51% \t train_loss: 1.06 took: 2.81s\n",
            "Epoch 7, Batch_no 77, 61% \t train_loss: 1.09 took: 2.29s\n",
            "Epoch 7, Batch_no 90, 72% \t train_loss: 1.08 took: 2.36s\n",
            "Epoch 7, Batch_no 103, 82% \t train_loss: 1.13 took: 2.37s\n",
            "Epoch 7, Batch_no 116, 92% \t train_loss: 1.21 took: 2.90s\n",
            "Training accuracy: 57 %\n",
            "Validation accuracy: 59 %\n",
            "Validation loss = 1.08\n",
            "Epoch 8, Batch_no 12, 10% \t train_loss: 1.12 took: 3.23s\n",
            "Epoch 8, Batch_no 25, 20% \t train_loss: 1.02 took: 2.71s\n",
            "Epoch 8, Batch_no 38, 30% \t train_loss: 1.12 took: 2.29s\n",
            "Epoch 8, Batch_no 51, 41% \t train_loss: 1.13 took: 2.43s\n",
            "Epoch 8, Batch_no 64, 51% \t train_loss: 1.08 took: 2.32s\n",
            "Epoch 8, Batch_no 77, 61% \t train_loss: 1.02 took: 2.79s\n",
            "Epoch 8, Batch_no 90, 72% \t train_loss: 1.10 took: 2.34s\n",
            "Epoch 8, Batch_no 103, 82% \t train_loss: 0.99 took: 2.47s\n",
            "Epoch 8, Batch_no 116, 92% \t train_loss: 1.16 took: 2.17s\n",
            "Training accuracy: 57 %\n",
            "Validation accuracy: 50 %\n",
            "Validation loss = 1.53\n",
            "Epoch 9, Batch_no 12, 10% \t train_loss: 1.12 took: 3.55s\n",
            "Epoch 9, Batch_no 25, 20% \t train_loss: 1.03 took: 2.51s\n",
            "Epoch 9, Batch_no 38, 30% \t train_loss: 1.02 took: 2.00s\n",
            "Epoch 9, Batch_no 51, 41% \t train_loss: 1.06 took: 2.58s\n",
            "Epoch 9, Batch_no 64, 51% \t train_loss: 1.04 took: 2.81s\n",
            "Epoch 9, Batch_no 77, 61% \t train_loss: 0.99 took: 2.13s\n",
            "Epoch 9, Batch_no 90, 72% \t train_loss: 1.10 took: 2.52s\n",
            "Epoch 9, Batch_no 103, 82% \t train_loss: 1.06 took: 2.47s\n",
            "Epoch 9, Batch_no 116, 92% \t train_loss: 0.94 took: 2.93s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 57 %\n",
            "Validation loss = 1.19\n",
            "Epoch 10, Batch_no 12, 10% \t train_loss: 0.95 took: 3.62s\n",
            "Epoch 10, Batch_no 25, 20% \t train_loss: 1.04 took: 2.50s\n",
            "Epoch 10, Batch_no 38, 30% \t train_loss: 0.99 took: 2.54s\n",
            "Epoch 10, Batch_no 51, 41% \t train_loss: 1.04 took: 2.29s\n",
            "Epoch 10, Batch_no 64, 51% \t train_loss: 0.97 took: 2.90s\n",
            "Epoch 10, Batch_no 77, 61% \t train_loss: 1.13 took: 2.28s\n",
            "Epoch 10, Batch_no 90, 72% \t train_loss: 0.93 took: 2.52s\n",
            "Epoch 10, Batch_no 103, 82% \t train_loss: 1.13 took: 2.44s\n",
            "Epoch 10, Batch_no 116, 92% \t train_loss: 1.08 took: 2.45s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 54 %\n",
            "Validation loss = 1.31\n",
            "Epoch 11, Batch_no 12, 10% \t train_loss: 1.12 took: 3.56s\n",
            "Epoch 11, Batch_no 25, 20% \t train_loss: 1.05 took: 2.78s\n",
            "Epoch 11, Batch_no 38, 30% \t train_loss: 0.99 took: 2.36s\n",
            "Epoch 11, Batch_no 51, 41% \t train_loss: 1.03 took: 2.45s\n",
            "Epoch 11, Batch_no 64, 51% \t train_loss: 0.97 took: 2.55s\n",
            "Epoch 11, Batch_no 77, 61% \t train_loss: 1.03 took: 2.97s\n",
            "Epoch 11, Batch_no 90, 72% \t train_loss: 1.06 took: 2.45s\n",
            "Epoch 11, Batch_no 103, 82% \t train_loss: 1.08 took: 2.12s\n",
            "Epoch 11, Batch_no 116, 92% \t train_loss: 1.05 took: 2.92s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 58 %\n",
            "Validation loss = 1.20\n",
            "Epoch 12, Batch_no 12, 10% \t train_loss: 1.06 took: 3.27s\n",
            "Epoch 12, Batch_no 25, 20% \t train_loss: 0.95 took: 2.66s\n",
            "Epoch 12, Batch_no 38, 30% \t train_loss: 0.99 took: 2.46s\n",
            "Epoch 12, Batch_no 51, 41% \t train_loss: 0.95 took: 2.64s\n",
            "Epoch 12, Batch_no 64, 51% \t train_loss: 1.00 took: 3.12s\n",
            "Epoch 12, Batch_no 77, 61% \t train_loss: 1.08 took: 2.44s\n",
            "Epoch 12, Batch_no 90, 72% \t train_loss: 1.04 took: 2.50s\n",
            "Epoch 12, Batch_no 103, 82% \t train_loss: 1.17 took: 2.30s\n",
            "Epoch 12, Batch_no 116, 92% \t train_loss: 1.04 took: 2.76s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 61 %\n",
            "Validation loss = 0.92\n",
            "Epoch 13, Batch_no 12, 10% \t train_loss: 0.92 took: 3.33s\n",
            "Epoch 13, Batch_no 25, 20% \t train_loss: 0.99 took: 2.65s\n",
            "Epoch 13, Batch_no 38, 30% \t train_loss: 0.96 took: 2.51s\n",
            "Epoch 13, Batch_no 51, 41% \t train_loss: 0.94 took: 2.32s\n",
            "Epoch 13, Batch_no 64, 51% \t train_loss: 0.97 took: 2.69s\n",
            "Epoch 13, Batch_no 77, 61% \t train_loss: 1.03 took: 2.41s\n",
            "Epoch 13, Batch_no 90, 72% \t train_loss: 0.85 took: 2.24s\n",
            "Epoch 13, Batch_no 103, 82% \t train_loss: 0.99 took: 2.78s\n",
            "Epoch 13, Batch_no 116, 92% \t train_loss: 1.04 took: 2.65s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 58 %\n",
            "Validation loss = 1.13\n",
            "Epoch 14, Batch_no 12, 10% \t train_loss: 1.06 took: 3.94s\n",
            "Epoch 14, Batch_no 25, 20% \t train_loss: 1.00 took: 2.33s\n",
            "Epoch 14, Batch_no 38, 30% \t train_loss: 0.91 took: 2.16s\n",
            "Epoch 14, Batch_no 51, 41% \t train_loss: 1.01 took: 2.45s\n",
            "Epoch 14, Batch_no 64, 51% \t train_loss: 0.92 took: 3.07s\n",
            "Epoch 14, Batch_no 77, 61% \t train_loss: 0.96 took: 2.40s\n",
            "Epoch 14, Batch_no 90, 72% \t train_loss: 1.04 took: 2.50s\n",
            "Epoch 14, Batch_no 103, 82% \t train_loss: 1.06 took: 2.20s\n",
            "Epoch 14, Batch_no 116, 92% \t train_loss: 1.04 took: 2.57s\n",
            "Training accuracy: 63 %\n",
            "Validation accuracy: 62 %\n",
            "Validation loss = 0.86\n",
            "Epoch 15, Batch_no 12, 10% \t train_loss: 1.04 took: 3.29s\n",
            "Epoch 15, Batch_no 25, 20% \t train_loss: 0.87 took: 2.63s\n",
            "Epoch 15, Batch_no 38, 30% \t train_loss: 0.88 took: 2.45s\n",
            "Epoch 15, Batch_no 51, 41% \t train_loss: 0.92 took: 2.20s\n",
            "Epoch 15, Batch_no 64, 51% \t train_loss: 0.91 took: 2.96s\n",
            "Epoch 15, Batch_no 77, 61% \t train_loss: 0.96 took: 2.62s\n",
            "Epoch 15, Batch_no 90, 72% \t train_loss: 0.92 took: 2.41s\n",
            "Epoch 15, Batch_no 103, 82% \t train_loss: 1.06 took: 2.68s\n",
            "Epoch 15, Batch_no 116, 92% \t train_loss: 1.01 took: 2.88s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.89\n",
            "Epoch 16, Batch_no 12, 10% \t train_loss: 0.93 took: 3.01s\n",
            "Epoch 16, Batch_no 25, 20% \t train_loss: 0.97 took: 2.42s\n",
            "Epoch 16, Batch_no 38, 30% \t train_loss: 0.85 took: 2.73s\n",
            "Epoch 16, Batch_no 51, 41% \t train_loss: 1.02 took: 2.40s\n",
            "Epoch 16, Batch_no 64, 51% \t train_loss: 0.91 took: 2.42s\n",
            "Epoch 16, Batch_no 77, 61% \t train_loss: 0.96 took: 3.09s\n",
            "Epoch 16, Batch_no 90, 72% \t train_loss: 0.94 took: 2.67s\n",
            "Epoch 16, Batch_no 103, 82% \t train_loss: 0.95 took: 2.46s\n",
            "Epoch 16, Batch_no 116, 92% \t train_loss: 0.92 took: 2.41s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 64 %\n",
            "Validation loss = 0.94\n",
            "Epoch 17, Batch_no 12, 10% \t train_loss: 0.85 took: 3.41s\n",
            "Epoch 17, Batch_no 25, 20% \t train_loss: 0.96 took: 2.73s\n",
            "Epoch 17, Batch_no 38, 30% \t train_loss: 0.96 took: 2.18s\n",
            "Epoch 17, Batch_no 51, 41% \t train_loss: 0.85 took: 2.67s\n",
            "Epoch 17, Batch_no 64, 51% \t train_loss: 0.99 took: 2.36s\n",
            "Epoch 17, Batch_no 77, 61% \t train_loss: 0.90 took: 2.86s\n",
            "Epoch 17, Batch_no 90, 72% \t train_loss: 0.91 took: 3.15s\n",
            "Epoch 17, Batch_no 103, 82% \t train_loss: 0.93 took: 2.45s\n",
            "Epoch 17, Batch_no 116, 92% \t train_loss: 0.92 took: 2.55s\n",
            "Training accuracy: 65 %\n",
            "Validation accuracy: 68 %\n",
            "Validation loss = 0.81\n",
            "Epoch 18, Batch_no 12, 10% \t train_loss: 0.85 took: 3.18s\n",
            "Epoch 18, Batch_no 25, 20% \t train_loss: 0.91 took: 2.91s\n",
            "Epoch 18, Batch_no 38, 30% \t train_loss: 0.87 took: 2.40s\n",
            "Epoch 18, Batch_no 51, 41% \t train_loss: 0.80 took: 2.35s\n",
            "Epoch 18, Batch_no 64, 51% \t train_loss: 0.92 took: 3.13s\n",
            "Epoch 18, Batch_no 77, 61% \t train_loss: 0.88 took: 2.44s\n",
            "Epoch 18, Batch_no 90, 72% \t train_loss: 1.09 took: 2.58s\n",
            "Epoch 18, Batch_no 103, 82% \t train_loss: 0.87 took: 2.45s\n",
            "Epoch 18, Batch_no 116, 92% \t train_loss: 0.94 took: 2.80s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 64 %\n",
            "Validation loss = 0.91\n",
            "Epoch 19, Batch_no 12, 10% \t train_loss: 1.00 took: 3.35s\n",
            "Epoch 19, Batch_no 25, 20% \t train_loss: 0.99 took: 2.28s\n",
            "Epoch 19, Batch_no 38, 30% \t train_loss: 0.89 took: 2.97s\n",
            "Epoch 19, Batch_no 51, 41% \t train_loss: 0.95 took: 2.34s\n",
            "Epoch 19, Batch_no 64, 51% \t train_loss: 0.94 took: 2.43s\n",
            "Epoch 19, Batch_no 77, 61% \t train_loss: 0.94 took: 2.81s\n",
            "Epoch 19, Batch_no 90, 72% \t train_loss: 0.95 took: 2.66s\n",
            "Epoch 19, Batch_no 103, 82% \t train_loss: 0.88 took: 2.57s\n",
            "Epoch 19, Batch_no 116, 92% \t train_loss: 0.99 took: 2.72s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.87\n",
            "Epoch 20, Batch_no 12, 10% \t train_loss: 0.95 took: 3.60s\n",
            "Epoch 20, Batch_no 25, 20% \t train_loss: 0.83 took: 2.15s\n",
            "Epoch 20, Batch_no 38, 30% \t train_loss: 0.89 took: 2.81s\n",
            "Epoch 20, Batch_no 51, 41% \t train_loss: 0.94 took: 2.64s\n",
            "Epoch 20, Batch_no 64, 51% \t train_loss: 0.92 took: 2.50s\n",
            "Epoch 20, Batch_no 77, 61% \t train_loss: 0.95 took: 2.44s\n",
            "Epoch 20, Batch_no 90, 72% \t train_loss: 0.90 took: 2.69s\n",
            "Epoch 20, Batch_no 103, 82% \t train_loss: 0.86 took: 2.47s\n",
            "Epoch 20, Batch_no 116, 92% \t train_loss: 0.91 took: 2.77s\n",
            "Training accuracy: 65 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.83\n",
            "Epoch 21, Batch_no 12, 10% \t train_loss: 0.89 took: 3.44s\n",
            "Epoch 21, Batch_no 25, 20% \t train_loss: 0.85 took: 2.55s\n",
            "Epoch 21, Batch_no 38, 30% \t train_loss: 1.02 took: 2.41s\n",
            "Epoch 21, Batch_no 51, 41% \t train_loss: 0.88 took: 2.44s\n",
            "Epoch 21, Batch_no 64, 51% \t train_loss: 0.93 took: 2.98s\n",
            "Epoch 21, Batch_no 77, 61% \t train_loss: 0.81 took: 2.45s\n",
            "Epoch 21, Batch_no 90, 72% \t train_loss: 1.01 took: 2.36s\n",
            "Epoch 21, Batch_no 103, 82% \t train_loss: 0.87 took: 2.48s\n",
            "Epoch 21, Batch_no 116, 92% \t train_loss: 0.85 took: 3.20s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 62 %\n",
            "Validation loss = 1.05\n",
            "Epoch 22, Batch_no 12, 10% \t train_loss: 0.87 took: 3.36s\n",
            "Epoch 22, Batch_no 25, 20% \t train_loss: 1.00 took: 2.97s\n",
            "Epoch 22, Batch_no 38, 30% \t train_loss: 0.82 took: 2.74s\n",
            "Epoch 22, Batch_no 51, 41% \t train_loss: 0.91 took: 2.60s\n",
            "Epoch 22, Batch_no 64, 51% \t train_loss: 0.79 took: 2.41s\n",
            "Epoch 22, Batch_no 77, 61% \t train_loss: 0.77 took: 2.67s\n",
            "Epoch 22, Batch_no 90, 72% \t train_loss: 0.83 took: 2.83s\n",
            "Epoch 22, Batch_no 103, 82% \t train_loss: 0.92 took: 2.81s\n",
            "Epoch 22, Batch_no 116, 92% \t train_loss: 0.86 took: 2.53s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.83\n",
            "Epoch 23, Batch_no 12, 10% \t train_loss: 0.82 took: 3.29s\n",
            "Epoch 23, Batch_no 25, 20% \t train_loss: 1.00 took: 2.49s\n",
            "Epoch 23, Batch_no 38, 30% \t train_loss: 0.87 took: 2.33s\n",
            "Epoch 23, Batch_no 51, 41% \t train_loss: 0.80 took: 2.53s\n",
            "Epoch 23, Batch_no 64, 51% \t train_loss: 0.88 took: 3.26s\n",
            "Epoch 23, Batch_no 77, 61% \t train_loss: 0.97 took: 2.53s\n",
            "Epoch 23, Batch_no 90, 72% \t train_loss: 1.00 took: 2.41s\n",
            "Epoch 23, Batch_no 103, 82% \t train_loss: 0.88 took: 2.52s\n",
            "Epoch 23, Batch_no 116, 92% \t train_loss: 0.88 took: 2.80s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.91\n",
            "Epoch 24, Batch_no 12, 10% \t train_loss: 0.97 took: 3.47s\n",
            "Epoch 24, Batch_no 25, 20% \t train_loss: 0.92 took: 2.22s\n",
            "Epoch 24, Batch_no 38, 30% \t train_loss: 0.86 took: 2.96s\n",
            "Epoch 24, Batch_no 51, 41% \t train_loss: 0.85 took: 2.53s\n",
            "Epoch 24, Batch_no 64, 51% \t train_loss: 0.81 took: 2.42s\n",
            "Epoch 24, Batch_no 77, 61% \t train_loss: 0.88 took: 2.32s\n",
            "Epoch 24, Batch_no 90, 72% \t train_loss: 0.92 took: 3.13s\n",
            "Epoch 24, Batch_no 103, 82% \t train_loss: 0.86 took: 2.36s\n",
            "Epoch 24, Batch_no 116, 92% \t train_loss: 0.84 took: 3.00s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 62 %\n",
            "Validation loss = 0.94\n",
            "Epoch 25, Batch_no 12, 10% \t train_loss: 0.87 took: 3.53s\n",
            "Epoch 25, Batch_no 25, 20% \t train_loss: 0.88 took: 2.59s\n",
            "Epoch 25, Batch_no 38, 30% \t train_loss: 0.89 took: 2.38s\n",
            "Epoch 25, Batch_no 51, 41% \t train_loss: 0.89 took: 2.63s\n",
            "Epoch 25, Batch_no 64, 51% \t train_loss: 0.98 took: 2.80s\n",
            "Epoch 25, Batch_no 77, 61% \t train_loss: 0.83 took: 2.42s\n",
            "Epoch 25, Batch_no 90, 72% \t train_loss: 0.75 took: 2.68s\n",
            "Epoch 25, Batch_no 103, 82% \t train_loss: 0.86 took: 2.19s\n",
            "Epoch 25, Batch_no 116, 92% \t train_loss: 1.01 took: 3.41s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 61 %\n",
            "Validation loss = 0.91\n",
            "Epoch 26, Batch_no 12, 10% \t train_loss: 0.87 took: 3.01s\n",
            "Epoch 26, Batch_no 25, 20% \t train_loss: 0.78 took: 2.95s\n",
            "Epoch 26, Batch_no 38, 30% \t train_loss: 0.81 took: 3.11s\n",
            "Epoch 26, Batch_no 51, 41% \t train_loss: 0.90 took: 2.24s\n",
            "Epoch 26, Batch_no 64, 51% \t train_loss: 0.80 took: 2.25s\n",
            "Epoch 26, Batch_no 77, 61% \t train_loss: 0.88 took: 2.64s\n",
            "Epoch 26, Batch_no 90, 72% \t train_loss: 0.93 took: 2.63s\n",
            "Epoch 26, Batch_no 103, 82% \t train_loss: 0.75 took: 2.34s\n",
            "Epoch 26, Batch_no 116, 92% \t train_loss: 1.03 took: 2.73s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.77\n",
            "Epoch 27, Batch_no 12, 10% \t train_loss: 0.77 took: 3.54s\n",
            "Epoch 27, Batch_no 25, 20% \t train_loss: 0.89 took: 2.68s\n",
            "Epoch 27, Batch_no 38, 30% \t train_loss: 0.82 took: 2.75s\n",
            "Epoch 27, Batch_no 51, 41% \t train_loss: 0.80 took: 2.11s\n",
            "Epoch 27, Batch_no 64, 51% \t train_loss: 0.79 took: 3.04s\n",
            "Epoch 27, Batch_no 77, 61% \t train_loss: 0.85 took: 2.56s\n",
            "Epoch 27, Batch_no 90, 72% \t train_loss: 0.91 took: 2.54s\n",
            "Epoch 27, Batch_no 103, 82% \t train_loss: 0.84 took: 2.34s\n",
            "Epoch 27, Batch_no 116, 92% \t train_loss: 0.89 took: 3.16s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 71 %\n",
            "Validation loss = 0.78\n",
            "Epoch 28, Batch_no 12, 10% \t train_loss: 0.70 took: 3.94s\n",
            "Epoch 28, Batch_no 25, 20% \t train_loss: 0.77 took: 2.30s\n",
            "Epoch 28, Batch_no 38, 30% \t train_loss: 1.01 took: 2.82s\n",
            "Epoch 28, Batch_no 51, 41% \t train_loss: 0.84 took: 2.61s\n",
            "Epoch 28, Batch_no 64, 51% \t train_loss: 0.91 took: 2.97s\n",
            "Epoch 28, Batch_no 77, 61% \t train_loss: 0.89 took: 2.70s\n",
            "Epoch 28, Batch_no 90, 72% \t train_loss: 0.92 took: 2.20s\n",
            "Epoch 28, Batch_no 103, 82% \t train_loss: 0.86 took: 2.67s\n",
            "Epoch 28, Batch_no 116, 92% \t train_loss: 0.90 took: 3.23s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.87\n",
            "Epoch 29, Batch_no 12, 10% \t train_loss: 0.92 took: 3.49s\n",
            "Epoch 29, Batch_no 25, 20% \t train_loss: 0.78 took: 2.41s\n",
            "Epoch 29, Batch_no 38, 30% \t train_loss: 0.94 took: 2.65s\n",
            "Epoch 29, Batch_no 51, 41% \t train_loss: 0.84 took: 2.42s\n",
            "Epoch 29, Batch_no 64, 51% \t train_loss: 0.82 took: 2.42s\n",
            "Epoch 29, Batch_no 77, 61% \t train_loss: 0.77 took: 3.15s\n",
            "Epoch 29, Batch_no 90, 72% \t train_loss: 0.95 took: 2.33s\n",
            "Epoch 29, Batch_no 103, 82% \t train_loss: 0.93 took: 2.51s\n",
            "Epoch 29, Batch_no 116, 92% \t train_loss: 0.77 took: 3.05s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 72 %\n",
            "Validation loss = 0.67\n",
            "Epoch 30, Batch_no 12, 10% \t train_loss: 0.98 took: 3.64s\n",
            "Epoch 30, Batch_no 25, 20% \t train_loss: 0.86 took: 2.27s\n",
            "Epoch 30, Batch_no 38, 30% \t train_loss: 0.87 took: 2.56s\n",
            "Epoch 30, Batch_no 51, 41% \t train_loss: 0.84 took: 2.48s\n",
            "Epoch 30, Batch_no 64, 51% \t train_loss: 0.80 took: 3.22s\n",
            "Epoch 30, Batch_no 77, 61% \t train_loss: 0.93 took: 2.39s\n",
            "Epoch 30, Batch_no 90, 72% \t train_loss: 0.84 took: 2.57s\n",
            "Epoch 30, Batch_no 103, 82% \t train_loss: 0.89 took: 2.61s\n",
            "Epoch 30, Batch_no 116, 92% \t train_loss: 0.83 took: 3.18s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 69 %\n",
            "Validation loss = 0.78\n",
            "Epoch 31, Batch_no 12, 10% \t train_loss: 0.81 took: 3.09s\n",
            "Epoch 31, Batch_no 25, 20% \t train_loss: 0.76 took: 2.75s\n",
            "Epoch 31, Batch_no 38, 30% \t train_loss: 0.82 took: 2.52s\n",
            "Epoch 31, Batch_no 51, 41% \t train_loss: 0.87 took: 2.71s\n",
            "Epoch 31, Batch_no 64, 51% \t train_loss: 0.81 took: 2.44s\n",
            "Epoch 31, Batch_no 77, 61% \t train_loss: 0.90 took: 3.03s\n",
            "Epoch 31, Batch_no 90, 72% \t train_loss: 0.76 took: 2.62s\n",
            "Epoch 31, Batch_no 103, 82% \t train_loss: 0.86 took: 2.36s\n",
            "Epoch 31, Batch_no 116, 92% \t train_loss: 0.99 took: 2.84s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 68 %\n",
            "Validation loss = 0.78\n",
            "Epoch 32, Batch_no 12, 10% \t train_loss: 0.88 took: 3.17s\n",
            "Epoch 32, Batch_no 25, 20% \t train_loss: 0.89 took: 2.72s\n",
            "Epoch 32, Batch_no 38, 30% \t train_loss: 0.85 took: 3.00s\n",
            "Epoch 32, Batch_no 51, 41% \t train_loss: 0.79 took: 2.87s\n",
            "Epoch 32, Batch_no 64, 51% \t train_loss: 0.76 took: 2.48s\n",
            "Epoch 32, Batch_no 77, 61% \t train_loss: 0.78 took: 2.40s\n",
            "Epoch 32, Batch_no 90, 72% \t train_loss: 0.91 took: 2.72s\n",
            "Epoch 32, Batch_no 103, 82% \t train_loss: 0.74 took: 2.48s\n",
            "Epoch 32, Batch_no 116, 92% \t train_loss: 0.76 took: 2.58s\n",
            "Training accuracy: 68 %\n",
            "Validation accuracy: 69 %\n",
            "Validation loss = 0.88\n",
            "Epoch 33, Batch_no 12, 10% \t train_loss: 0.79 took: 3.12s\n",
            "Epoch 33, Batch_no 25, 20% \t train_loss: 0.76 took: 2.74s\n",
            "Epoch 33, Batch_no 38, 30% \t train_loss: 0.85 took: 2.49s\n",
            "Epoch 33, Batch_no 51, 41% \t train_loss: 0.96 took: 2.54s\n",
            "Epoch 33, Batch_no 64, 51% \t train_loss: 0.85 took: 3.08s\n",
            "Epoch 33, Batch_no 77, 61% \t train_loss: 0.84 took: 2.46s\n",
            "Epoch 33, Batch_no 90, 72% \t train_loss: 0.83 took: 2.86s\n",
            "Epoch 33, Batch_no 103, 82% \t train_loss: 0.81 took: 2.42s\n",
            "Epoch 33, Batch_no 116, 92% \t train_loss: 0.90 took: 2.51s\n",
            "Training accuracy: 68 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.81\n",
            "Epoch 34, Batch_no 12, 10% \t train_loss: 0.88 took: 3.36s\n",
            "Epoch 34, Batch_no 25, 20% \t train_loss: 0.78 took: 2.38s\n",
            "Epoch 34, Batch_no 38, 30% \t train_loss: 0.90 took: 2.89s\n",
            "Epoch 34, Batch_no 51, 41% \t train_loss: 0.89 took: 2.61s\n",
            "Epoch 34, Batch_no 64, 51% \t train_loss: 0.85 took: 2.99s\n",
            "Epoch 34, Batch_no 77, 61% \t train_loss: 0.86 took: 3.22s\n",
            "Epoch 34, Batch_no 90, 72% \t train_loss: 0.80 took: 2.55s\n",
            "Epoch 34, Batch_no 103, 82% \t train_loss: 0.78 took: 2.28s\n",
            "Epoch 34, Batch_no 116, 92% \t train_loss: 0.72 took: 2.47s\n",
            "Training accuracy: 68 %\n",
            "Validation accuracy: 70 %\n",
            "Validation loss = 0.75\n",
            "Epoch 35, Batch_no 12, 10% \t train_loss: 0.76 took: 3.05s\n",
            "Epoch 35, Batch_no 25, 20% \t train_loss: 0.81 took: 3.05s\n",
            "Epoch 35, Batch_no 38, 30% \t train_loss: 0.81 took: 2.48s\n",
            "Epoch 35, Batch_no 51, 41% \t train_loss: 0.78 took: 2.43s\n",
            "Epoch 35, Batch_no 64, 51% \t train_loss: 0.71 took: 2.91s\n",
            "Epoch 35, Batch_no 77, 61% \t train_loss: 0.90 took: 2.59s\n",
            "Epoch 35, Batch_no 90, 72% \t train_loss: 0.83 took: 2.63s\n",
            "Epoch 35, Batch_no 103, 82% \t train_loss: 0.75 took: 2.44s\n",
            "Epoch 35, Batch_no 116, 92% \t train_loss: 0.86 took: 3.24s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 68 %\n",
            "Validation loss = 0.80\n",
            "Epoch 36, Batch_no 12, 10% \t train_loss: 0.75 took: 3.60s\n",
            "Epoch 36, Batch_no 25, 20% \t train_loss: 0.82 took: 2.57s\n",
            "Epoch 36, Batch_no 38, 30% \t train_loss: 0.78 took: 2.62s\n",
            "Epoch 36, Batch_no 51, 41% \t train_loss: 0.76 took: 2.20s\n",
            "Epoch 36, Batch_no 64, 51% \t train_loss: 0.91 took: 2.98s\n",
            "Epoch 36, Batch_no 77, 61% \t train_loss: 0.84 took: 2.72s\n",
            "Epoch 36, Batch_no 90, 72% \t train_loss: 0.79 took: 3.05s\n",
            "Epoch 36, Batch_no 103, 82% \t train_loss: 0.79 took: 2.36s\n",
            "Epoch 36, Batch_no 116, 92% \t train_loss: 0.74 took: 3.27s\n",
            "Training accuracy: 70 %\n",
            "Validation accuracy: 72 %\n",
            "Validation loss = 0.69\n",
            "Epoch 37, Batch_no 12, 10% \t train_loss: 0.76 took: 3.57s\n",
            "Epoch 37, Batch_no 25, 20% \t train_loss: 0.84 took: 2.54s\n",
            "Epoch 37, Batch_no 38, 30% \t train_loss: 0.82 took: 2.50s\n",
            "Epoch 37, Batch_no 51, 41% \t train_loss: 0.72 took: 3.00s\n",
            "Epoch 37, Batch_no 64, 51% \t train_loss: 0.93 took: 2.62s\n",
            "Epoch 37, Batch_no 77, 61% \t train_loss: 0.88 took: 3.31s\n",
            "Epoch 37, Batch_no 90, 72% \t train_loss: 0.86 took: 2.33s\n",
            "Epoch 37, Batch_no 103, 82% \t train_loss: 0.86 took: 2.68s\n",
            "Epoch 37, Batch_no 116, 92% \t train_loss: 0.88 took: 2.54s\n",
            "Training accuracy: 68 %\n",
            "Validation accuracy: 70 %\n",
            "Validation loss = 0.69\n",
            "Epoch 38, Batch_no 12, 10% \t train_loss: 0.77 took: 3.56s\n",
            "Epoch 38, Batch_no 25, 20% \t train_loss: 0.86 took: 2.97s\n",
            "Epoch 38, Batch_no 38, 30% \t train_loss: 0.77 took: 2.60s\n",
            "Epoch 38, Batch_no 51, 41% \t train_loss: 0.81 took: 2.63s\n",
            "Epoch 38, Batch_no 64, 51% \t train_loss: 0.71 took: 2.49s\n",
            "Epoch 38, Batch_no 77, 61% \t train_loss: 0.71 took: 3.13s\n",
            "Epoch 38, Batch_no 90, 72% \t train_loss: 0.81 took: 2.25s\n",
            "Epoch 38, Batch_no 103, 82% \t train_loss: 0.81 took: 2.37s\n",
            "Epoch 38, Batch_no 116, 92% \t train_loss: 0.79 took: 2.71s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 70 %\n",
            "Validation loss = 0.75\n",
            "Epoch 39, Batch_no 12, 10% \t train_loss: 0.88 took: 3.25s\n",
            "Epoch 39, Batch_no 25, 20% \t train_loss: 0.80 took: 2.63s\n",
            "Epoch 39, Batch_no 38, 30% \t train_loss: 0.80 took: 2.76s\n",
            "Epoch 39, Batch_no 51, 41% \t train_loss: 0.77 took: 2.97s\n",
            "Epoch 39, Batch_no 64, 51% \t train_loss: 0.69 took: 2.59s\n",
            "Epoch 39, Batch_no 77, 61% \t train_loss: 0.80 took: 2.49s\n",
            "Epoch 39, Batch_no 90, 72% \t train_loss: 0.65 took: 2.53s\n",
            "Epoch 39, Batch_no 103, 82% \t train_loss: 0.87 took: 3.13s\n",
            "Epoch 39, Batch_no 116, 92% \t train_loss: 0.78 took: 2.82s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 73 %\n",
            "Validation loss = 0.65\n",
            "Epoch 40, Batch_no 12, 10% \t train_loss: 0.86 took: 3.50s\n",
            "Epoch 40, Batch_no 25, 20% \t train_loss: 0.82 took: 2.48s\n",
            "Epoch 40, Batch_no 38, 30% \t train_loss: 0.69 took: 2.60s\n",
            "Epoch 40, Batch_no 51, 41% \t train_loss: 0.81 took: 2.98s\n",
            "Epoch 40, Batch_no 64, 51% \t train_loss: 0.83 took: 3.09s\n",
            "Epoch 40, Batch_no 77, 61% \t train_loss: 0.81 took: 2.55s\n",
            "Epoch 40, Batch_no 90, 72% \t train_loss: 0.84 took: 2.43s\n",
            "Epoch 40, Batch_no 103, 82% \t train_loss: 0.77 took: 2.83s\n",
            "Epoch 40, Batch_no 116, 92% \t train_loss: 0.74 took: 3.31s\n",
            "Training accuracy: 72 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.81\n",
            "Epoch 41, Batch_no 12, 10% \t train_loss: 0.82 took: 3.11s\n",
            "Epoch 41, Batch_no 25, 20% \t train_loss: 0.72 took: 2.61s\n",
            "Epoch 41, Batch_no 38, 30% \t train_loss: 0.87 took: 2.90s\n",
            "Epoch 41, Batch_no 51, 41% \t train_loss: 0.88 took: 2.60s\n",
            "Epoch 41, Batch_no 64, 51% \t train_loss: 0.90 took: 2.85s\n",
            "Epoch 41, Batch_no 77, 61% \t train_loss: 0.71 took: 2.56s\n",
            "Epoch 41, Batch_no 90, 72% \t train_loss: 0.92 took: 2.87s\n",
            "Epoch 41, Batch_no 103, 82% \t train_loss: 0.76 took: 3.05s\n",
            "Epoch 41, Batch_no 116, 92% \t train_loss: 0.81 took: 2.61s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 75 %\n",
            "Validation loss = 0.61\n",
            "Epoch 42, Batch_no 12, 10% \t train_loss: 0.78 took: 3.44s\n",
            "Epoch 42, Batch_no 25, 20% \t train_loss: 0.74 took: 2.66s\n",
            "Epoch 42, Batch_no 38, 30% \t train_loss: 0.74 took: 2.91s\n",
            "Epoch 42, Batch_no 51, 41% \t train_loss: 0.73 took: 2.63s\n",
            "Epoch 42, Batch_no 64, 51% \t train_loss: 0.82 took: 2.52s\n",
            "Epoch 42, Batch_no 77, 61% \t train_loss: 0.75 took: 2.66s\n",
            "Epoch 42, Batch_no 90, 72% \t train_loss: 0.84 took: 3.03s\n",
            "Epoch 42, Batch_no 103, 82% \t train_loss: 0.90 took: 2.46s\n",
            "Epoch 42, Batch_no 116, 92% \t train_loss: 0.73 took: 2.37s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 73 %\n",
            "Validation loss = 0.66\n",
            "Epoch 43, Batch_no 12, 10% \t train_loss: 0.79 took: 3.15s\n",
            "Epoch 43, Batch_no 25, 20% \t train_loss: 0.88 took: 3.08s\n",
            "Epoch 43, Batch_no 38, 30% \t train_loss: 0.90 took: 2.19s\n",
            "Epoch 43, Batch_no 51, 41% \t train_loss: 0.80 took: 2.48s\n",
            "Epoch 43, Batch_no 64, 51% \t train_loss: 0.85 took: 2.81s\n",
            "Epoch 43, Batch_no 77, 61% \t train_loss: 0.77 took: 2.46s\n",
            "Epoch 43, Batch_no 90, 72% \t train_loss: 0.80 took: 2.88s\n",
            "Epoch 43, Batch_no 103, 82% \t train_loss: 0.85 took: 2.28s\n",
            "Epoch 43, Batch_no 116, 92% \t train_loss: 0.76 took: 3.19s\n",
            "Training accuracy: 68 %\n",
            "Validation accuracy: 72 %\n",
            "Validation loss = 0.67\n",
            "Epoch 44, Batch_no 12, 10% \t train_loss: 0.84 took: 3.82s\n",
            "Epoch 44, Batch_no 25, 20% \t train_loss: 0.78 took: 2.43s\n",
            "Epoch 44, Batch_no 38, 30% \t train_loss: 0.80 took: 2.74s\n",
            "Epoch 44, Batch_no 51, 41% \t train_loss: 0.83 took: 2.43s\n",
            "Epoch 44, Batch_no 64, 51% \t train_loss: 0.74 took: 2.84s\n",
            "Epoch 44, Batch_no 77, 61% \t train_loss: 0.73 took: 2.48s\n",
            "Epoch 44, Batch_no 90, 72% \t train_loss: 0.77 took: 2.19s\n",
            "Epoch 44, Batch_no 103, 82% \t train_loss: 0.76 took: 2.75s\n",
            "Epoch 44, Batch_no 116, 92% \t train_loss: 0.84 took: 2.98s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 72 %\n",
            "Validation loss = 0.69\n",
            "Epoch 45, Batch_no 12, 10% \t train_loss: 0.84 took: 3.35s\n",
            "Epoch 45, Batch_no 25, 20% \t train_loss: 0.70 took: 2.24s\n",
            "Epoch 45, Batch_no 38, 30% \t train_loss: 0.86 took: 2.62s\n",
            "Epoch 45, Batch_no 51, 41% \t train_loss: 0.78 took: 2.52s\n",
            "Epoch 45, Batch_no 64, 51% \t train_loss: 0.83 took: 2.62s\n",
            "Epoch 45, Batch_no 77, 61% \t train_loss: 0.77 took: 2.73s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjtKl85UCfWy",
        "colab_type": "text"
      },
      "source": [
        "Generating trained model pt file using a fresh instance of network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnHGvLxyBDEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model to memory\n",
        "restore_point = torch.load('./drive/My Drive/Papsmear_dataset/testnet_normalized.pth')\n",
        "#fresh_net = MobileNetV3.mobilenetv3_large()\n",
        "fresh_net = simple_testnet(num_classes=5)\n",
        "fresh_net.load_state_dict(restore_point)\n",
        "fresh_net.eval()\n",
        "\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(fresh_net, example)\n",
        "traced_script_module.save('./drive/My Drive/Papsmear_dataset/testnet_normalized_reloaded.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2UODirBCrE6",
        "colab_type": "text"
      },
      "source": [
        "Testing image on 5 random images and compare the output tensors with Android output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT4QfDCjj4Al",
        "colab_type": "code",
        "outputId": "8b51a23c-f1dd-4089-bda8-70987e2a9dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        #transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265])\n",
        "    ])\n",
        "Test_neu_dataset = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/test',transform=data_transform)\n",
        "Testloader = torch.utils.data.DataLoader(Test_neu_dataset,batch_size=10, shuffle=True,num_workers=4)\n",
        "\n",
        "confusion_matrix = torch.zeros(5, 5) # no_classes*no_classes      \n",
        "fresh_net = fresh_net.to(device, )\n",
        "correct = 0\n",
        "total = 0\n",
        "for data, classes in Testloader:\n",
        "    data, classes = data.to(device), classes.to(device)\n",
        "    output = fresh_net(data)\n",
        "    _, preds = torch.max(output, 1)\n",
        "    total += classes.size(0)\n",
        "    correct += (preds == classes).sum().item()\n",
        "    for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "      confusion_matrix[t.long(), p.long()] += 1\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 79 %\n",
            "Confusion matrix:\n",
            "tensor([[146.,  13.,   0.,   4.,   0.],\n",
            "        [ 17., 146.,   1.,   0.,   1.],\n",
            "        [  0.,   2.,  53.,   0.,   0.],\n",
            "        [ 11.,  35.,   0., 111.,   1.],\n",
            "        [  7.,  46.,   0.,   0.,  80.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eijZ4VeYCVj9",
        "colab_type": "code",
        "outputId": "de82cfaf-3dcb-4157-b69c-8ca2e570f025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        #transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265])\n",
        "    ])\n",
        "\n",
        "def image_loader_display(image_path):\n",
        "    \"\"\"load image, returns cuda tensor\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image = data_transform(image).float()\n",
        "    image = Variable(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
        "    return image.cpu()  #assumes that you're using GPU\n",
        "parent_paths = './drive/My Drive/Papsmear_dataset/smear2005Format/test/im_Metaplastic/' #this is class two \n",
        "image_paths = ['271.bmp', '270.bmp', '265.bmp', '259.bmp', '258.bmp']    \n",
        "#parent_paths = './drive/My Drive/Papsmear_dataset/smear2005Format/test/im_Dyskeratotic/' #this is class four \n",
        "#image_paths = ['001_02.bmp', '002_05.bmp', '002_06.bmp', '002_08.bmp', '002_09.bmp']\n",
        "#parent_paths = './drive/My Drive/Papsmear_dataset/smear2005Format/test/im_Koilocytotic/' #this is class three \n",
        "#image_paths = ['001_01.bmp', '003_01.bmp', '010_01.bmp', '011_06.bmp', '015_04.bmp'] \n",
        "#parent_paths = './drive/My Drive/Papsmear_dataset/smear2005Format/test/im_Parabasal/' #this is class one \n",
        "#image_paths = ['107_04.bmp', '106_09.bmp', '106_07.bmp', '100_15.bmp', '101_03.bmp'] \n",
        "#parent_paths = './drive/My Drive/Papsmear_dataset/smear2005Format/test/im_Superficial-Intermediate/' #this is class zero \n",
        "#image_paths = ['118_01.bmp', '126_06.bmp', '117_04.bmp', '116_01.bmp', '116_04.bmp'] \n",
        "for image_path in image_paths:\n",
        "  image_uri = parent_paths + image_path\n",
        "  image = image_loader_display(image_uri)\n",
        "  image = image.to(device)\n",
        "  prediction = fresh_net(image)\n",
        "  _, preds = torch.max(prediction, 1)\n",
        "  print('The value',_)\n",
        "  print('the class',preds)\n",
        "  print(f\"prediction: {prediction.data}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The value tensor([10.5777], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "the class tensor([2], device='cuda:0')\n",
            "prediction: tensor([[-12.9554,   5.4303,  10.5777, -14.0763,  -8.2147]], device='cuda:0')\n",
            "The value tensor([12.0213], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "the class tensor([2], device='cuda:0')\n",
            "prediction: tensor([[-14.8115,   6.0074,  12.0213, -16.5469,  -7.6554]], device='cuda:0')\n",
            "The value tensor([6.2110], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "the class tensor([2], device='cuda:0')\n",
            "prediction: tensor([[-14.8500,   1.3218,   6.2110,  -5.2823,  -2.1817]], device='cuda:0')\n",
            "The value tensor([10.0689], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "the class tensor([2], device='cuda:0')\n",
            "prediction: tensor([[-13.7818,   4.3707,  10.0689, -15.0314,  -4.7884]], device='cuda:0')\n",
            "The value tensor([5.0696], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "the class tensor([2], device='cuda:0')\n",
            "prediction: tensor([[-6.6712,  2.2894,  5.0696, -8.3717, -2.6275]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlzvwAqUwZQ9",
        "colab_type": "text"
      },
      "source": [
        "# Saving the Pytorch transforms test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3vl5HoewY5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize([224,224], interpolation=2),\n",
        "        #transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5370, 0.4916, 0.5907], [0.1948, 0.2018, 0.2265])\n",
        "    ])\n",
        "test_set = datasets.ImageFolder(root='./drive/My Drive/Papsmear_dataset/smear2005Format/test',transform=data_transform)\n",
        "t_loader = torch.utils.data.DataLoader(test_set,batch_size=1,num_workers=4,shuffle=False)#shuffle=True for training add\n",
        "\n",
        "for batch_idx, (data, target) in enumerate(t_loader):\n",
        "  data = data.squeeze(0)\n",
        "  data = torchvision.transforms.ToPILImage()(data)\n",
        "  if target == 0:\n",
        "    data.save(\"./drive/My Drive/Papsmear_dataset/test/0/\"+str(batch_idx)+\".jpg\")\n",
        "  elif target ==1:\n",
        "    data.save(\"./drive/My Drive/Papsmear_dataset/test/1/\"+str(batch_idx)+\".jpg\")\n",
        "  elif target ==2:\n",
        "    data.save(\"./drive/My Drive/Papsmear_dataset/test/2/\"+str(batch_idx)+\".jpg\")\n",
        "  elif target ==3:\n",
        "    data.save(\"./drive/My Drive/Papsmear_dataset/test/3/\"+str(batch_idx)+\".jpg\")\n",
        "  elif target ==4:\n",
        "    data.save(\"./drive/My Drive/Papsmear_dataset/test/4/\"+str(batch_idx)+\".jpg\")  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x7sqTUOBN4E",
        "colab_type": "text"
      },
      "source": [
        "# for resnet and simple net dont forget to change the name of the model in traiing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1XUIKNAjLD",
        "colab_type": "code",
        "outputId": "e213f629-f03e-4020-884b-5262fbb0db15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network_simple = simple_testnet(num_classes=5)\n",
        "#network_simple = ResNet18()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "network_simple.to(device)\n",
        "train_list,test_list = trainNet(network_simple, batch_size=16, n_epochs=210, learning_rate=0.0009662520003992714)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "im_Koilocytotic im_Dyskeratotic im_Metaplastic im_Koilocytotic im_Koilocytotic\n",
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 16\n",
            "epochs= 210\n",
            "learning_rate= 0.0009662520003992713\n",
            "==============================\n",
            "Epoch 1, Batch_no 12, 10% \t train_loss: 1.75 took: 23.13s\n",
            "Epoch 1, Batch_no 25, 20% \t train_loss: 1.54 took: 20.50s\n",
            "Epoch 1, Batch_no 38, 30% \t train_loss: 1.38 took: 17.73s\n",
            "Epoch 1, Batch_no 51, 41% \t train_loss: 1.52 took: 19.33s\n",
            "Epoch 1, Batch_no 64, 51% \t train_loss: 1.48 took: 19.71s\n",
            "Epoch 1, Batch_no 77, 61% \t train_loss: 1.41 took: 24.65s\n",
            "Epoch 1, Batch_no 90, 72% \t train_loss: 1.30 took: 19.25s\n",
            "Epoch 1, Batch_no 103, 82% \t train_loss: 1.32 took: 15.52s\n",
            "Epoch 1, Batch_no 116, 92% \t train_loss: 1.24 took: 21.69s\n",
            "Training accuracy: 43 %\n",
            "Validation accuracy: 60 %\n",
            "Validation loss = 1.02\n",
            "Epoch 2, Batch_no 12, 10% \t train_loss: 1.24 took: 3.09s\n",
            "Epoch 2, Batch_no 25, 20% \t train_loss: 1.35 took: 2.27s\n",
            "Epoch 2, Batch_no 38, 30% \t train_loss: 1.24 took: 2.31s\n",
            "Epoch 2, Batch_no 51, 41% \t train_loss: 1.31 took: 2.66s\n",
            "Epoch 2, Batch_no 64, 51% \t train_loss: 1.19 took: 1.87s\n",
            "Epoch 2, Batch_no 77, 61% \t train_loss: 1.26 took: 1.99s\n",
            "Epoch 2, Batch_no 90, 72% \t train_loss: 1.27 took: 2.87s\n",
            "Epoch 2, Batch_no 103, 82% \t train_loss: 1.31 took: 2.55s\n",
            "Epoch 2, Batch_no 116, 92% \t train_loss: 1.11 took: 2.01s\n",
            "Training accuracy: 51 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.87\n",
            "Epoch 3, Batch_no 12, 10% \t train_loss: 1.08 took: 2.68s\n",
            "Epoch 3, Batch_no 25, 20% \t train_loss: 1.22 took: 2.14s\n",
            "Epoch 3, Batch_no 38, 30% \t train_loss: 1.16 took: 3.28s\n",
            "Epoch 3, Batch_no 51, 41% \t train_loss: 1.17 took: 2.23s\n",
            "Epoch 3, Batch_no 64, 51% \t train_loss: 1.15 took: 2.12s\n",
            "Epoch 3, Batch_no 77, 61% \t train_loss: 1.13 took: 1.86s\n",
            "Epoch 3, Batch_no 90, 72% \t train_loss: 1.10 took: 3.04s\n",
            "Epoch 3, Batch_no 103, 82% \t train_loss: 1.11 took: 2.35s\n",
            "Epoch 3, Batch_no 116, 92% \t train_loss: 1.24 took: 2.28s\n",
            "Training accuracy: 57 %\n",
            "Validation accuracy: 65 %\n",
            "Validation loss = 0.79\n",
            "Epoch 4, Batch_no 12, 10% \t train_loss: 1.18 took: 2.65s\n",
            "Epoch 4, Batch_no 25, 20% \t train_loss: 1.18 took: 3.02s\n",
            "Epoch 4, Batch_no 38, 30% \t train_loss: 1.06 took: 2.20s\n",
            "Epoch 4, Batch_no 51, 41% \t train_loss: 1.05 took: 2.29s\n",
            "Epoch 4, Batch_no 64, 51% \t train_loss: 1.10 took: 1.86s\n",
            "Epoch 4, Batch_no 77, 61% \t train_loss: 1.15 took: 2.72s\n",
            "Epoch 4, Batch_no 90, 72% \t train_loss: 1.05 took: 2.25s\n",
            "Epoch 4, Batch_no 103, 82% \t train_loss: 1.13 took: 2.10s\n",
            "Epoch 4, Batch_no 116, 92% \t train_loss: 1.12 took: 2.28s\n",
            "Training accuracy: 59 %\n",
            "Validation accuracy: 66 %\n",
            "Validation loss = 0.80\n",
            "Epoch 5, Batch_no 12, 10% \t train_loss: 0.93 took: 3.37s\n",
            "Epoch 5, Batch_no 25, 20% \t train_loss: 1.07 took: 2.14s\n",
            "Epoch 5, Batch_no 38, 30% \t train_loss: 1.06 took: 2.15s\n",
            "Epoch 5, Batch_no 51, 41% \t train_loss: 1.13 took: 2.22s\n",
            "Epoch 5, Batch_no 64, 51% \t train_loss: 1.00 took: 2.86s\n",
            "Epoch 5, Batch_no 77, 61% \t train_loss: 1.18 took: 2.55s\n",
            "Epoch 5, Batch_no 90, 72% \t train_loss: 1.08 took: 2.37s\n",
            "Epoch 5, Batch_no 103, 82% \t train_loss: 1.18 took: 2.07s\n",
            "Epoch 5, Batch_no 116, 92% \t train_loss: 1.01 took: 2.65s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 69 %\n",
            "Validation loss = 0.76\n",
            "Epoch 6, Batch_no 12, 10% \t train_loss: 0.98 took: 3.07s\n",
            "Epoch 6, Batch_no 25, 20% \t train_loss: 0.98 took: 2.27s\n",
            "Epoch 6, Batch_no 38, 30% \t train_loss: 1.08 took: 2.47s\n",
            "Epoch 6, Batch_no 51, 41% \t train_loss: 1.00 took: 2.32s\n",
            "Epoch 6, Batch_no 64, 51% \t train_loss: 1.05 took: 2.04s\n",
            "Epoch 6, Batch_no 77, 61% \t train_loss: 1.00 took: 2.96s\n",
            "Epoch 6, Batch_no 90, 72% \t train_loss: 1.03 took: 2.18s\n",
            "Epoch 6, Batch_no 103, 82% \t train_loss: 0.97 took: 2.06s\n",
            "Epoch 6, Batch_no 116, 92% \t train_loss: 1.15 took: 2.88s\n",
            "Training accuracy: 60 %\n",
            "Validation accuracy: 70 %\n",
            "Validation loss = 0.76\n",
            "Epoch 7, Batch_no 12, 10% \t train_loss: 1.07 took: 2.97s\n",
            "Epoch 7, Batch_no 25, 20% \t train_loss: 1.04 took: 2.33s\n",
            "Epoch 7, Batch_no 38, 30% \t train_loss: 0.98 took: 2.21s\n",
            "Epoch 7, Batch_no 51, 41% \t train_loss: 0.92 took: 2.42s\n",
            "Epoch 7, Batch_no 64, 51% \t train_loss: 1.06 took: 2.68s\n",
            "Epoch 7, Batch_no 77, 61% \t train_loss: 1.04 took: 3.01s\n",
            "Epoch 7, Batch_no 90, 72% \t train_loss: 1.01 took: 2.22s\n",
            "Epoch 7, Batch_no 103, 82% \t train_loss: 0.96 took: 2.20s\n",
            "Epoch 7, Batch_no 116, 92% \t train_loss: 0.98 took: 1.96s\n",
            "Training accuracy: 61 %\n",
            "Validation accuracy: 64 %\n",
            "Validation loss = 0.78\n",
            "Epoch 8, Batch_no 12, 10% \t train_loss: 0.97 took: 3.04s\n",
            "Epoch 8, Batch_no 25, 20% \t train_loss: 0.87 took: 2.22s\n",
            "Epoch 8, Batch_no 38, 30% \t train_loss: 1.01 took: 2.51s\n",
            "Epoch 8, Batch_no 51, 41% \t train_loss: 1.03 took: 2.60s\n",
            "Epoch 8, Batch_no 64, 51% \t train_loss: 1.14 took: 2.21s\n",
            "Epoch 8, Batch_no 77, 61% \t train_loss: 0.98 took: 2.15s\n",
            "Epoch 8, Batch_no 90, 72% \t train_loss: 1.09 took: 2.47s\n",
            "Epoch 8, Batch_no 103, 82% \t train_loss: 1.04 took: 2.91s\n",
            "Epoch 8, Batch_no 116, 92% \t train_loss: 1.07 took: 2.07s\n",
            "Training accuracy: 62 %\n",
            "Validation accuracy: 62 %\n",
            "Validation loss = 0.87\n",
            "Epoch 9, Batch_no 12, 10% \t train_loss: 1.00 took: 2.93s\n",
            "Epoch 9, Batch_no 25, 20% \t train_loss: 0.92 took: 2.37s\n",
            "Epoch 9, Batch_no 38, 30% \t train_loss: 1.08 took: 2.75s\n",
            "Epoch 9, Batch_no 51, 41% \t train_loss: 0.87 took: 2.00s\n",
            "Epoch 9, Batch_no 64, 51% \t train_loss: 0.97 took: 2.31s\n",
            "Epoch 9, Batch_no 77, 61% \t train_loss: 1.13 took: 2.76s\n",
            "Epoch 9, Batch_no 90, 72% \t train_loss: 0.89 took: 2.28s\n",
            "Epoch 9, Batch_no 103, 82% \t train_loss: 0.82 took: 2.87s\n",
            "Epoch 9, Batch_no 116, 92% \t train_loss: 1.03 took: 2.38s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 71 %\n",
            "Validation loss = 0.79\n",
            "Epoch 10, Batch_no 12, 10% \t train_loss: 0.99 took: 2.72s\n",
            "Epoch 10, Batch_no 25, 20% \t train_loss: 0.95 took: 2.77s\n",
            "Epoch 10, Batch_no 38, 30% \t train_loss: 0.95 took: 2.01s\n",
            "Epoch 10, Batch_no 51, 41% \t train_loss: 0.87 took: 2.33s\n",
            "Epoch 10, Batch_no 64, 51% \t train_loss: 0.94 took: 2.32s\n",
            "Epoch 10, Batch_no 77, 61% \t train_loss: 1.12 took: 2.49s\n",
            "Epoch 10, Batch_no 90, 72% \t train_loss: 1.04 took: 2.75s\n",
            "Epoch 10, Batch_no 103, 82% \t train_loss: 1.03 took: 2.24s\n",
            "Epoch 10, Batch_no 116, 92% \t train_loss: 0.92 took: 2.37s\n",
            "Training accuracy: 63 %\n",
            "Validation accuracy: 67 %\n",
            "Validation loss = 0.78\n",
            "Epoch 11, Batch_no 12, 10% \t train_loss: 0.89 took: 2.67s\n",
            "Epoch 11, Batch_no 25, 20% \t train_loss: 0.94 took: 2.64s\n",
            "Epoch 11, Batch_no 38, 30% \t train_loss: 0.98 took: 2.23s\n",
            "Epoch 11, Batch_no 51, 41% \t train_loss: 0.92 took: 2.24s\n",
            "Epoch 11, Batch_no 64, 51% \t train_loss: 0.98 took: 2.57s\n",
            "Epoch 11, Batch_no 77, 61% \t train_loss: 1.00 took: 2.29s\n",
            "Epoch 11, Batch_no 90, 72% \t train_loss: 0.96 took: 2.66s\n",
            "Epoch 11, Batch_no 103, 82% \t train_loss: 1.02 took: 2.41s\n",
            "Epoch 11, Batch_no 116, 92% \t train_loss: 1.01 took: 2.82s\n",
            "Training accuracy: 64 %\n",
            "Validation accuracy: 69 %\n",
            "Validation loss = 0.74\n",
            "Epoch 12, Batch_no 12, 10% \t train_loss: 0.93 took: 3.14s\n",
            "Epoch 12, Batch_no 25, 20% \t train_loss: 0.98 took: 2.29s\n",
            "Epoch 12, Batch_no 38, 30% \t train_loss: 1.01 took: 2.35s\n",
            "Epoch 12, Batch_no 51, 41% \t train_loss: 0.77 took: 2.13s\n",
            "Epoch 12, Batch_no 64, 51% \t train_loss: 1.00 took: 2.44s\n",
            "Epoch 12, Batch_no 77, 61% \t train_loss: 0.99 took: 2.24s\n",
            "Epoch 12, Batch_no 90, 72% \t train_loss: 0.79 took: 2.76s\n",
            "Epoch 12, Batch_no 103, 82% \t train_loss: 0.96 took: 2.20s\n",
            "Epoch 12, Batch_no 116, 92% \t train_loss: 0.91 took: 2.91s\n",
            "Training accuracy: 65 %\n",
            "Validation accuracy: 66 %\n",
            "Validation loss = 0.78\n",
            "Epoch 13, Batch_no 12, 10% \t train_loss: 0.87 took: 2.97s\n",
            "Epoch 13, Batch_no 25, 20% \t train_loss: 0.86 took: 2.89s\n",
            "Epoch 13, Batch_no 38, 30% \t train_loss: 0.95 took: 2.31s\n",
            "Epoch 13, Batch_no 51, 41% \t train_loss: 0.98 took: 2.40s\n",
            "Epoch 13, Batch_no 64, 51% \t train_loss: 0.95 took: 2.05s\n",
            "Epoch 13, Batch_no 77, 61% \t train_loss: 1.01 took: 2.98s\n",
            "Epoch 13, Batch_no 90, 72% \t train_loss: 0.87 took: 2.17s\n",
            "Epoch 13, Batch_no 103, 82% \t train_loss: 1.00 took: 2.24s\n",
            "Epoch 13, Batch_no 116, 92% \t train_loss: 0.83 took: 2.36s\n",
            "Training accuracy: 65 %\n",
            "Validation accuracy: 69 %\n",
            "Validation loss = 0.74\n",
            "Epoch 14, Batch_no 12, 10% \t train_loss: 0.88 took: 3.23s\n",
            "Epoch 14, Batch_no 25, 20% \t train_loss: 0.90 took: 2.53s\n",
            "Epoch 14, Batch_no 38, 30% \t train_loss: 0.84 took: 2.34s\n",
            "Epoch 14, Batch_no 51, 41% \t train_loss: 0.95 took: 2.09s\n",
            "Epoch 14, Batch_no 64, 51% \t train_loss: 0.91 took: 2.67s\n",
            "Epoch 14, Batch_no 77, 61% \t train_loss: 0.87 took: 2.30s\n",
            "Epoch 14, Batch_no 90, 72% \t train_loss: 0.89 took: 2.52s\n",
            "Epoch 14, Batch_no 103, 82% \t train_loss: 0.82 took: 2.24s\n",
            "Epoch 14, Batch_no 116, 92% \t train_loss: 0.85 took: 2.87s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 75 %\n",
            "Validation loss = 0.61\n",
            "Epoch 15, Batch_no 12, 10% \t train_loss: 1.00 took: 3.33s\n",
            "Epoch 15, Batch_no 25, 20% \t train_loss: 0.91 took: 2.27s\n",
            "Epoch 15, Batch_no 38, 30% \t train_loss: 0.87 took: 2.04s\n",
            "Epoch 15, Batch_no 51, 41% \t train_loss: 0.97 took: 2.80s\n",
            "Epoch 15, Batch_no 64, 51% \t train_loss: 0.89 took: 2.55s\n",
            "Epoch 15, Batch_no 77, 61% \t train_loss: 0.90 took: 2.01s\n",
            "Epoch 15, Batch_no 90, 72% \t train_loss: 0.86 took: 2.47s\n",
            "Epoch 15, Batch_no 103, 82% \t train_loss: 0.94 took: 2.76s\n",
            "Epoch 15, Batch_no 116, 92% \t train_loss: 0.98 took: 2.29s\n",
            "Training accuracy: 65 %\n",
            "Validation accuracy: 70 %\n",
            "Validation loss = 0.71\n",
            "Epoch 16, Batch_no 12, 10% \t train_loss: 0.86 took: 3.37s\n",
            "Epoch 16, Batch_no 25, 20% \t train_loss: 0.92 took: 2.29s\n",
            "Epoch 16, Batch_no 38, 30% \t train_loss: 0.83 took: 1.94s\n",
            "Epoch 16, Batch_no 51, 41% \t train_loss: 0.96 took: 2.64s\n",
            "Epoch 16, Batch_no 64, 51% \t train_loss: 0.91 took: 2.40s\n",
            "Epoch 16, Batch_no 77, 61% \t train_loss: 0.80 took: 2.74s\n",
            "Epoch 16, Batch_no 90, 72% \t train_loss: 0.94 took: 2.05s\n",
            "Epoch 16, Batch_no 103, 82% \t train_loss: 0.90 took: 2.24s\n",
            "Epoch 16, Batch_no 116, 92% \t train_loss: 0.82 took: 3.05s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 77 %\n",
            "Validation loss = 0.59\n",
            "Epoch 17, Batch_no 12, 10% \t train_loss: 0.93 took: 3.33s\n",
            "Epoch 17, Batch_no 25, 20% \t train_loss: 0.76 took: 2.61s\n",
            "Epoch 17, Batch_no 38, 30% \t train_loss: 0.88 took: 2.39s\n",
            "Epoch 17, Batch_no 51, 41% \t train_loss: 1.00 took: 2.09s\n",
            "Epoch 17, Batch_no 64, 51% \t train_loss: 0.79 took: 2.99s\n",
            "Epoch 17, Batch_no 77, 61% \t train_loss: 0.78 took: 1.98s\n",
            "Epoch 17, Batch_no 90, 72% \t train_loss: 0.94 took: 2.40s\n",
            "Epoch 17, Batch_no 103, 82% \t train_loss: 0.82 took: 2.02s\n",
            "Epoch 17, Batch_no 116, 92% \t train_loss: 0.85 took: 2.62s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 78 %\n",
            "Validation loss = 0.55\n",
            "Epoch 18, Batch_no 12, 10% \t train_loss: 0.91 took: 3.17s\n",
            "Epoch 18, Batch_no 25, 20% \t train_loss: 0.96 took: 2.57s\n",
            "Epoch 18, Batch_no 38, 30% \t train_loss: 0.78 took: 2.05s\n",
            "Epoch 18, Batch_no 51, 41% \t train_loss: 1.02 took: 2.31s\n",
            "Epoch 18, Batch_no 64, 51% \t train_loss: 0.88 took: 2.45s\n",
            "Epoch 18, Batch_no 77, 61% \t train_loss: 0.83 took: 2.48s\n",
            "Epoch 18, Batch_no 90, 72% \t train_loss: 0.91 took: 2.79s\n",
            "Epoch 18, Batch_no 103, 82% \t train_loss: 0.81 took: 2.38s\n",
            "Epoch 18, Batch_no 116, 92% \t train_loss: 0.91 took: 2.07s\n",
            "Training accuracy: 66 %\n",
            "Validation accuracy: 77 %\n",
            "Validation loss = 0.56\n",
            "Epoch 19, Batch_no 12, 10% \t train_loss: 0.73 took: 3.27s\n",
            "Epoch 19, Batch_no 25, 20% \t train_loss: 0.89 took: 2.19s\n",
            "Epoch 19, Batch_no 38, 30% \t train_loss: 0.87 took: 2.27s\n",
            "Epoch 19, Batch_no 51, 41% \t train_loss: 0.90 took: 2.39s\n",
            "Epoch 19, Batch_no 64, 51% \t train_loss: 0.70 took: 2.51s\n",
            "Epoch 19, Batch_no 77, 61% \t train_loss: 0.87 took: 2.04s\n",
            "Epoch 19, Batch_no 90, 72% \t train_loss: 0.91 took: 2.47s\n",
            "Epoch 19, Batch_no 103, 82% \t train_loss: 0.88 took: 2.44s\n",
            "Epoch 19, Batch_no 116, 92% \t train_loss: 0.92 took: 2.93s\n",
            "Training accuracy: 67 %\n",
            "Validation accuracy: 80 %\n",
            "Validation loss = 0.55\n",
            "Epoch 20, Batch_no 12, 10% \t train_loss: 0.85 took: 3.26s\n",
            "Epoch 20, Batch_no 25, 20% \t train_loss: 0.85 took: 2.09s\n",
            "Epoch 20, Batch_no 38, 30% \t train_loss: 0.89 took: 2.47s\n",
            "Epoch 20, Batch_no 51, 41% \t train_loss: 0.75 took: 2.34s\n",
            "Epoch 20, Batch_no 64, 51% \t train_loss: 0.85 took: 2.70s\n",
            "Epoch 20, Batch_no 77, 61% \t train_loss: 0.84 took: 2.50s\n",
            "Epoch 20, Batch_no 90, 72% \t train_loss: 0.78 took: 2.37s\n",
            "Epoch 20, Batch_no 103, 82% \t train_loss: 0.86 took: 2.35s\n",
            "Epoch 20, Batch_no 116, 92% \t train_loss: 0.83 took: 2.54s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 76 %\n",
            "Validation loss = 0.64\n",
            "Epoch 21, Batch_no 12, 10% \t train_loss: 0.84 took: 3.03s\n",
            "Epoch 21, Batch_no 25, 20% \t train_loss: 0.81 took: 2.32s\n",
            "Epoch 21, Batch_no 38, 30% \t train_loss: 0.82 took: 2.66s\n",
            "Epoch 21, Batch_no 51, 41% \t train_loss: 0.84 took: 2.27s\n",
            "Epoch 21, Batch_no 64, 51% \t train_loss: 0.83 took: 2.91s\n",
            "Epoch 21, Batch_no 77, 61% \t train_loss: 0.83 took: 1.96s\n",
            "Epoch 21, Batch_no 90, 72% \t train_loss: 0.91 took: 2.15s\n",
            "Epoch 21, Batch_no 103, 82% \t train_loss: 0.81 took: 2.38s\n",
            "Epoch 21, Batch_no 116, 92% \t train_loss: 0.81 took: 2.74s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.53\n",
            "Epoch 22, Batch_no 12, 10% \t train_loss: 0.85 took: 3.16s\n",
            "Epoch 22, Batch_no 25, 20% \t train_loss: 0.78 took: 2.21s\n",
            "Epoch 22, Batch_no 38, 30% \t train_loss: 0.90 took: 2.33s\n",
            "Epoch 22, Batch_no 51, 41% \t train_loss: 0.89 took: 2.33s\n",
            "Epoch 22, Batch_no 64, 51% \t train_loss: 0.79 took: 2.26s\n",
            "Epoch 22, Batch_no 77, 61% \t train_loss: 0.76 took: 3.02s\n",
            "Epoch 22, Batch_no 90, 72% \t train_loss: 0.79 took: 1.95s\n",
            "Epoch 22, Batch_no 103, 82% \t train_loss: 0.78 took: 2.56s\n",
            "Epoch 22, Batch_no 116, 92% \t train_loss: 0.82 took: 2.42s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 77 %\n",
            "Validation loss = 0.55\n",
            "Epoch 23, Batch_no 12, 10% \t train_loss: 0.80 took: 3.21s\n",
            "Epoch 23, Batch_no 25, 20% \t train_loss: 0.83 took: 1.99s\n",
            "Epoch 23, Batch_no 38, 30% \t train_loss: 0.84 took: 2.24s\n",
            "Epoch 23, Batch_no 51, 41% \t train_loss: 0.71 took: 2.69s\n",
            "Epoch 23, Batch_no 64, 51% \t train_loss: 0.86 took: 2.24s\n",
            "Epoch 23, Batch_no 77, 61% \t train_loss: 0.89 took: 2.54s\n",
            "Epoch 23, Batch_no 90, 72% \t train_loss: 0.83 took: 2.15s\n",
            "Epoch 23, Batch_no 103, 82% \t train_loss: 0.88 took: 2.31s\n",
            "Epoch 23, Batch_no 116, 92% \t train_loss: 0.81 took: 3.02s\n",
            "Training accuracy: 69 %\n",
            "Validation accuracy: 81 %\n",
            "Validation loss = 0.50\n",
            "Epoch 24, Batch_no 12, 10% \t train_loss: 0.83 took: 3.37s\n",
            "Epoch 24, Batch_no 25, 20% \t train_loss: 0.76 took: 2.46s\n",
            "Epoch 24, Batch_no 38, 30% \t train_loss: 0.79 took: 2.28s\n",
            "Epoch 24, Batch_no 51, 41% \t train_loss: 0.79 took: 2.14s\n",
            "Epoch 24, Batch_no 64, 51% \t train_loss: 0.79 took: 2.24s\n",
            "Epoch 24, Batch_no 77, 61% \t train_loss: 0.85 took: 2.85s\n",
            "Epoch 24, Batch_no 90, 72% \t train_loss: 0.79 took: 2.26s\n",
            "Epoch 24, Batch_no 103, 82% \t train_loss: 0.80 took: 2.45s\n",
            "Epoch 24, Batch_no 116, 92% \t train_loss: 0.73 took: 2.20s\n",
            "Training accuracy: 70 %\n",
            "Validation accuracy: 78 %\n",
            "Validation loss = 0.53\n",
            "Epoch 25, Batch_no 12, 10% \t train_loss: 0.75 took: 3.10s\n",
            "Epoch 25, Batch_no 25, 20% \t train_loss: 0.73 took: 2.18s\n",
            "Epoch 25, Batch_no 38, 30% \t train_loss: 0.72 took: 2.32s\n",
            "Epoch 25, Batch_no 51, 41% \t train_loss: 0.77 took: 2.12s\n",
            "Epoch 25, Batch_no 64, 51% \t train_loss: 0.82 took: 3.24s\n",
            "Epoch 25, Batch_no 77, 61% \t train_loss: 0.85 took: 2.10s\n",
            "Epoch 25, Batch_no 90, 72% \t train_loss: 0.87 took: 2.26s\n",
            "Epoch 25, Batch_no 103, 82% \t train_loss: 0.78 took: 2.16s\n",
            "Epoch 25, Batch_no 116, 92% \t train_loss: 0.81 took: 2.97s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 75 %\n",
            "Validation loss = 0.65\n",
            "Epoch 26, Batch_no 12, 10% \t train_loss: 0.72 took: 3.44s\n",
            "Epoch 26, Batch_no 25, 20% \t train_loss: 0.79 took: 2.55s\n",
            "Epoch 26, Batch_no 38, 30% \t train_loss: 0.68 took: 2.11s\n",
            "Epoch 26, Batch_no 51, 41% \t train_loss: 0.74 took: 2.15s\n",
            "Epoch 26, Batch_no 64, 51% \t train_loss: 0.73 took: 2.54s\n",
            "Epoch 26, Batch_no 77, 61% \t train_loss: 0.77 took: 2.19s\n",
            "Epoch 26, Batch_no 90, 72% \t train_loss: 0.76 took: 2.70s\n",
            "Epoch 26, Batch_no 103, 82% \t train_loss: 0.88 took: 1.87s\n",
            "Epoch 26, Batch_no 116, 92% \t train_loss: 0.79 took: 2.38s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 81 %\n",
            "Validation loss = 0.46\n",
            "Epoch 27, Batch_no 12, 10% \t train_loss: 0.71 took: 2.77s\n",
            "Epoch 27, Batch_no 25, 20% \t train_loss: 0.81 took: 2.85s\n",
            "Epoch 27, Batch_no 38, 30% \t train_loss: 0.71 took: 2.30s\n",
            "Epoch 27, Batch_no 51, 41% \t train_loss: 0.73 took: 2.24s\n",
            "Epoch 27, Batch_no 64, 51% \t train_loss: 0.78 took: 2.22s\n",
            "Epoch 27, Batch_no 77, 61% \t train_loss: 0.69 took: 3.20s\n",
            "Epoch 27, Batch_no 90, 72% \t train_loss: 0.68 took: 2.02s\n",
            "Epoch 27, Batch_no 103, 82% \t train_loss: 0.97 took: 1.92s\n",
            "Epoch 27, Batch_no 116, 92% \t train_loss: 0.87 took: 2.23s\n",
            "Training accuracy: 70 %\n",
            "Validation accuracy: 81 %\n",
            "Validation loss = 0.49\n",
            "Epoch 28, Batch_no 12, 10% \t train_loss: 0.78 took: 3.03s\n",
            "Epoch 28, Batch_no 25, 20% \t train_loss: 0.77 took: 3.08s\n",
            "Epoch 28, Batch_no 38, 30% \t train_loss: 0.77 took: 2.31s\n",
            "Epoch 28, Batch_no 51, 41% \t train_loss: 0.79 took: 2.02s\n",
            "Epoch 28, Batch_no 64, 51% \t train_loss: 0.69 took: 2.38s\n",
            "Epoch 28, Batch_no 77, 61% \t train_loss: 0.77 took: 2.14s\n",
            "Epoch 28, Batch_no 90, 72% \t train_loss: 0.64 took: 2.45s\n",
            "Epoch 28, Batch_no 103, 82% \t train_loss: 0.79 took: 2.35s\n",
            "Epoch 28, Batch_no 116, 92% \t train_loss: 0.78 took: 2.12s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 78 %\n",
            "Validation loss = 0.53\n",
            "Epoch 29, Batch_no 12, 10% \t train_loss: 0.78 took: 3.29s\n",
            "Epoch 29, Batch_no 25, 20% \t train_loss: 0.73 took: 2.37s\n",
            "Epoch 29, Batch_no 38, 30% \t train_loss: 0.80 took: 2.20s\n",
            "Epoch 29, Batch_no 51, 41% \t train_loss: 0.80 took: 2.38s\n",
            "Epoch 29, Batch_no 64, 51% \t train_loss: 0.76 took: 2.34s\n",
            "Epoch 29, Batch_no 77, 61% \t train_loss: 0.72 took: 2.27s\n",
            "Epoch 29, Batch_no 90, 72% \t train_loss: 0.67 took: 2.56s\n",
            "Epoch 29, Batch_no 103, 82% \t train_loss: 0.76 took: 2.14s\n",
            "Epoch 29, Batch_no 116, 92% \t train_loss: 0.71 took: 2.84s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.50\n",
            "Epoch 30, Batch_no 12, 10% \t train_loss: 0.70 took: 2.83s\n",
            "Epoch 30, Batch_no 25, 20% \t train_loss: 0.71 took: 2.55s\n",
            "Epoch 30, Batch_no 38, 30% \t train_loss: 0.83 took: 2.25s\n",
            "Epoch 30, Batch_no 51, 41% \t train_loss: 0.64 took: 2.60s\n",
            "Epoch 30, Batch_no 64, 51% \t train_loss: 0.80 took: 2.39s\n",
            "Epoch 30, Batch_no 77, 61% \t train_loss: 0.96 took: 2.45s\n",
            "Epoch 30, Batch_no 90, 72% \t train_loss: 0.81 took: 2.18s\n",
            "Epoch 30, Batch_no 103, 82% \t train_loss: 0.72 took: 2.50s\n",
            "Epoch 30, Batch_no 116, 92% \t train_loss: 0.71 took: 2.56s\n",
            "Training accuracy: 72 %\n",
            "Validation accuracy: 80 %\n",
            "Validation loss = 0.49\n",
            "Epoch 31, Batch_no 12, 10% \t train_loss: 0.75 took: 2.72s\n",
            "Epoch 31, Batch_no 25, 20% \t train_loss: 0.81 took: 2.25s\n",
            "Epoch 31, Batch_no 38, 30% \t train_loss: 0.71 took: 2.59s\n",
            "Epoch 31, Batch_no 51, 41% \t train_loss: 0.65 took: 2.20s\n",
            "Epoch 31, Batch_no 64, 51% \t train_loss: 0.86 took: 2.52s\n",
            "Epoch 31, Batch_no 77, 61% \t train_loss: 0.74 took: 2.75s\n",
            "Epoch 31, Batch_no 90, 72% \t train_loss: 0.83 took: 2.40s\n",
            "Epoch 31, Batch_no 103, 82% \t train_loss: 0.72 took: 2.34s\n",
            "Epoch 31, Batch_no 116, 92% \t train_loss: 0.83 took: 1.92s\n",
            "Training accuracy: 71 %\n",
            "Validation accuracy: 80 %\n",
            "Validation loss = 0.49\n",
            "Epoch 32, Batch_no 12, 10% \t train_loss: 0.67 took: 2.98s\n",
            "Epoch 32, Batch_no 25, 20% \t train_loss: 0.70 took: 2.12s\n",
            "Epoch 32, Batch_no 38, 30% \t train_loss: 0.76 took: 2.88s\n",
            "Epoch 32, Batch_no 51, 41% \t train_loss: 0.83 took: 2.65s\n",
            "Epoch 32, Batch_no 64, 51% \t train_loss: 0.76 took: 1.91s\n",
            "Epoch 32, Batch_no 77, 61% \t train_loss: 0.82 took: 1.99s\n",
            "Epoch 32, Batch_no 90, 72% \t train_loss: 0.82 took: 2.84s\n",
            "Epoch 32, Batch_no 103, 82% \t train_loss: 0.65 took: 2.25s\n",
            "Epoch 32, Batch_no 116, 92% \t train_loss: 0.57 took: 2.20s\n",
            "Training accuracy: 72 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.52\n",
            "Epoch 33, Batch_no 12, 10% \t train_loss: 0.58 took: 3.06s\n",
            "Epoch 33, Batch_no 25, 20% \t train_loss: 0.75 took: 2.46s\n",
            "Epoch 33, Batch_no 38, 30% \t train_loss: 0.78 took: 2.73s\n",
            "Epoch 33, Batch_no 51, 41% \t train_loss: 0.70 took: 2.46s\n",
            "Epoch 33, Batch_no 64, 51% \t train_loss: 0.67 took: 2.09s\n",
            "Epoch 33, Batch_no 77, 61% \t train_loss: 0.82 took: 2.08s\n",
            "Epoch 33, Batch_no 90, 72% \t train_loss: 0.75 took: 2.68s\n",
            "Epoch 33, Batch_no 103, 82% \t train_loss: 0.74 took: 2.35s\n",
            "Epoch 33, Batch_no 116, 92% \t train_loss: 0.83 took: 2.25s\n",
            "Training accuracy: 72 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.43\n",
            "Epoch 34, Batch_no 12, 10% \t train_loss: 0.70 took: 2.90s\n",
            "Epoch 34, Batch_no 25, 20% \t train_loss: 0.62 took: 2.55s\n",
            "Epoch 34, Batch_no 38, 30% \t train_loss: 0.70 took: 2.46s\n",
            "Epoch 34, Batch_no 51, 41% \t train_loss: 0.63 took: 2.04s\n",
            "Epoch 34, Batch_no 64, 51% \t train_loss: 0.72 took: 2.92s\n",
            "Epoch 34, Batch_no 77, 61% \t train_loss: 0.69 took: 2.07s\n",
            "Epoch 34, Batch_no 90, 72% \t train_loss: 0.76 took: 2.26s\n",
            "Epoch 34, Batch_no 103, 82% \t train_loss: 0.69 took: 2.22s\n",
            "Epoch 34, Batch_no 116, 92% \t train_loss: 0.80 took: 2.71s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.50\n",
            "Epoch 35, Batch_no 12, 10% \t train_loss: 0.68 took: 3.01s\n",
            "Epoch 35, Batch_no 25, 20% \t train_loss: 0.74 took: 2.28s\n",
            "Epoch 35, Batch_no 38, 30% \t train_loss: 0.74 took: 2.52s\n",
            "Epoch 35, Batch_no 51, 41% \t train_loss: 0.66 took: 2.20s\n",
            "Epoch 35, Batch_no 64, 51% \t train_loss: 0.68 took: 2.28s\n",
            "Epoch 35, Batch_no 77, 61% \t train_loss: 0.73 took: 2.81s\n",
            "Epoch 35, Batch_no 90, 72% \t train_loss: 0.71 took: 2.44s\n",
            "Epoch 35, Batch_no 103, 82% \t train_loss: 0.76 took: 2.23s\n",
            "Epoch 35, Batch_no 116, 92% \t train_loss: 0.66 took: 2.25s\n",
            "Training accuracy: 74 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.46\n",
            "Epoch 36, Batch_no 12, 10% \t train_loss: 0.68 took: 3.23s\n",
            "Epoch 36, Batch_no 25, 20% \t train_loss: 0.68 took: 2.45s\n",
            "Epoch 36, Batch_no 38, 30% \t train_loss: 0.62 took: 2.39s\n",
            "Epoch 36, Batch_no 51, 41% \t train_loss: 0.74 took: 2.50s\n",
            "Epoch 36, Batch_no 64, 51% \t train_loss: 0.68 took: 2.26s\n",
            "Epoch 36, Batch_no 77, 61% \t train_loss: 0.80 took: 2.64s\n",
            "Epoch 36, Batch_no 90, 72% \t train_loss: 0.69 took: 2.05s\n",
            "Epoch 36, Batch_no 103, 82% \t train_loss: 0.74 took: 2.66s\n",
            "Epoch 36, Batch_no 116, 92% \t train_loss: 0.72 took: 2.08s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.42\n",
            "Epoch 37, Batch_no 12, 10% \t train_loss: 0.73 took: 3.12s\n",
            "Epoch 37, Batch_no 25, 20% \t train_loss: 0.78 took: 1.94s\n",
            "Epoch 37, Batch_no 38, 30% \t train_loss: 0.71 took: 2.26s\n",
            "Epoch 37, Batch_no 51, 41% \t train_loss: 0.78 took: 2.70s\n",
            "Epoch 37, Batch_no 64, 51% \t train_loss: 0.63 took: 2.46s\n",
            "Epoch 37, Batch_no 77, 61% \t train_loss: 0.69 took: 1.99s\n",
            "Epoch 37, Batch_no 90, 72% \t train_loss: 0.56 took: 2.90s\n",
            "Epoch 37, Batch_no 103, 82% \t train_loss: 0.80 took: 2.32s\n",
            "Epoch 37, Batch_no 116, 92% \t train_loss: 0.79 took: 2.46s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.39\n",
            "Epoch 38, Batch_no 12, 10% \t train_loss: 0.71 took: 3.11s\n",
            "Epoch 38, Batch_no 25, 20% \t train_loss: 0.63 took: 2.45s\n",
            "Epoch 38, Batch_no 38, 30% \t train_loss: 0.76 took: 2.30s\n",
            "Epoch 38, Batch_no 51, 41% \t train_loss: 0.72 took: 2.37s\n",
            "Epoch 38, Batch_no 64, 51% \t train_loss: 0.68 took: 2.16s\n",
            "Epoch 38, Batch_no 77, 61% \t train_loss: 0.65 took: 2.88s\n",
            "Epoch 38, Batch_no 90, 72% \t train_loss: 0.66 took: 2.09s\n",
            "Epoch 38, Batch_no 103, 82% \t train_loss: 0.72 took: 2.15s\n",
            "Epoch 38, Batch_no 116, 92% \t train_loss: 0.75 took: 2.32s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.51\n",
            "Epoch 39, Batch_no 12, 10% \t train_loss: 0.66 took: 3.04s\n",
            "Epoch 39, Batch_no 25, 20% \t train_loss: 0.63 took: 2.54s\n",
            "Epoch 39, Batch_no 38, 30% \t train_loss: 0.77 took: 2.16s\n",
            "Epoch 39, Batch_no 51, 41% \t train_loss: 0.74 took: 2.44s\n",
            "Epoch 39, Batch_no 64, 51% \t train_loss: 0.78 took: 2.32s\n",
            "Epoch 39, Batch_no 77, 61% \t train_loss: 0.76 took: 2.97s\n",
            "Epoch 39, Batch_no 90, 72% \t train_loss: 0.80 took: 2.10s\n",
            "Epoch 39, Batch_no 103, 82% \t train_loss: 0.74 took: 2.49s\n",
            "Epoch 39, Batch_no 116, 92% \t train_loss: 0.70 took: 2.02s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.43\n",
            "Epoch 40, Batch_no 12, 10% \t train_loss: 0.74 took: 2.90s\n",
            "Epoch 40, Batch_no 25, 20% \t train_loss: 0.82 took: 2.67s\n",
            "Epoch 40, Batch_no 38, 30% \t train_loss: 0.75 took: 2.35s\n",
            "Epoch 40, Batch_no 51, 41% \t train_loss: 0.68 took: 2.01s\n",
            "Epoch 40, Batch_no 64, 51% \t train_loss: 0.65 took: 2.34s\n",
            "Epoch 40, Batch_no 77, 61% \t train_loss: 0.74 took: 2.60s\n",
            "Epoch 40, Batch_no 90, 72% \t train_loss: 0.75 took: 1.93s\n",
            "Epoch 40, Batch_no 103, 82% \t train_loss: 0.62 took: 2.37s\n",
            "Epoch 40, Batch_no 116, 92% \t train_loss: 0.76 took: 2.30s\n",
            "Training accuracy: 73 %\n",
            "Validation accuracy: 82 %\n",
            "Validation loss = 0.42\n",
            "Epoch 41, Batch_no 12, 10% \t train_loss: 0.81 took: 3.09s\n",
            "Epoch 41, Batch_no 25, 20% \t train_loss: 0.79 took: 2.23s\n",
            "Epoch 41, Batch_no 38, 30% \t train_loss: 0.72 took: 2.42s\n",
            "Epoch 41, Batch_no 51, 41% \t train_loss: 0.60 took: 2.28s\n",
            "Epoch 41, Batch_no 64, 51% \t train_loss: 0.69 took: 2.30s\n",
            "Epoch 41, Batch_no 77, 61% \t train_loss: 0.54 took: 3.24s\n",
            "Epoch 41, Batch_no 90, 72% \t train_loss: 0.81 took: 2.29s\n",
            "Epoch 41, Batch_no 103, 82% \t train_loss: 0.74 took: 2.27s\n",
            "Epoch 41, Batch_no 116, 92% \t train_loss: 0.51 took: 2.02s\n",
            "Training accuracy: 74 %\n",
            "Validation accuracy: 81 %\n",
            "Validation loss = 0.49\n",
            "Epoch 42, Batch_no 12, 10% \t train_loss: 0.77 took: 3.38s\n",
            "Epoch 42, Batch_no 25, 20% \t train_loss: 0.70 took: 1.87s\n",
            "Epoch 42, Batch_no 38, 30% \t train_loss: 0.64 took: 2.50s\n",
            "Epoch 42, Batch_no 51, 41% \t train_loss: 0.68 took: 2.11s\n",
            "Epoch 42, Batch_no 64, 51% \t train_loss: 0.73 took: 2.60s\n",
            "Epoch 42, Batch_no 77, 61% \t train_loss: 0.54 took: 2.36s\n",
            "Epoch 42, Batch_no 90, 72% \t train_loss: 0.68 took: 2.95s\n",
            "Epoch 42, Batch_no 103, 82% \t train_loss: 0.66 took: 2.02s\n",
            "Epoch 42, Batch_no 116, 92% \t train_loss: 0.70 took: 2.23s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 85 %\n",
            "Validation loss = 0.39\n",
            "Epoch 43, Batch_no 12, 10% \t train_loss: 0.77 took: 2.78s\n",
            "Epoch 43, Batch_no 25, 20% \t train_loss: 0.74 took: 2.13s\n",
            "Epoch 43, Batch_no 38, 30% \t train_loss: 0.66 took: 2.48s\n",
            "Epoch 43, Batch_no 51, 41% \t train_loss: 0.55 took: 2.37s\n",
            "Epoch 43, Batch_no 64, 51% \t train_loss: 0.75 took: 2.71s\n",
            "Epoch 43, Batch_no 77, 61% \t train_loss: 0.82 took: 2.51s\n",
            "Epoch 43, Batch_no 90, 72% \t train_loss: 0.77 took: 2.35s\n",
            "Epoch 43, Batch_no 103, 82% \t train_loss: 0.70 took: 2.40s\n",
            "Epoch 43, Batch_no 116, 92% \t train_loss: 0.74 took: 2.31s\n",
            "Training accuracy: 72 %\n",
            "Validation accuracy: 85 %\n",
            "Validation loss = 0.37\n",
            "Epoch 44, Batch_no 12, 10% \t train_loss: 0.61 took: 3.03s\n",
            "Epoch 44, Batch_no 25, 20% \t train_loss: 0.63 took: 1.98s\n",
            "Epoch 44, Batch_no 38, 30% \t train_loss: 0.75 took: 2.66s\n",
            "Epoch 44, Batch_no 51, 41% \t train_loss: 0.70 took: 1.97s\n",
            "Epoch 44, Batch_no 64, 51% \t train_loss: 0.75 took: 2.44s\n",
            "Epoch 44, Batch_no 77, 61% \t train_loss: 0.62 took: 2.49s\n",
            "Epoch 44, Batch_no 90, 72% \t train_loss: 0.72 took: 2.44s\n",
            "Epoch 44, Batch_no 103, 82% \t train_loss: 0.71 took: 2.25s\n",
            "Epoch 44, Batch_no 116, 92% \t train_loss: 0.69 took: 2.99s\n",
            "Training accuracy: 74 %\n",
            "Validation accuracy: 82 %\n",
            "Validation loss = 0.42\n",
            "Epoch 45, Batch_no 12, 10% \t train_loss: 0.61 took: 3.19s\n",
            "Epoch 45, Batch_no 25, 20% \t train_loss: 0.62 took: 2.01s\n",
            "Epoch 45, Batch_no 38, 30% \t train_loss: 0.62 took: 2.24s\n",
            "Epoch 45, Batch_no 51, 41% \t train_loss: 0.78 took: 2.53s\n",
            "Epoch 45, Batch_no 64, 51% \t train_loss: 0.65 took: 2.81s\n",
            "Epoch 45, Batch_no 77, 61% \t train_loss: 0.66 took: 2.32s\n",
            "Epoch 45, Batch_no 90, 72% \t train_loss: 0.63 took: 2.30s\n",
            "Epoch 45, Batch_no 103, 82% \t train_loss: 0.72 took: 2.24s\n",
            "Epoch 45, Batch_no 116, 92% \t train_loss: 0.71 took: 2.96s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.40\n",
            "Epoch 46, Batch_no 12, 10% \t train_loss: 0.70 took: 3.10s\n",
            "Epoch 46, Batch_no 25, 20% \t train_loss: 0.58 took: 2.07s\n",
            "Epoch 46, Batch_no 38, 30% \t train_loss: 0.66 took: 2.37s\n",
            "Epoch 46, Batch_no 51, 41% \t train_loss: 0.63 took: 2.17s\n",
            "Epoch 46, Batch_no 64, 51% \t train_loss: 0.60 took: 2.43s\n",
            "Epoch 46, Batch_no 77, 61% \t train_loss: 0.63 took: 2.60s\n",
            "Epoch 46, Batch_no 90, 72% \t train_loss: 0.70 took: 2.73s\n",
            "Epoch 46, Batch_no 103, 82% \t train_loss: 0.78 took: 2.27s\n",
            "Epoch 46, Batch_no 116, 92% \t train_loss: 0.68 took: 2.41s\n",
            "Training accuracy: 74 %\n",
            "Validation accuracy: 79 %\n",
            "Validation loss = 0.54\n",
            "Epoch 47, Batch_no 12, 10% \t train_loss: 0.80 took: 3.06s\n",
            "Epoch 47, Batch_no 25, 20% \t train_loss: 0.67 took: 2.82s\n",
            "Epoch 47, Batch_no 38, 30% \t train_loss: 0.63 took: 2.14s\n",
            "Epoch 47, Batch_no 51, 41% \t train_loss: 0.57 took: 2.04s\n",
            "Epoch 47, Batch_no 64, 51% \t train_loss: 0.63 took: 2.11s\n",
            "Epoch 47, Batch_no 77, 61% \t train_loss: 0.65 took: 2.54s\n",
            "Epoch 47, Batch_no 90, 72% \t train_loss: 0.79 took: 2.16s\n",
            "Epoch 47, Batch_no 103, 82% \t train_loss: 0.83 took: 2.48s\n",
            "Epoch 47, Batch_no 116, 92% \t train_loss: 0.60 took: 2.40s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 84 %\n",
            "Validation loss = 0.41\n",
            "Epoch 48, Batch_no 12, 10% \t train_loss: 0.58 took: 3.09s\n",
            "Epoch 48, Batch_no 25, 20% \t train_loss: 0.60 took: 2.67s\n",
            "Epoch 48, Batch_no 38, 30% \t train_loss: 0.81 took: 2.22s\n",
            "Epoch 48, Batch_no 51, 41% \t train_loss: 0.71 took: 2.06s\n",
            "Epoch 48, Batch_no 64, 51% \t train_loss: 0.68 took: 2.49s\n",
            "Epoch 48, Batch_no 77, 61% \t train_loss: 0.67 took: 2.37s\n",
            "Epoch 48, Batch_no 90, 72% \t train_loss: 0.58 took: 2.30s\n",
            "Epoch 48, Batch_no 103, 82% \t train_loss: 0.65 took: 2.17s\n",
            "Epoch 48, Batch_no 116, 92% \t train_loss: 0.71 took: 2.39s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 81 %\n",
            "Validation loss = 0.45\n",
            "Epoch 49, Batch_no 12, 10% \t train_loss: 0.61 took: 3.72s\n",
            "Epoch 49, Batch_no 25, 20% \t train_loss: 0.57 took: 2.24s\n",
            "Epoch 49, Batch_no 38, 30% \t train_loss: 0.70 took: 2.27s\n",
            "Epoch 49, Batch_no 51, 41% \t train_loss: 0.67 took: 1.99s\n",
            "Epoch 49, Batch_no 64, 51% \t train_loss: 0.70 took: 2.62s\n",
            "Epoch 49, Batch_no 77, 61% \t train_loss: 0.67 took: 2.37s\n",
            "Epoch 49, Batch_no 90, 72% \t train_loss: 0.63 took: 2.29s\n",
            "Epoch 49, Batch_no 103, 82% \t train_loss: 0.63 took: 2.09s\n",
            "Epoch 49, Batch_no 116, 92% \t train_loss: 0.63 took: 2.76s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.38\n",
            "Epoch 50, Batch_no 12, 10% \t train_loss: 0.70 took: 2.98s\n",
            "Epoch 50, Batch_no 25, 20% \t train_loss: 0.63 took: 2.35s\n",
            "Epoch 50, Batch_no 38, 30% \t train_loss: 0.56 took: 2.18s\n",
            "Epoch 50, Batch_no 51, 41% \t train_loss: 0.62 took: 2.37s\n",
            "Epoch 50, Batch_no 64, 51% \t train_loss: 0.56 took: 2.78s\n",
            "Epoch 50, Batch_no 77, 61% \t train_loss: 0.61 took: 2.39s\n",
            "Epoch 50, Batch_no 90, 72% \t train_loss: 0.63 took: 2.06s\n",
            "Epoch 50, Batch_no 103, 82% \t train_loss: 0.69 took: 2.54s\n",
            "Epoch 50, Batch_no 116, 92% \t train_loss: 0.57 took: 2.63s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.41\n",
            "Epoch 51, Batch_no 12, 10% \t train_loss: 0.66 took: 3.09s\n",
            "Epoch 51, Batch_no 25, 20% \t train_loss: 0.68 took: 2.25s\n",
            "Epoch 51, Batch_no 38, 30% \t train_loss: 0.56 took: 2.54s\n",
            "Epoch 51, Batch_no 51, 41% \t train_loss: 0.69 took: 2.19s\n",
            "Epoch 51, Batch_no 64, 51% \t train_loss: 0.55 took: 2.35s\n",
            "Epoch 51, Batch_no 77, 61% \t train_loss: 0.56 took: 2.55s\n",
            "Epoch 51, Batch_no 90, 72% \t train_loss: 0.73 took: 2.16s\n",
            "Epoch 51, Batch_no 103, 82% \t train_loss: 0.75 took: 2.32s\n",
            "Epoch 51, Batch_no 116, 92% \t train_loss: 0.86 took: 3.18s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 82 %\n",
            "Validation loss = 0.42\n",
            "Epoch 52, Batch_no 12, 10% \t train_loss: 0.65 took: 3.46s\n",
            "Epoch 52, Batch_no 25, 20% \t train_loss: 0.63 took: 1.88s\n",
            "Epoch 52, Batch_no 38, 30% \t train_loss: 0.72 took: 2.40s\n",
            "Epoch 52, Batch_no 51, 41% \t train_loss: 0.61 took: 2.07s\n",
            "Epoch 52, Batch_no 64, 51% \t train_loss: 0.56 took: 2.52s\n",
            "Epoch 52, Batch_no 77, 61% \t train_loss: 0.61 took: 2.92s\n",
            "Epoch 52, Batch_no 90, 72% \t train_loss: 0.63 took: 2.27s\n",
            "Epoch 52, Batch_no 103, 82% \t train_loss: 0.58 took: 2.28s\n",
            "Epoch 52, Batch_no 116, 92% \t train_loss: 0.59 took: 2.22s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.37\n",
            "Epoch 53, Batch_no 12, 10% \t train_loss: 0.65 took: 3.49s\n",
            "Epoch 53, Batch_no 25, 20% \t train_loss: 0.65 took: 2.17s\n",
            "Epoch 53, Batch_no 38, 30% \t train_loss: 0.64 took: 2.42s\n",
            "Epoch 53, Batch_no 51, 41% \t train_loss: 0.70 took: 2.28s\n",
            "Epoch 53, Batch_no 64, 51% \t train_loss: 0.71 took: 2.77s\n",
            "Epoch 53, Batch_no 77, 61% \t train_loss: 0.51 took: 2.27s\n",
            "Epoch 53, Batch_no 90, 72% \t train_loss: 0.58 took: 2.30s\n",
            "Epoch 53, Batch_no 103, 82% \t train_loss: 0.75 took: 1.88s\n",
            "Epoch 53, Batch_no 116, 92% \t train_loss: 0.82 took: 2.89s\n",
            "Training accuracy: 75 %\n",
            "Validation accuracy: 84 %\n",
            "Validation loss = 0.43\n",
            "Epoch 54, Batch_no 12, 10% \t train_loss: 0.66 took: 3.37s\n",
            "Epoch 54, Batch_no 25, 20% \t train_loss: 0.52 took: 2.22s\n",
            "Epoch 54, Batch_no 38, 30% \t train_loss: 0.58 took: 2.37s\n",
            "Epoch 54, Batch_no 51, 41% \t train_loss: 0.66 took: 2.65s\n",
            "Epoch 54, Batch_no 64, 51% \t train_loss: 0.66 took: 2.73s\n",
            "Epoch 54, Batch_no 77, 61% \t train_loss: 0.72 took: 2.45s\n",
            "Epoch 54, Batch_no 90, 72% \t train_loss: 0.72 took: 2.11s\n",
            "Epoch 54, Batch_no 103, 82% \t train_loss: 0.60 took: 2.23s\n",
            "Epoch 54, Batch_no 116, 92% \t train_loss: 0.62 took: 2.84s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.33\n",
            "Epoch 55, Batch_no 12, 10% \t train_loss: 0.65 took: 3.53s\n",
            "Epoch 55, Batch_no 25, 20% \t train_loss: 0.68 took: 2.23s\n",
            "Epoch 55, Batch_no 38, 30% \t train_loss: 0.64 took: 2.36s\n",
            "Epoch 55, Batch_no 51, 41% \t train_loss: 0.62 took: 2.19s\n",
            "Epoch 55, Batch_no 64, 51% \t train_loss: 0.62 took: 2.66s\n",
            "Epoch 55, Batch_no 77, 61% \t train_loss: 0.62 took: 2.56s\n",
            "Epoch 55, Batch_no 90, 72% \t train_loss: 0.61 took: 2.21s\n",
            "Epoch 55, Batch_no 103, 82% \t train_loss: 0.61 took: 2.28s\n",
            "Epoch 55, Batch_no 116, 92% \t train_loss: 0.68 took: 2.56s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.37\n",
            "Epoch 56, Batch_no 12, 10% \t train_loss: 0.60 took: 3.03s\n",
            "Epoch 56, Batch_no 25, 20% \t train_loss: 0.63 took: 2.71s\n",
            "Epoch 56, Batch_no 38, 30% \t train_loss: 0.66 took: 2.23s\n",
            "Epoch 56, Batch_no 51, 41% \t train_loss: 0.56 took: 2.25s\n",
            "Epoch 56, Batch_no 64, 51% \t train_loss: 0.62 took: 3.03s\n",
            "Epoch 56, Batch_no 77, 61% \t train_loss: 0.76 took: 2.34s\n",
            "Epoch 56, Batch_no 90, 72% \t train_loss: 0.80 took: 2.15s\n",
            "Epoch 56, Batch_no 103, 82% \t train_loss: 0.65 took: 2.17s\n",
            "Epoch 56, Batch_no 116, 92% \t train_loss: 0.63 took: 2.84s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 82 %\n",
            "Validation loss = 0.45\n",
            "Epoch 57, Batch_no 12, 10% \t train_loss: 0.48 took: 3.22s\n",
            "Epoch 57, Batch_no 25, 20% \t train_loss: 0.53 took: 2.13s\n",
            "Epoch 57, Batch_no 38, 30% \t train_loss: 0.66 took: 2.85s\n",
            "Epoch 57, Batch_no 51, 41% \t train_loss: 0.59 took: 2.29s\n",
            "Epoch 57, Batch_no 64, 51% \t train_loss: 0.65 took: 2.43s\n",
            "Epoch 57, Batch_no 77, 61% \t train_loss: 0.71 took: 2.10s\n",
            "Epoch 57, Batch_no 90, 72% \t train_loss: 0.58 took: 2.96s\n",
            "Epoch 57, Batch_no 103, 82% \t train_loss: 0.70 took: 2.49s\n",
            "Epoch 57, Batch_no 116, 92% \t train_loss: 0.65 took: 2.15s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 83 %\n",
            "Validation loss = 0.41\n",
            "Epoch 58, Batch_no 12, 10% \t train_loss: 0.65 took: 3.12s\n",
            "Epoch 58, Batch_no 25, 20% \t train_loss: 0.64 took: 2.36s\n",
            "Epoch 58, Batch_no 38, 30% \t train_loss: 0.67 took: 2.28s\n",
            "Epoch 58, Batch_no 51, 41% \t train_loss: 0.62 took: 2.32s\n",
            "Epoch 58, Batch_no 64, 51% \t train_loss: 0.63 took: 2.61s\n",
            "Epoch 58, Batch_no 77, 61% \t train_loss: 0.62 took: 2.42s\n",
            "Epoch 58, Batch_no 90, 72% \t train_loss: 0.68 took: 2.29s\n",
            "Epoch 58, Batch_no 103, 82% \t train_loss: 0.60 took: 2.64s\n",
            "Epoch 58, Batch_no 116, 92% \t train_loss: 0.68 took: 2.34s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 82 %\n",
            "Validation loss = 0.42\n",
            "Epoch 59, Batch_no 12, 10% \t train_loss: 0.59 took: 3.13s\n",
            "Epoch 59, Batch_no 25, 20% \t train_loss: 0.79 took: 2.26s\n",
            "Epoch 59, Batch_no 38, 30% \t train_loss: 0.55 took: 2.11s\n",
            "Epoch 59, Batch_no 51, 41% \t train_loss: 0.59 took: 2.48s\n",
            "Epoch 59, Batch_no 64, 51% \t train_loss: 0.68 took: 2.24s\n",
            "Epoch 59, Batch_no 77, 61% \t train_loss: 0.65 took: 2.40s\n",
            "Epoch 59, Batch_no 90, 72% \t train_loss: 0.58 took: 2.74s\n",
            "Epoch 59, Batch_no 103, 82% \t train_loss: 0.53 took: 2.12s\n",
            "Epoch 59, Batch_no 116, 92% \t train_loss: 0.69 took: 2.76s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.35\n",
            "Epoch 60, Batch_no 12, 10% \t train_loss: 0.59 took: 3.39s\n",
            "Epoch 60, Batch_no 25, 20% \t train_loss: 0.55 took: 2.34s\n",
            "Epoch 60, Batch_no 38, 30% \t train_loss: 0.57 took: 2.13s\n",
            "Epoch 60, Batch_no 51, 41% \t train_loss: 0.65 took: 2.19s\n",
            "Epoch 60, Batch_no 64, 51% \t train_loss: 0.65 took: 2.57s\n",
            "Epoch 60, Batch_no 77, 61% \t train_loss: 0.65 took: 2.35s\n",
            "Epoch 60, Batch_no 90, 72% \t train_loss: 0.56 took: 2.58s\n",
            "Epoch 60, Batch_no 103, 82% \t train_loss: 0.65 took: 2.28s\n",
            "Epoch 60, Batch_no 116, 92% \t train_loss: 0.66 took: 2.94s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.39\n",
            "Epoch 61, Batch_no 12, 10% \t train_loss: 0.53 took: 3.28s\n",
            "Epoch 61, Batch_no 25, 20% \t train_loss: 0.61 took: 2.38s\n",
            "Epoch 61, Batch_no 38, 30% \t train_loss: 0.50 took: 2.16s\n",
            "Epoch 61, Batch_no 51, 41% \t train_loss: 0.54 took: 2.07s\n",
            "Epoch 61, Batch_no 64, 51% \t train_loss: 0.69 took: 2.95s\n",
            "Epoch 61, Batch_no 77, 61% \t train_loss: 0.63 took: 2.23s\n",
            "Epoch 61, Batch_no 90, 72% \t train_loss: 0.68 took: 2.03s\n",
            "Epoch 61, Batch_no 103, 82% \t train_loss: 0.67 took: 2.22s\n",
            "Epoch 61, Batch_no 116, 92% \t train_loss: 0.78 took: 2.99s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 89 %\n",
            "Validation loss = 0.29\n",
            "Epoch 62, Batch_no 12, 10% \t train_loss: 0.54 took: 3.57s\n",
            "Epoch 62, Batch_no 25, 20% \t train_loss: 0.64 took: 1.95s\n",
            "Epoch 62, Batch_no 38, 30% \t train_loss: 0.57 took: 2.54s\n",
            "Epoch 62, Batch_no 51, 41% \t train_loss: 0.60 took: 1.94s\n",
            "Epoch 62, Batch_no 64, 51% \t train_loss: 0.64 took: 2.77s\n",
            "Epoch 62, Batch_no 77, 61% \t train_loss: 0.57 took: 2.62s\n",
            "Epoch 62, Batch_no 90, 72% \t train_loss: 0.81 took: 2.10s\n",
            "Epoch 62, Batch_no 103, 82% \t train_loss: 0.63 took: 2.08s\n",
            "Epoch 62, Batch_no 116, 92% \t train_loss: 0.70 took: 2.85s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.33\n",
            "Epoch 63, Batch_no 12, 10% \t train_loss: 0.57 took: 2.77s\n",
            "Epoch 63, Batch_no 25, 20% \t train_loss: 0.59 took: 2.80s\n",
            "Epoch 63, Batch_no 38, 30% \t train_loss: 0.60 took: 2.17s\n",
            "Epoch 63, Batch_no 51, 41% \t train_loss: 0.52 took: 2.41s\n",
            "Epoch 63, Batch_no 64, 51% \t train_loss: 0.63 took: 2.29s\n",
            "Epoch 63, Batch_no 77, 61% \t train_loss: 0.63 took: 2.38s\n",
            "Epoch 63, Batch_no 90, 72% \t train_loss: 0.62 took: 2.93s\n",
            "Epoch 63, Batch_no 103, 82% \t train_loss: 0.59 took: 2.01s\n",
            "Epoch 63, Batch_no 116, 92% \t train_loss: 0.64 took: 3.01s\n",
            "Training accuracy: 78 %\n",
            "Validation accuracy: 85 %\n",
            "Validation loss = 0.42\n",
            "Epoch 64, Batch_no 12, 10% \t train_loss: 0.56 took: 3.23s\n",
            "Epoch 64, Batch_no 25, 20% \t train_loss: 0.60 took: 2.32s\n",
            "Epoch 64, Batch_no 38, 30% \t train_loss: 0.58 took: 2.48s\n",
            "Epoch 64, Batch_no 51, 41% \t train_loss: 0.50 took: 2.31s\n",
            "Epoch 64, Batch_no 64, 51% \t train_loss: 0.69 took: 2.63s\n",
            "Epoch 64, Batch_no 77, 61% \t train_loss: 0.63 took: 2.17s\n",
            "Epoch 64, Batch_no 90, 72% \t train_loss: 0.58 took: 2.22s\n",
            "Epoch 64, Batch_no 103, 82% \t train_loss: 0.72 took: 2.59s\n",
            "Epoch 64, Batch_no 116, 92% \t train_loss: 0.50 took: 2.68s\n",
            "Training accuracy: 79 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.30\n",
            "Epoch 65, Batch_no 12, 10% \t train_loss: 0.57 took: 3.47s\n",
            "Epoch 65, Batch_no 25, 20% \t train_loss: 0.70 took: 2.52s\n",
            "Epoch 65, Batch_no 38, 30% \t train_loss: 0.63 took: 1.89s\n",
            "Epoch 65, Batch_no 51, 41% \t train_loss: 0.60 took: 2.44s\n",
            "Epoch 65, Batch_no 64, 51% \t train_loss: 0.66 took: 2.46s\n",
            "Epoch 65, Batch_no 77, 61% \t train_loss: 0.58 took: 2.33s\n",
            "Epoch 65, Batch_no 90, 72% \t train_loss: 0.57 took: 1.97s\n",
            "Epoch 65, Batch_no 103, 82% \t train_loss: 0.57 took: 2.67s\n",
            "Epoch 65, Batch_no 116, 92% \t train_loss: 0.63 took: 2.62s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 66, Batch_no 12, 10% \t train_loss: 0.66 took: 3.21s\n",
            "Epoch 66, Batch_no 25, 20% \t train_loss: 0.49 took: 2.25s\n",
            "Epoch 66, Batch_no 38, 30% \t train_loss: 0.58 took: 2.52s\n",
            "Epoch 66, Batch_no 51, 41% \t train_loss: 0.60 took: 2.47s\n",
            "Epoch 66, Batch_no 64, 51% \t train_loss: 0.66 took: 2.61s\n",
            "Epoch 66, Batch_no 77, 61% \t train_loss: 0.59 took: 2.45s\n",
            "Epoch 66, Batch_no 90, 72% \t train_loss: 0.55 took: 2.19s\n",
            "Epoch 66, Batch_no 103, 82% \t train_loss: 0.64 took: 2.26s\n",
            "Epoch 66, Batch_no 116, 92% \t train_loss: 0.69 took: 2.61s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 84 %\n",
            "Validation loss = 0.39\n",
            "Epoch 67, Batch_no 12, 10% \t train_loss: 0.64 took: 2.95s\n",
            "Epoch 67, Batch_no 25, 20% \t train_loss: 0.61 took: 2.56s\n",
            "Epoch 67, Batch_no 38, 30% \t train_loss: 0.57 took: 2.54s\n",
            "Epoch 67, Batch_no 51, 41% \t train_loss: 0.68 took: 2.16s\n",
            "Epoch 67, Batch_no 64, 51% \t train_loss: 0.56 took: 2.21s\n",
            "Epoch 67, Batch_no 77, 61% \t train_loss: 0.64 took: 2.58s\n",
            "Epoch 67, Batch_no 90, 72% \t train_loss: 0.70 took: 2.39s\n",
            "Epoch 67, Batch_no 103, 82% \t train_loss: 0.62 took: 2.51s\n",
            "Epoch 67, Batch_no 116, 92% \t train_loss: 0.64 took: 1.94s\n",
            "Training accuracy: 76 %\n",
            "Validation accuracy: 89 %\n",
            "Validation loss = 0.31\n",
            "Epoch 68, Batch_no 12, 10% \t train_loss: 0.57 took: 3.33s\n",
            "Epoch 68, Batch_no 25, 20% \t train_loss: 0.64 took: 2.28s\n",
            "Epoch 68, Batch_no 38, 30% \t train_loss: 0.57 took: 2.43s\n",
            "Epoch 68, Batch_no 51, 41% \t train_loss: 0.60 took: 2.43s\n",
            "Epoch 68, Batch_no 64, 51% \t train_loss: 0.58 took: 2.21s\n",
            "Epoch 68, Batch_no 77, 61% \t train_loss: 0.62 took: 2.69s\n",
            "Epoch 68, Batch_no 90, 72% \t train_loss: 0.64 took: 1.95s\n",
            "Epoch 68, Batch_no 103, 82% \t train_loss: 0.57 took: 2.34s\n",
            "Epoch 68, Batch_no 116, 92% \t train_loss: 0.77 took: 2.36s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.34\n",
            "Epoch 69, Batch_no 12, 10% \t train_loss: 0.55 took: 3.33s\n",
            "Epoch 69, Batch_no 25, 20% \t train_loss: 0.67 took: 2.09s\n",
            "Epoch 69, Batch_no 38, 30% \t train_loss: 0.50 took: 2.15s\n",
            "Epoch 69, Batch_no 51, 41% \t train_loss: 0.65 took: 2.96s\n",
            "Epoch 69, Batch_no 64, 51% \t train_loss: 0.55 took: 2.27s\n",
            "Epoch 69, Batch_no 77, 61% \t train_loss: 0.66 took: 1.97s\n",
            "Epoch 69, Batch_no 90, 72% \t train_loss: 0.58 took: 2.48s\n",
            "Epoch 69, Batch_no 103, 82% \t train_loss: 0.66 took: 2.71s\n",
            "Epoch 69, Batch_no 116, 92% \t train_loss: 0.66 took: 2.23s\n",
            "Training accuracy: 77 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.35\n",
            "Epoch 70, Batch_no 12, 10% \t train_loss: 0.52 took: 3.34s\n",
            "Epoch 70, Batch_no 25, 20% \t train_loss: 0.67 took: 2.06s\n",
            "Epoch 70, Batch_no 38, 30% \t train_loss: 0.68 took: 2.41s\n",
            "Epoch 70, Batch_no 51, 41% \t train_loss: 0.50 took: 2.20s\n",
            "Epoch 70, Batch_no 64, 51% \t train_loss: 0.51 took: 2.68s\n",
            "Epoch 70, Batch_no 77, 61% \t train_loss: 0.54 took: 1.95s\n",
            "Epoch 70, Batch_no 90, 72% \t train_loss: 0.53 took: 2.28s\n",
            "Epoch 70, Batch_no 103, 82% \t train_loss: 0.59 took: 2.40s\n",
            "Epoch 70, Batch_no 116, 92% \t train_loss: 0.65 took: 2.84s\n",
            "Training accuracy: 79 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 71, Batch_no 12, 10% \t train_loss: 0.73 took: 2.96s\n",
            "Epoch 71, Batch_no 25, 20% \t train_loss: 0.55 took: 2.55s\n",
            "Epoch 71, Batch_no 38, 30% \t train_loss: 0.52 took: 2.18s\n",
            "Epoch 71, Batch_no 51, 41% \t train_loss: 0.49 took: 2.25s\n",
            "Epoch 71, Batch_no 64, 51% \t train_loss: 0.54 took: 2.85s\n",
            "Epoch 71, Batch_no 77, 61% \t train_loss: 0.51 took: 2.42s\n",
            "Epoch 71, Batch_no 90, 72% \t train_loss: 0.48 took: 2.02s\n",
            "Epoch 71, Batch_no 103, 82% \t train_loss: 0.50 took: 2.39s\n",
            "Epoch 71, Batch_no 116, 92% \t train_loss: 0.50 took: 2.50s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 72, Batch_no 12, 10% \t train_loss: 0.50 took: 3.60s\n",
            "Epoch 72, Batch_no 25, 20% \t train_loss: 0.47 took: 2.10s\n",
            "Epoch 72, Batch_no 38, 30% \t train_loss: 0.62 took: 1.97s\n",
            "Epoch 72, Batch_no 51, 41% \t train_loss: 0.63 took: 2.50s\n",
            "Epoch 72, Batch_no 64, 51% \t train_loss: 0.49 took: 3.11s\n",
            "Epoch 72, Batch_no 77, 61% \t train_loss: 0.51 took: 2.29s\n",
            "Epoch 72, Batch_no 90, 72% \t train_loss: 0.56 took: 2.10s\n",
            "Epoch 72, Batch_no 103, 82% \t train_loss: 0.50 took: 2.19s\n",
            "Epoch 72, Batch_no 116, 92% \t train_loss: 0.46 took: 2.60s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 85 %\n",
            "Validation loss = 0.31\n",
            "Epoch 73, Batch_no 12, 10% \t train_loss: 0.47 took: 3.28s\n",
            "Epoch 73, Batch_no 25, 20% \t train_loss: 0.47 took: 2.30s\n",
            "Epoch 73, Batch_no 38, 30% \t train_loss: 0.58 took: 2.43s\n",
            "Epoch 73, Batch_no 51, 41% \t train_loss: 0.55 took: 2.31s\n",
            "Epoch 73, Batch_no 64, 51% \t train_loss: 0.53 took: 2.38s\n",
            "Epoch 73, Batch_no 77, 61% \t train_loss: 0.47 took: 2.25s\n",
            "Epoch 73, Batch_no 90, 72% \t train_loss: 0.48 took: 3.07s\n",
            "Epoch 73, Batch_no 103, 82% \t train_loss: 0.50 took: 2.01s\n",
            "Epoch 73, Batch_no 116, 92% \t train_loss: 0.52 took: 2.37s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 74, Batch_no 12, 10% \t train_loss: 0.56 took: 2.86s\n",
            "Epoch 74, Batch_no 25, 20% \t train_loss: 0.55 took: 2.41s\n",
            "Epoch 74, Batch_no 38, 30% \t train_loss: 0.45 took: 2.34s\n",
            "Epoch 74, Batch_no 51, 41% \t train_loss: 0.50 took: 2.53s\n",
            "Epoch 74, Batch_no 64, 51% \t train_loss: 0.38 took: 2.63s\n",
            "Epoch 74, Batch_no 77, 61% \t train_loss: 0.62 took: 2.27s\n",
            "Epoch 74, Batch_no 90, 72% \t train_loss: 0.47 took: 2.25s\n",
            "Epoch 74, Batch_no 103, 82% \t train_loss: 0.63 took: 1.98s\n",
            "Epoch 74, Batch_no 116, 92% \t train_loss: 0.46 took: 2.99s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 75, Batch_no 12, 10% \t train_loss: 0.52 took: 3.18s\n",
            "Epoch 75, Batch_no 25, 20% \t train_loss: 0.45 took: 2.33s\n",
            "Epoch 75, Batch_no 38, 30% \t train_loss: 0.47 took: 2.17s\n",
            "Epoch 75, Batch_no 51, 41% \t train_loss: 0.63 took: 2.41s\n",
            "Epoch 75, Batch_no 64, 51% \t train_loss: 0.52 took: 2.28s\n",
            "Epoch 75, Batch_no 77, 61% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 75, Batch_no 90, 72% \t train_loss: 0.49 took: 2.67s\n",
            "Epoch 75, Batch_no 103, 82% \t train_loss: 0.50 took: 2.35s\n",
            "Epoch 75, Batch_no 116, 92% \t train_loss: 0.47 took: 2.67s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 76, Batch_no 12, 10% \t train_loss: 0.48 took: 3.32s\n",
            "Epoch 76, Batch_no 25, 20% \t train_loss: 0.51 took: 2.33s\n",
            "Epoch 76, Batch_no 38, 30% \t train_loss: 0.57 took: 2.44s\n",
            "Epoch 76, Batch_no 51, 41% \t train_loss: 0.50 took: 2.11s\n",
            "Epoch 76, Batch_no 64, 51% \t train_loss: 0.47 took: 2.46s\n",
            "Epoch 76, Batch_no 77, 61% \t train_loss: 0.44 took: 2.43s\n",
            "Epoch 76, Batch_no 90, 72% \t train_loss: 0.54 took: 2.92s\n",
            "Epoch 76, Batch_no 103, 82% \t train_loss: 0.51 took: 2.05s\n",
            "Epoch 76, Batch_no 116, 92% \t train_loss: 0.55 took: 1.90s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 77, Batch_no 12, 10% \t train_loss: 0.48 took: 3.49s\n",
            "Epoch 77, Batch_no 25, 20% \t train_loss: 0.57 took: 2.08s\n",
            "Epoch 77, Batch_no 38, 30% \t train_loss: 0.65 took: 2.28s\n",
            "Epoch 77, Batch_no 51, 41% \t train_loss: 0.53 took: 2.33s\n",
            "Epoch 77, Batch_no 64, 51% \t train_loss: 0.55 took: 2.52s\n",
            "Epoch 77, Batch_no 77, 61% \t train_loss: 0.44 took: 3.07s\n",
            "Epoch 77, Batch_no 90, 72% \t train_loss: 0.48 took: 2.26s\n",
            "Epoch 77, Batch_no 103, 82% \t train_loss: 0.44 took: 2.28s\n",
            "Epoch 77, Batch_no 116, 92% \t train_loss: 0.64 took: 2.17s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.27\n",
            "Epoch 78, Batch_no 12, 10% \t train_loss: 0.42 took: 3.21s\n",
            "Epoch 78, Batch_no 25, 20% \t train_loss: 0.54 took: 2.13s\n",
            "Epoch 78, Batch_no 38, 30% \t train_loss: 0.45 took: 2.65s\n",
            "Epoch 78, Batch_no 51, 41% \t train_loss: 0.46 took: 2.50s\n",
            "Epoch 78, Batch_no 64, 51% \t train_loss: 0.46 took: 2.14s\n",
            "Epoch 78, Batch_no 77, 61% \t train_loss: 0.44 took: 2.34s\n",
            "Epoch 78, Batch_no 90, 72% \t train_loss: 0.63 took: 2.59s\n",
            "Epoch 78, Batch_no 103, 82% \t train_loss: 0.54 took: 2.44s\n",
            "Epoch 78, Batch_no 116, 92% \t train_loss: 0.61 took: 2.34s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 79, Batch_no 12, 10% \t train_loss: 0.41 took: 2.77s\n",
            "Epoch 79, Batch_no 25, 20% \t train_loss: 0.53 took: 2.84s\n",
            "Epoch 79, Batch_no 38, 30% \t train_loss: 0.51 took: 2.57s\n",
            "Epoch 79, Batch_no 51, 41% \t train_loss: 0.61 took: 2.26s\n",
            "Epoch 79, Batch_no 64, 51% \t train_loss: 0.48 took: 2.18s\n",
            "Epoch 79, Batch_no 77, 61% \t train_loss: 0.49 took: 2.79s\n",
            "Epoch 79, Batch_no 90, 72% \t train_loss: 0.54 took: 2.30s\n",
            "Epoch 79, Batch_no 103, 82% \t train_loss: 0.54 took: 2.01s\n",
            "Epoch 79, Batch_no 116, 92% \t train_loss: 0.48 took: 2.21s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.27\n",
            "Epoch 80, Batch_no 12, 10% \t train_loss: 0.61 took: 2.87s\n",
            "Epoch 80, Batch_no 25, 20% \t train_loss: 0.44 took: 2.43s\n",
            "Epoch 80, Batch_no 38, 30% \t train_loss: 0.49 took: 2.51s\n",
            "Epoch 80, Batch_no 51, 41% \t train_loss: 0.41 took: 2.53s\n",
            "Epoch 80, Batch_no 64, 51% \t train_loss: 0.59 took: 2.40s\n",
            "Epoch 80, Batch_no 77, 61% \t train_loss: 0.44 took: 1.86s\n",
            "Epoch 80, Batch_no 90, 72% \t train_loss: 0.55 took: 2.38s\n",
            "Epoch 80, Batch_no 103, 82% \t train_loss: 0.44 took: 2.69s\n",
            "Epoch 80, Batch_no 116, 92% \t train_loss: 0.49 took: 2.58s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 81, Batch_no 12, 10% \t train_loss: 0.49 took: 3.48s\n",
            "Epoch 81, Batch_no 25, 20% \t train_loss: 0.50 took: 1.90s\n",
            "Epoch 81, Batch_no 38, 30% \t train_loss: 0.58 took: 2.76s\n",
            "Epoch 81, Batch_no 51, 41% \t train_loss: 0.51 took: 1.91s\n",
            "Epoch 81, Batch_no 64, 51% \t train_loss: 0.58 took: 2.26s\n",
            "Epoch 81, Batch_no 77, 61% \t train_loss: 0.44 took: 2.68s\n",
            "Epoch 81, Batch_no 90, 72% \t train_loss: 0.57 took: 2.00s\n",
            "Epoch 81, Batch_no 103, 82% \t train_loss: 0.38 took: 2.71s\n",
            "Epoch 81, Batch_no 116, 92% \t train_loss: 0.50 took: 2.52s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 82, Batch_no 12, 10% \t train_loss: 0.47 took: 3.38s\n",
            "Epoch 82, Batch_no 25, 20% \t train_loss: 0.49 took: 2.31s\n",
            "Epoch 82, Batch_no 38, 30% \t train_loss: 0.45 took: 2.38s\n",
            "Epoch 82, Batch_no 51, 41% \t train_loss: 0.49 took: 2.34s\n",
            "Epoch 82, Batch_no 64, 51% \t train_loss: 0.49 took: 2.12s\n",
            "Epoch 82, Batch_no 77, 61% \t train_loss: 0.48 took: 2.35s\n",
            "Epoch 82, Batch_no 90, 72% \t train_loss: 0.58 took: 2.49s\n",
            "Epoch 82, Batch_no 103, 82% \t train_loss: 0.51 took: 2.53s\n",
            "Epoch 82, Batch_no 116, 92% \t train_loss: 0.53 took: 2.17s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 89 %\n",
            "Validation loss = 0.27\n",
            "Epoch 83, Batch_no 12, 10% \t train_loss: 0.44 took: 2.93s\n",
            "Epoch 83, Batch_no 25, 20% \t train_loss: 0.47 took: 2.62s\n",
            "Epoch 83, Batch_no 38, 30% \t train_loss: 0.48 took: 2.15s\n",
            "Epoch 83, Batch_no 51, 41% \t train_loss: 0.54 took: 2.09s\n",
            "Epoch 83, Batch_no 64, 51% \t train_loss: 0.53 took: 2.94s\n",
            "Epoch 83, Batch_no 77, 61% \t train_loss: 0.47 took: 2.29s\n",
            "Epoch 83, Batch_no 90, 72% \t train_loss: 0.53 took: 2.46s\n",
            "Epoch 83, Batch_no 103, 82% \t train_loss: 0.56 took: 2.34s\n",
            "Epoch 83, Batch_no 116, 92% \t train_loss: 0.45 took: 2.80s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 84, Batch_no 12, 10% \t train_loss: 0.54 took: 3.07s\n",
            "Epoch 84, Batch_no 25, 20% \t train_loss: 0.56 took: 2.37s\n",
            "Epoch 84, Batch_no 38, 30% \t train_loss: 0.45 took: 2.21s\n",
            "Epoch 84, Batch_no 51, 41% \t train_loss: 0.45 took: 2.52s\n",
            "Epoch 84, Batch_no 64, 51% \t train_loss: 0.54 took: 2.90s\n",
            "Epoch 84, Batch_no 77, 61% \t train_loss: 0.48 took: 2.22s\n",
            "Epoch 84, Batch_no 90, 72% \t train_loss: 0.52 took: 2.26s\n",
            "Epoch 84, Batch_no 103, 82% \t train_loss: 0.49 took: 2.34s\n",
            "Epoch 84, Batch_no 116, 92% \t train_loss: 0.43 took: 2.67s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 85, Batch_no 12, 10% \t train_loss: 0.48 took: 2.71s\n",
            "Epoch 85, Batch_no 25, 20% \t train_loss: 0.62 took: 2.81s\n",
            "Epoch 85, Batch_no 38, 30% \t train_loss: 0.53 took: 2.13s\n",
            "Epoch 85, Batch_no 51, 41% \t train_loss: 0.44 took: 2.34s\n",
            "Epoch 85, Batch_no 64, 51% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 85, Batch_no 77, 61% \t train_loss: 0.44 took: 2.44s\n",
            "Epoch 85, Batch_no 90, 72% \t train_loss: 0.60 took: 2.61s\n",
            "Epoch 85, Batch_no 103, 82% \t train_loss: 0.53 took: 2.59s\n",
            "Epoch 85, Batch_no 116, 92% \t train_loss: 0.44 took: 2.63s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 86, Batch_no 12, 10% \t train_loss: 0.44 took: 2.96s\n",
            "Epoch 86, Batch_no 25, 20% \t train_loss: 0.54 took: 2.29s\n",
            "Epoch 86, Batch_no 38, 30% \t train_loss: 0.53 took: 2.06s\n",
            "Epoch 86, Batch_no 51, 41% \t train_loss: 0.46 took: 2.60s\n",
            "Epoch 86, Batch_no 64, 51% \t train_loss: 0.47 took: 2.55s\n",
            "Epoch 86, Batch_no 77, 61% \t train_loss: 0.41 took: 2.38s\n",
            "Epoch 86, Batch_no 90, 72% \t train_loss: 0.51 took: 2.29s\n",
            "Epoch 86, Batch_no 103, 82% \t train_loss: 0.51 took: 2.35s\n",
            "Epoch 86, Batch_no 116, 92% \t train_loss: 0.55 took: 2.78s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 87, Batch_no 12, 10% \t train_loss: 0.41 took: 3.14s\n",
            "Epoch 87, Batch_no 25, 20% \t train_loss: 0.52 took: 2.76s\n",
            "Epoch 87, Batch_no 38, 30% \t train_loss: 0.47 took: 2.34s\n",
            "Epoch 87, Batch_no 51, 41% \t train_loss: 0.53 took: 2.16s\n",
            "Epoch 87, Batch_no 64, 51% \t train_loss: 0.52 took: 2.28s\n",
            "Epoch 87, Batch_no 77, 61% \t train_loss: 0.56 took: 2.66s\n",
            "Epoch 87, Batch_no 90, 72% \t train_loss: 0.43 took: 1.86s\n",
            "Epoch 87, Batch_no 103, 82% \t train_loss: 0.50 took: 2.32s\n",
            "Epoch 87, Batch_no 116, 92% \t train_loss: 0.59 took: 2.50s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 88, Batch_no 12, 10% \t train_loss: 0.50 took: 3.36s\n",
            "Epoch 88, Batch_no 25, 20% \t train_loss: 0.52 took: 2.34s\n",
            "Epoch 88, Batch_no 38, 30% \t train_loss: 0.49 took: 2.31s\n",
            "Epoch 88, Batch_no 51, 41% \t train_loss: 0.53 took: 2.57s\n",
            "Epoch 88, Batch_no 64, 51% \t train_loss: 0.50 took: 2.43s\n",
            "Epoch 88, Batch_no 77, 61% \t train_loss: 0.51 took: 3.18s\n",
            "Epoch 88, Batch_no 90, 72% \t train_loss: 0.59 took: 1.96s\n",
            "Epoch 88, Batch_no 103, 82% \t train_loss: 0.48 took: 2.29s\n",
            "Epoch 88, Batch_no 116, 92% \t train_loss: 0.52 took: 2.58s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 89, Batch_no 12, 10% \t train_loss: 0.55 took: 3.34s\n",
            "Epoch 89, Batch_no 25, 20% \t train_loss: 0.46 took: 2.45s\n",
            "Epoch 89, Batch_no 38, 30% \t train_loss: 0.45 took: 2.36s\n",
            "Epoch 89, Batch_no 51, 41% \t train_loss: 0.49 took: 2.12s\n",
            "Epoch 89, Batch_no 64, 51% \t train_loss: 0.56 took: 2.77s\n",
            "Epoch 89, Batch_no 77, 61% \t train_loss: 0.56 took: 2.28s\n",
            "Epoch 89, Batch_no 90, 72% \t train_loss: 0.51 took: 2.47s\n",
            "Epoch 89, Batch_no 103, 82% \t train_loss: 0.49 took: 2.24s\n",
            "Epoch 89, Batch_no 116, 92% \t train_loss: 0.51 took: 2.83s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 90, Batch_no 12, 10% \t train_loss: 0.54 took: 3.16s\n",
            "Epoch 90, Batch_no 25, 20% \t train_loss: 0.36 took: 1.93s\n",
            "Epoch 90, Batch_no 38, 30% \t train_loss: 0.53 took: 2.79s\n",
            "Epoch 90, Batch_no 51, 41% \t train_loss: 0.45 took: 2.35s\n",
            "Epoch 90, Batch_no 64, 51% \t train_loss: 0.58 took: 2.30s\n",
            "Epoch 90, Batch_no 77, 61% \t train_loss: 0.44 took: 2.60s\n",
            "Epoch 90, Batch_no 90, 72% \t train_loss: 0.47 took: 2.74s\n",
            "Epoch 90, Batch_no 103, 82% \t train_loss: 0.56 took: 2.24s\n",
            "Epoch 90, Batch_no 116, 92% \t train_loss: 0.48 took: 2.27s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 91, Batch_no 12, 10% \t train_loss: 0.53 took: 3.28s\n",
            "Epoch 91, Batch_no 25, 20% \t train_loss: 0.55 took: 2.27s\n",
            "Epoch 91, Batch_no 38, 30% \t train_loss: 0.53 took: 2.44s\n",
            "Epoch 91, Batch_no 51, 41% \t train_loss: 0.47 took: 2.22s\n",
            "Epoch 91, Batch_no 64, 51% \t train_loss: 0.49 took: 3.20s\n",
            "Epoch 91, Batch_no 77, 61% \t train_loss: 0.36 took: 2.13s\n",
            "Epoch 91, Batch_no 90, 72% \t train_loss: 0.52 took: 2.34s\n",
            "Epoch 91, Batch_no 103, 82% \t train_loss: 0.55 took: 2.07s\n",
            "Epoch 91, Batch_no 116, 92% \t train_loss: 0.42 took: 2.93s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 92, Batch_no 12, 10% \t train_loss: 0.47 took: 3.30s\n",
            "Epoch 92, Batch_no 25, 20% \t train_loss: 0.46 took: 2.47s\n",
            "Epoch 92, Batch_no 38, 30% \t train_loss: 0.58 took: 2.23s\n",
            "Epoch 92, Batch_no 51, 41% \t train_loss: 0.44 took: 2.37s\n",
            "Epoch 92, Batch_no 64, 51% \t train_loss: 0.51 took: 2.93s\n",
            "Epoch 92, Batch_no 77, 61% \t train_loss: 0.54 took: 2.46s\n",
            "Epoch 92, Batch_no 90, 72% \t train_loss: 0.44 took: 2.28s\n",
            "Epoch 92, Batch_no 103, 82% \t train_loss: 0.43 took: 1.94s\n",
            "Epoch 92, Batch_no 116, 92% \t train_loss: 0.48 took: 2.78s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 93, Batch_no 12, 10% \t train_loss: 0.44 took: 3.07s\n",
            "Epoch 93, Batch_no 25, 20% \t train_loss: 0.54 took: 2.70s\n",
            "Epoch 93, Batch_no 38, 30% \t train_loss: 0.48 took: 2.30s\n",
            "Epoch 93, Batch_no 51, 41% \t train_loss: 0.38 took: 2.14s\n",
            "Epoch 93, Batch_no 64, 51% \t train_loss: 0.56 took: 2.22s\n",
            "Epoch 93, Batch_no 77, 61% \t train_loss: 0.53 took: 2.98s\n",
            "Epoch 93, Batch_no 90, 72% \t train_loss: 0.48 took: 2.14s\n",
            "Epoch 93, Batch_no 103, 82% \t train_loss: 0.50 took: 2.09s\n",
            "Epoch 93, Batch_no 116, 92% \t train_loss: 0.52 took: 2.64s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 94, Batch_no 12, 10% \t train_loss: 0.51 took: 2.85s\n",
            "Epoch 94, Batch_no 25, 20% \t train_loss: 0.60 took: 2.13s\n",
            "Epoch 94, Batch_no 38, 30% \t train_loss: 0.45 took: 2.60s\n",
            "Epoch 94, Batch_no 51, 41% \t train_loss: 0.61 took: 2.48s\n",
            "Epoch 94, Batch_no 64, 51% \t train_loss: 0.52 took: 2.41s\n",
            "Epoch 94, Batch_no 77, 61% \t train_loss: 0.50 took: 2.41s\n",
            "Epoch 94, Batch_no 90, 72% \t train_loss: 0.45 took: 2.64s\n",
            "Epoch 94, Batch_no 103, 82% \t train_loss: 0.52 took: 2.36s\n",
            "Epoch 94, Batch_no 116, 92% \t train_loss: 0.49 took: 2.31s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 95, Batch_no 12, 10% \t train_loss: 0.51 took: 2.86s\n",
            "Epoch 95, Batch_no 25, 20% \t train_loss: 0.51 took: 2.38s\n",
            "Epoch 95, Batch_no 38, 30% \t train_loss: 0.51 took: 2.25s\n",
            "Epoch 95, Batch_no 51, 41% \t train_loss: 0.45 took: 3.05s\n",
            "Epoch 95, Batch_no 64, 51% \t train_loss: 0.47 took: 2.34s\n",
            "Epoch 95, Batch_no 77, 61% \t train_loss: 0.55 took: 2.28s\n",
            "Epoch 95, Batch_no 90, 72% \t train_loss: 0.47 took: 2.39s\n",
            "Epoch 95, Batch_no 103, 82% \t train_loss: 0.45 took: 2.61s\n",
            "Epoch 95, Batch_no 116, 92% \t train_loss: 0.52 took: 2.57s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 96, Batch_no 12, 10% \t train_loss: 0.51 took: 3.30s\n",
            "Epoch 96, Batch_no 25, 20% \t train_loss: 0.59 took: 2.32s\n",
            "Epoch 96, Batch_no 38, 30% \t train_loss: 0.46 took: 2.40s\n",
            "Epoch 96, Batch_no 51, 41% \t train_loss: 0.48 took: 2.13s\n",
            "Epoch 96, Batch_no 64, 51% \t train_loss: 0.42 took: 2.44s\n",
            "Epoch 96, Batch_no 77, 61% \t train_loss: 0.59 took: 2.09s\n",
            "Epoch 96, Batch_no 90, 72% \t train_loss: 0.51 took: 2.92s\n",
            "Epoch 96, Batch_no 103, 82% \t train_loss: 0.48 took: 2.33s\n",
            "Epoch 96, Batch_no 116, 92% \t train_loss: 0.51 took: 2.93s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 97, Batch_no 12, 10% \t train_loss: 0.52 took: 3.02s\n",
            "Epoch 97, Batch_no 25, 20% \t train_loss: 0.51 took: 2.46s\n",
            "Epoch 97, Batch_no 38, 30% \t train_loss: 0.56 took: 2.41s\n",
            "Epoch 97, Batch_no 51, 41% \t train_loss: 0.49 took: 3.06s\n",
            "Epoch 97, Batch_no 64, 51% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 97, Batch_no 77, 61% \t train_loss: 0.44 took: 2.22s\n",
            "Epoch 97, Batch_no 90, 72% \t train_loss: 0.52 took: 2.09s\n",
            "Epoch 97, Batch_no 103, 82% \t train_loss: 0.46 took: 2.59s\n",
            "Epoch 97, Batch_no 116, 92% \t train_loss: 0.49 took: 2.09s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 98, Batch_no 12, 10% \t train_loss: 0.42 took: 3.39s\n",
            "Epoch 98, Batch_no 25, 20% \t train_loss: 0.43 took: 2.30s\n",
            "Epoch 98, Batch_no 38, 30% \t train_loss: 0.52 took: 1.93s\n",
            "Epoch 98, Batch_no 51, 41% \t train_loss: 0.47 took: 2.34s\n",
            "Epoch 98, Batch_no 64, 51% \t train_loss: 0.43 took: 2.97s\n",
            "Epoch 98, Batch_no 77, 61% \t train_loss: 0.50 took: 2.32s\n",
            "Epoch 98, Batch_no 90, 72% \t train_loss: 0.54 took: 2.24s\n",
            "Epoch 98, Batch_no 103, 82% \t train_loss: 0.49 took: 2.47s\n",
            "Epoch 98, Batch_no 116, 92% \t train_loss: 0.52 took: 2.82s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 99, Batch_no 12, 10% \t train_loss: 0.43 took: 3.01s\n",
            "Epoch 99, Batch_no 25, 20% \t train_loss: 0.48 took: 2.58s\n",
            "Epoch 99, Batch_no 38, 30% \t train_loss: 0.48 took: 2.68s\n",
            "Epoch 99, Batch_no 51, 41% \t train_loss: 0.54 took: 2.96s\n",
            "Epoch 99, Batch_no 64, 51% \t train_loss: 0.45 took: 2.20s\n",
            "Epoch 99, Batch_no 77, 61% \t train_loss: 0.45 took: 2.14s\n",
            "Epoch 99, Batch_no 90, 72% \t train_loss: 0.50 took: 3.38s\n",
            "Epoch 99, Batch_no 103, 82% \t train_loss: 0.47 took: 2.22s\n",
            "Epoch 99, Batch_no 116, 92% \t train_loss: 0.56 took: 2.39s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.30\n",
            "Epoch 100, Batch_no 12, 10% \t train_loss: 0.42 took: 3.47s\n",
            "Epoch 100, Batch_no 25, 20% \t train_loss: 0.51 took: 2.51s\n",
            "Epoch 100, Batch_no 38, 30% \t train_loss: 0.49 took: 2.80s\n",
            "Epoch 100, Batch_no 51, 41% \t train_loss: 0.53 took: 2.38s\n",
            "Epoch 100, Batch_no 64, 51% \t train_loss: 0.46 took: 2.52s\n",
            "Epoch 100, Batch_no 77, 61% \t train_loss: 0.56 took: 2.81s\n",
            "Epoch 100, Batch_no 90, 72% \t train_loss: 0.42 took: 2.27s\n",
            "Epoch 100, Batch_no 103, 82% \t train_loss: 0.57 took: 2.65s\n",
            "Epoch 100, Batch_no 116, 92% \t train_loss: 0.49 took: 2.72s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 101, Batch_no 12, 10% \t train_loss: 0.53 took: 2.94s\n",
            "Epoch 101, Batch_no 25, 20% \t train_loss: 0.57 took: 2.57s\n",
            "Epoch 101, Batch_no 38, 30% \t train_loss: 0.53 took: 2.83s\n",
            "Epoch 101, Batch_no 51, 41% \t train_loss: 0.46 took: 2.38s\n",
            "Epoch 101, Batch_no 64, 51% \t train_loss: 0.50 took: 2.25s\n",
            "Epoch 101, Batch_no 77, 61% \t train_loss: 0.48 took: 2.22s\n",
            "Epoch 101, Batch_no 90, 72% \t train_loss: 0.59 took: 2.40s\n",
            "Epoch 101, Batch_no 103, 82% \t train_loss: 0.46 took: 3.23s\n",
            "Epoch 101, Batch_no 116, 92% \t train_loss: 0.42 took: 2.51s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 102, Batch_no 12, 10% \t train_loss: 0.46 took: 3.23s\n",
            "Epoch 102, Batch_no 25, 20% \t train_loss: 0.47 took: 2.53s\n",
            "Epoch 102, Batch_no 38, 30% \t train_loss: 0.49 took: 2.86s\n",
            "Epoch 102, Batch_no 51, 41% \t train_loss: 0.49 took: 2.29s\n",
            "Epoch 102, Batch_no 64, 51% \t train_loss: 0.44 took: 2.41s\n",
            "Epoch 102, Batch_no 77, 61% \t train_loss: 0.50 took: 2.30s\n",
            "Epoch 102, Batch_no 90, 72% \t train_loss: 0.48 took: 2.82s\n",
            "Epoch 102, Batch_no 103, 82% \t train_loss: 0.52 took: 2.34s\n",
            "Epoch 102, Batch_no 116, 92% \t train_loss: 0.49 took: 2.31s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 103, Batch_no 12, 10% \t train_loss: 0.48 took: 3.48s\n",
            "Epoch 103, Batch_no 25, 20% \t train_loss: 0.50 took: 2.68s\n",
            "Epoch 103, Batch_no 38, 30% \t train_loss: 0.53 took: 2.30s\n",
            "Epoch 103, Batch_no 51, 41% \t train_loss: 0.44 took: 2.08s\n",
            "Epoch 103, Batch_no 64, 51% \t train_loss: 0.54 took: 2.76s\n",
            "Epoch 103, Batch_no 77, 61% \t train_loss: 0.48 took: 2.44s\n",
            "Epoch 103, Batch_no 90, 72% \t train_loss: 0.42 took: 2.04s\n",
            "Epoch 103, Batch_no 103, 82% \t train_loss: 0.40 took: 2.13s\n",
            "Epoch 103, Batch_no 116, 92% \t train_loss: 0.52 took: 2.71s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 104, Batch_no 12, 10% \t train_loss: 0.46 took: 2.85s\n",
            "Epoch 104, Batch_no 25, 20% \t train_loss: 0.51 took: 2.55s\n",
            "Epoch 104, Batch_no 38, 30% \t train_loss: 0.45 took: 2.82s\n",
            "Epoch 104, Batch_no 51, 41% \t train_loss: 0.45 took: 2.61s\n",
            "Epoch 104, Batch_no 64, 51% \t train_loss: 0.38 took: 2.52s\n",
            "Epoch 104, Batch_no 77, 61% \t train_loss: 0.56 took: 2.13s\n",
            "Epoch 104, Batch_no 90, 72% \t train_loss: 0.46 took: 2.42s\n",
            "Epoch 104, Batch_no 103, 82% \t train_loss: 0.50 took: 2.80s\n",
            "Epoch 104, Batch_no 116, 92% \t train_loss: 0.46 took: 2.23s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 105, Batch_no 12, 10% \t train_loss: 0.49 took: 3.46s\n",
            "Epoch 105, Batch_no 25, 20% \t train_loss: 0.52 took: 2.30s\n",
            "Epoch 105, Batch_no 38, 30% \t train_loss: 0.51 took: 2.06s\n",
            "Epoch 105, Batch_no 51, 41% \t train_loss: 0.47 took: 2.24s\n",
            "Epoch 105, Batch_no 64, 51% \t train_loss: 0.58 took: 2.90s\n",
            "Epoch 105, Batch_no 77, 61% \t train_loss: 0.46 took: 2.50s\n",
            "Epoch 105, Batch_no 90, 72% \t train_loss: 0.37 took: 2.28s\n",
            "Epoch 105, Batch_no 103, 82% \t train_loss: 0.47 took: 2.45s\n",
            "Epoch 105, Batch_no 116, 92% \t train_loss: 0.42 took: 2.39s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.32\n",
            "Epoch 106, Batch_no 12, 10% \t train_loss: 0.60 took: 3.01s\n",
            "Epoch 106, Batch_no 25, 20% \t train_loss: 0.55 took: 2.85s\n",
            "Epoch 106, Batch_no 38, 30% \t train_loss: 0.50 took: 2.32s\n",
            "Epoch 106, Batch_no 51, 41% \t train_loss: 0.45 took: 2.46s\n",
            "Epoch 106, Batch_no 64, 51% \t train_loss: 0.56 took: 2.47s\n",
            "Epoch 106, Batch_no 77, 61% \t train_loss: 0.41 took: 3.08s\n",
            "Epoch 106, Batch_no 90, 72% \t train_loss: 0.44 took: 2.48s\n",
            "Epoch 106, Batch_no 103, 82% \t train_loss: 0.42 took: 1.91s\n",
            "Epoch 106, Batch_no 116, 92% \t train_loss: 0.46 took: 2.19s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 107, Batch_no 12, 10% \t train_loss: 0.46 took: 3.19s\n",
            "Epoch 107, Batch_no 25, 20% \t train_loss: 0.55 took: 2.45s\n",
            "Epoch 107, Batch_no 38, 30% \t train_loss: 0.47 took: 2.29s\n",
            "Epoch 107, Batch_no 51, 41% \t train_loss: 0.45 took: 2.28s\n",
            "Epoch 107, Batch_no 64, 51% \t train_loss: 0.44 took: 2.51s\n",
            "Epoch 107, Batch_no 77, 61% \t train_loss: 0.55 took: 2.30s\n",
            "Epoch 107, Batch_no 90, 72% \t train_loss: 0.47 took: 2.68s\n",
            "Epoch 107, Batch_no 103, 82% \t train_loss: 0.49 took: 2.71s\n",
            "Epoch 107, Batch_no 116, 92% \t train_loss: 0.51 took: 2.32s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 108, Batch_no 12, 10% \t train_loss: 0.52 took: 3.59s\n",
            "Epoch 108, Batch_no 25, 20% \t train_loss: 0.40 took: 2.39s\n",
            "Epoch 108, Batch_no 38, 30% \t train_loss: 0.45 took: 2.22s\n",
            "Epoch 108, Batch_no 51, 41% \t train_loss: 0.42 took: 2.43s\n",
            "Epoch 108, Batch_no 64, 51% \t train_loss: 0.41 took: 3.13s\n",
            "Epoch 108, Batch_no 77, 61% \t train_loss: 0.46 took: 1.88s\n",
            "Epoch 108, Batch_no 90, 72% \t train_loss: 0.43 took: 2.39s\n",
            "Epoch 108, Batch_no 103, 82% \t train_loss: 0.49 took: 2.47s\n",
            "Epoch 108, Batch_no 116, 92% \t train_loss: 0.55 took: 2.80s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 109, Batch_no 12, 10% \t train_loss: 0.52 took: 2.77s\n",
            "Epoch 109, Batch_no 25, 20% \t train_loss: 0.58 took: 2.59s\n",
            "Epoch 109, Batch_no 38, 30% \t train_loss: 0.48 took: 2.26s\n",
            "Epoch 109, Batch_no 51, 41% \t train_loss: 0.48 took: 2.52s\n",
            "Epoch 109, Batch_no 64, 51% \t train_loss: 0.56 took: 3.08s\n",
            "Epoch 109, Batch_no 77, 61% \t train_loss: 0.50 took: 2.07s\n",
            "Epoch 109, Batch_no 90, 72% \t train_loss: 0.51 took: 2.06s\n",
            "Epoch 109, Batch_no 103, 82% \t train_loss: 0.42 took: 2.57s\n",
            "Epoch 109, Batch_no 116, 92% \t train_loss: 0.53 took: 2.91s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.30\n",
            "Epoch 110, Batch_no 12, 10% \t train_loss: 0.49 took: 3.33s\n",
            "Epoch 110, Batch_no 25, 20% \t train_loss: 0.50 took: 2.53s\n",
            "Epoch 110, Batch_no 38, 30% \t train_loss: 0.50 took: 2.36s\n",
            "Epoch 110, Batch_no 51, 41% \t train_loss: 0.61 took: 2.42s\n",
            "Epoch 110, Batch_no 64, 51% \t train_loss: 0.48 took: 2.82s\n",
            "Epoch 110, Batch_no 77, 61% \t train_loss: 0.51 took: 2.39s\n",
            "Epoch 110, Batch_no 90, 72% \t train_loss: 0.41 took: 2.16s\n",
            "Epoch 110, Batch_no 103, 82% \t train_loss: 0.43 took: 2.54s\n",
            "Epoch 110, Batch_no 116, 92% \t train_loss: 0.53 took: 2.58s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 111, Batch_no 12, 10% \t train_loss: 0.53 took: 2.84s\n",
            "Epoch 111, Batch_no 25, 20% \t train_loss: 0.48 took: 2.38s\n",
            "Epoch 111, Batch_no 38, 30% \t train_loss: 0.42 took: 2.75s\n",
            "Epoch 111, Batch_no 51, 41% \t train_loss: 0.49 took: 2.64s\n",
            "Epoch 111, Batch_no 64, 51% \t train_loss: 0.48 took: 2.51s\n",
            "Epoch 111, Batch_no 77, 61% \t train_loss: 0.44 took: 3.03s\n",
            "Epoch 111, Batch_no 90, 72% \t train_loss: 0.46 took: 2.44s\n",
            "Epoch 111, Batch_no 103, 82% \t train_loss: 0.51 took: 2.53s\n",
            "Epoch 111, Batch_no 116, 92% \t train_loss: 0.49 took: 2.06s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 112, Batch_no 12, 10% \t train_loss: 0.48 took: 2.88s\n",
            "Epoch 112, Batch_no 25, 20% \t train_loss: 0.44 took: 2.65s\n",
            "Epoch 112, Batch_no 38, 30% \t train_loss: 0.46 took: 2.88s\n",
            "Epoch 112, Batch_no 51, 41% \t train_loss: 0.50 took: 2.13s\n",
            "Epoch 112, Batch_no 64, 51% \t train_loss: 0.54 took: 2.53s\n",
            "Epoch 112, Batch_no 77, 61% \t train_loss: 0.48 took: 2.70s\n",
            "Epoch 112, Batch_no 90, 72% \t train_loss: 0.51 took: 2.84s\n",
            "Epoch 112, Batch_no 103, 82% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 112, Batch_no 116, 92% \t train_loss: 0.55 took: 2.38s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 113, Batch_no 12, 10% \t train_loss: 0.41 took: 3.56s\n",
            "Epoch 113, Batch_no 25, 20% \t train_loss: 0.52 took: 2.22s\n",
            "Epoch 113, Batch_no 38, 30% \t train_loss: 0.47 took: 2.39s\n",
            "Epoch 113, Batch_no 51, 41% \t train_loss: 0.54 took: 2.10s\n",
            "Epoch 113, Batch_no 64, 51% \t train_loss: 0.52 took: 3.13s\n",
            "Epoch 113, Batch_no 77, 61% \t train_loss: 0.55 took: 2.22s\n",
            "Epoch 113, Batch_no 90, 72% \t train_loss: 0.48 took: 2.14s\n",
            "Epoch 113, Batch_no 103, 82% \t train_loss: 0.56 took: 2.52s\n",
            "Epoch 113, Batch_no 116, 92% \t train_loss: 0.47 took: 2.86s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 114, Batch_no 12, 10% \t train_loss: 0.46 took: 3.31s\n",
            "Epoch 114, Batch_no 25, 20% \t train_loss: 0.44 took: 2.56s\n",
            "Epoch 114, Batch_no 38, 30% \t train_loss: 0.42 took: 2.38s\n",
            "Epoch 114, Batch_no 51, 41% \t train_loss: 0.52 took: 2.33s\n",
            "Epoch 114, Batch_no 64, 51% \t train_loss: 0.55 took: 2.89s\n",
            "Epoch 114, Batch_no 77, 61% \t train_loss: 0.44 took: 2.07s\n",
            "Epoch 114, Batch_no 90, 72% \t train_loss: 0.43 took: 2.48s\n",
            "Epoch 114, Batch_no 103, 82% \t train_loss: 0.42 took: 1.98s\n",
            "Epoch 114, Batch_no 116, 92% \t train_loss: 0.39 took: 2.70s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.31\n",
            "Epoch 115, Batch_no 12, 10% \t train_loss: 0.47 took: 2.83s\n",
            "Epoch 115, Batch_no 25, 20% \t train_loss: 0.49 took: 2.75s\n",
            "Epoch 115, Batch_no 38, 30% \t train_loss: 0.57 took: 2.36s\n",
            "Epoch 115, Batch_no 51, 41% \t train_loss: 0.54 took: 2.17s\n",
            "Epoch 115, Batch_no 64, 51% \t train_loss: 0.51 took: 2.39s\n",
            "Epoch 115, Batch_no 77, 61% \t train_loss: 0.39 took: 2.67s\n",
            "Epoch 115, Batch_no 90, 72% \t train_loss: 0.37 took: 2.73s\n",
            "Epoch 115, Batch_no 103, 82% \t train_loss: 0.50 took: 2.48s\n",
            "Epoch 115, Batch_no 116, 92% \t train_loss: 0.45 took: 2.47s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 116, Batch_no 12, 10% \t train_loss: 0.49 took: 3.36s\n",
            "Epoch 116, Batch_no 25, 20% \t train_loss: 0.44 took: 2.32s\n",
            "Epoch 116, Batch_no 38, 30% \t train_loss: 0.47 took: 2.42s\n",
            "Epoch 116, Batch_no 51, 41% \t train_loss: 0.44 took: 2.47s\n",
            "Epoch 116, Batch_no 64, 51% \t train_loss: 0.49 took: 2.38s\n",
            "Epoch 116, Batch_no 77, 61% \t train_loss: 0.50 took: 2.53s\n",
            "Epoch 116, Batch_no 90, 72% \t train_loss: 0.42 took: 2.28s\n",
            "Epoch 116, Batch_no 103, 82% \t train_loss: 0.50 took: 2.13s\n",
            "Epoch 116, Batch_no 116, 92% \t train_loss: 0.49 took: 2.56s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 117, Batch_no 12, 10% \t train_loss: 0.42 took: 3.07s\n",
            "Epoch 117, Batch_no 25, 20% \t train_loss: 0.45 took: 2.46s\n",
            "Epoch 117, Batch_no 38, 30% \t train_loss: 0.54 took: 2.61s\n",
            "Epoch 117, Batch_no 51, 41% \t train_loss: 0.50 took: 2.45s\n",
            "Epoch 117, Batch_no 64, 51% \t train_loss: 0.52 took: 2.57s\n",
            "Epoch 117, Batch_no 77, 61% \t train_loss: 0.48 took: 2.50s\n",
            "Epoch 117, Batch_no 90, 72% \t train_loss: 0.45 took: 2.15s\n",
            "Epoch 117, Batch_no 103, 82% \t train_loss: 0.53 took: 2.02s\n",
            "Epoch 117, Batch_no 116, 92% \t train_loss: 0.43 took: 2.67s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 118, Batch_no 12, 10% \t train_loss: 0.51 took: 3.13s\n",
            "Epoch 118, Batch_no 25, 20% \t train_loss: 0.49 took: 2.42s\n",
            "Epoch 118, Batch_no 38, 30% \t train_loss: 0.49 took: 2.22s\n",
            "Epoch 118, Batch_no 51, 41% \t train_loss: 0.59 took: 2.57s\n",
            "Epoch 118, Batch_no 64, 51% \t train_loss: 0.62 took: 2.76s\n",
            "Epoch 118, Batch_no 77, 61% \t train_loss: 0.47 took: 2.30s\n",
            "Epoch 118, Batch_no 90, 72% \t train_loss: 0.44 took: 2.23s\n",
            "Epoch 118, Batch_no 103, 82% \t train_loss: 0.49 took: 2.27s\n",
            "Epoch 118, Batch_no 116, 92% \t train_loss: 0.49 took: 2.85s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 119, Batch_no 12, 10% \t train_loss: 0.43 took: 3.54s\n",
            "Epoch 119, Batch_no 25, 20% \t train_loss: 0.40 took: 2.26s\n",
            "Epoch 119, Batch_no 38, 30% \t train_loss: 0.54 took: 2.21s\n",
            "Epoch 119, Batch_no 51, 41% \t train_loss: 0.40 took: 2.36s\n",
            "Epoch 119, Batch_no 64, 51% \t train_loss: 0.44 took: 2.86s\n",
            "Epoch 119, Batch_no 77, 61% \t train_loss: 0.51 took: 2.54s\n",
            "Epoch 119, Batch_no 90, 72% \t train_loss: 0.43 took: 2.18s\n",
            "Epoch 119, Batch_no 103, 82% \t train_loss: 0.46 took: 2.27s\n",
            "Epoch 119, Batch_no 116, 92% \t train_loss: 0.48 took: 3.01s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 120, Batch_no 12, 10% \t train_loss: 0.35 took: 3.18s\n",
            "Epoch 120, Batch_no 25, 20% \t train_loss: 0.45 took: 2.38s\n",
            "Epoch 120, Batch_no 38, 30% \t train_loss: 0.52 took: 2.34s\n",
            "Epoch 120, Batch_no 51, 41% \t train_loss: 0.65 took: 2.26s\n",
            "Epoch 120, Batch_no 64, 51% \t train_loss: 0.41 took: 2.80s\n",
            "Epoch 120, Batch_no 77, 61% \t train_loss: 0.38 took: 2.31s\n",
            "Epoch 120, Batch_no 90, 72% \t train_loss: 0.51 took: 2.38s\n",
            "Epoch 120, Batch_no 103, 82% \t train_loss: 0.49 took: 2.40s\n",
            "Epoch 120, Batch_no 116, 92% \t train_loss: 0.50 took: 2.52s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 121, Batch_no 12, 10% \t train_loss: 0.48 took: 3.00s\n",
            "Epoch 121, Batch_no 25, 20% \t train_loss: 0.50 took: 2.29s\n",
            "Epoch 121, Batch_no 38, 30% \t train_loss: 0.50 took: 2.60s\n",
            "Epoch 121, Batch_no 51, 41% \t train_loss: 0.50 took: 2.75s\n",
            "Epoch 121, Batch_no 64, 51% \t train_loss: 0.45 took: 2.45s\n",
            "Epoch 121, Batch_no 77, 61% \t train_loss: 0.50 took: 2.55s\n",
            "Epoch 121, Batch_no 90, 72% \t train_loss: 0.46 took: 2.30s\n",
            "Epoch 121, Batch_no 103, 82% \t train_loss: 0.49 took: 2.96s\n",
            "Epoch 121, Batch_no 116, 92% \t train_loss: 0.43 took: 2.29s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 122, Batch_no 12, 10% \t train_loss: 0.47 took: 3.03s\n",
            "Epoch 122, Batch_no 25, 20% \t train_loss: 0.51 took: 2.38s\n",
            "Epoch 122, Batch_no 38, 30% \t train_loss: 0.44 took: 2.70s\n",
            "Epoch 122, Batch_no 51, 41% \t train_loss: 0.68 took: 2.62s\n",
            "Epoch 122, Batch_no 64, 51% \t train_loss: 0.50 took: 2.06s\n",
            "Epoch 122, Batch_no 77, 61% \t train_loss: 0.42 took: 2.24s\n",
            "Epoch 122, Batch_no 90, 72% \t train_loss: 0.52 took: 3.34s\n",
            "Epoch 122, Batch_no 103, 82% \t train_loss: 0.47 took: 2.07s\n",
            "Epoch 122, Batch_no 116, 92% \t train_loss: 0.49 took: 2.39s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 123, Batch_no 12, 10% \t train_loss: 0.46 took: 2.97s\n",
            "Epoch 123, Batch_no 25, 20% \t train_loss: 0.49 took: 2.62s\n",
            "Epoch 123, Batch_no 38, 30% \t train_loss: 0.45 took: 2.62s\n",
            "Epoch 123, Batch_no 51, 41% \t train_loss: 0.41 took: 2.43s\n",
            "Epoch 123, Batch_no 64, 51% \t train_loss: 0.48 took: 2.25s\n",
            "Epoch 123, Batch_no 77, 61% \t train_loss: 0.51 took: 2.77s\n",
            "Epoch 123, Batch_no 90, 72% \t train_loss: 0.43 took: 2.42s\n",
            "Epoch 123, Batch_no 103, 82% \t train_loss: 0.46 took: 2.04s\n",
            "Epoch 123, Batch_no 116, 92% \t train_loss: 0.53 took: 2.83s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 89 %\n",
            "Validation loss = 0.26\n",
            "Epoch 124, Batch_no 12, 10% \t train_loss: 0.41 took: 2.89s\n",
            "Epoch 124, Batch_no 25, 20% \t train_loss: 0.50 took: 2.44s\n",
            "Epoch 124, Batch_no 38, 30% \t train_loss: 0.42 took: 2.70s\n",
            "Epoch 124, Batch_no 51, 41% \t train_loss: 0.41 took: 2.53s\n",
            "Epoch 124, Batch_no 64, 51% \t train_loss: 0.48 took: 2.76s\n",
            "Epoch 124, Batch_no 77, 61% \t train_loss: 0.51 took: 2.32s\n",
            "Epoch 124, Batch_no 90, 72% \t train_loss: 0.49 took: 2.58s\n",
            "Epoch 124, Batch_no 103, 82% \t train_loss: 0.46 took: 2.22s\n",
            "Epoch 124, Batch_no 116, 92% \t train_loss: 0.52 took: 2.65s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 125, Batch_no 12, 10% \t train_loss: 0.48 took: 3.05s\n",
            "Epoch 125, Batch_no 25, 20% \t train_loss: 0.47 took: 2.32s\n",
            "Epoch 125, Batch_no 38, 30% \t train_loss: 0.56 took: 2.47s\n",
            "Epoch 125, Batch_no 51, 41% \t train_loss: 0.46 took: 2.36s\n",
            "Epoch 125, Batch_no 64, 51% \t train_loss: 0.47 took: 2.71s\n",
            "Epoch 125, Batch_no 77, 61% \t train_loss: 0.37 took: 2.32s\n",
            "Epoch 125, Batch_no 90, 72% \t train_loss: 0.51 took: 2.47s\n",
            "Epoch 125, Batch_no 103, 82% \t train_loss: 0.55 took: 2.20s\n",
            "Epoch 125, Batch_no 116, 92% \t train_loss: 0.44 took: 3.06s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 126, Batch_no 12, 10% \t train_loss: 0.38 took: 3.33s\n",
            "Epoch 126, Batch_no 25, 20% \t train_loss: 0.51 took: 2.80s\n",
            "Epoch 126, Batch_no 38, 30% \t train_loss: 0.42 took: 2.57s\n",
            "Epoch 126, Batch_no 51, 41% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 126, Batch_no 64, 51% \t train_loss: 0.43 took: 2.66s\n",
            "Epoch 126, Batch_no 77, 61% \t train_loss: 0.61 took: 2.89s\n",
            "Epoch 126, Batch_no 90, 72% \t train_loss: 0.54 took: 2.52s\n",
            "Epoch 126, Batch_no 103, 82% \t train_loss: 0.50 took: 2.42s\n",
            "Epoch 126, Batch_no 116, 92% \t train_loss: 0.63 took: 2.28s\n",
            "Training accuracy: 80 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 127, Batch_no 12, 10% \t train_loss: 0.40 took: 3.11s\n",
            "Epoch 127, Batch_no 25, 20% \t train_loss: 0.47 took: 2.45s\n",
            "Epoch 127, Batch_no 38, 30% \t train_loss: 0.45 took: 2.30s\n",
            "Epoch 127, Batch_no 51, 41% \t train_loss: 0.55 took: 2.43s\n",
            "Epoch 127, Batch_no 64, 51% \t train_loss: 0.41 took: 2.69s\n",
            "Epoch 127, Batch_no 77, 61% \t train_loss: 0.37 took: 2.27s\n",
            "Epoch 127, Batch_no 90, 72% \t train_loss: 0.45 took: 2.28s\n",
            "Epoch 127, Batch_no 103, 82% \t train_loss: 0.42 took: 2.27s\n",
            "Epoch 127, Batch_no 116, 92% \t train_loss: 0.47 took: 2.41s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 128, Batch_no 12, 10% \t train_loss: 0.45 took: 2.91s\n",
            "Epoch 128, Batch_no 25, 20% \t train_loss: 0.41 took: 2.93s\n",
            "Epoch 128, Batch_no 38, 30% \t train_loss: 0.42 took: 2.17s\n",
            "Epoch 128, Batch_no 51, 41% \t train_loss: 0.54 took: 2.39s\n",
            "Epoch 128, Batch_no 64, 51% \t train_loss: 0.45 took: 2.20s\n",
            "Epoch 128, Batch_no 77, 61% \t train_loss: 0.46 took: 3.01s\n",
            "Epoch 128, Batch_no 90, 72% \t train_loss: 0.48 took: 2.03s\n",
            "Epoch 128, Batch_no 103, 82% \t train_loss: 0.49 took: 2.33s\n",
            "Epoch 128, Batch_no 116, 92% \t train_loss: 0.49 took: 2.27s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 129, Batch_no 12, 10% \t train_loss: 0.38 took: 3.27s\n",
            "Epoch 129, Batch_no 25, 20% \t train_loss: 0.40 took: 2.33s\n",
            "Epoch 129, Batch_no 38, 30% \t train_loss: 0.43 took: 2.46s\n",
            "Epoch 129, Batch_no 51, 41% \t train_loss: 0.56 took: 2.92s\n",
            "Epoch 129, Batch_no 64, 51% \t train_loss: 0.42 took: 2.19s\n",
            "Epoch 129, Batch_no 77, 61% \t train_loss: 0.55 took: 2.14s\n",
            "Epoch 129, Batch_no 90, 72% \t train_loss: 0.47 took: 2.38s\n",
            "Epoch 129, Batch_no 103, 82% \t train_loss: 0.42 took: 2.25s\n",
            "Epoch 129, Batch_no 116, 92% \t train_loss: 0.42 took: 2.39s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 130, Batch_no 12, 10% \t train_loss: 0.42 took: 3.18s\n",
            "Epoch 130, Batch_no 25, 20% \t train_loss: 0.48 took: 2.52s\n",
            "Epoch 130, Batch_no 38, 30% \t train_loss: 0.39 took: 2.13s\n",
            "Epoch 130, Batch_no 51, 41% \t train_loss: 0.49 took: 2.17s\n",
            "Epoch 130, Batch_no 64, 51% \t train_loss: 0.52 took: 2.69s\n",
            "Epoch 130, Batch_no 77, 61% \t train_loss: 0.47 took: 2.43s\n",
            "Epoch 130, Batch_no 90, 72% \t train_loss: 0.46 took: 2.22s\n",
            "Epoch 130, Batch_no 103, 82% \t train_loss: 0.51 took: 2.63s\n",
            "Epoch 130, Batch_no 116, 92% \t train_loss: 0.44 took: 2.71s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 131, Batch_no 12, 10% \t train_loss: 0.48 took: 2.99s\n",
            "Epoch 131, Batch_no 25, 20% \t train_loss: 0.40 took: 2.31s\n",
            "Epoch 131, Batch_no 38, 30% \t train_loss: 0.46 took: 2.53s\n",
            "Epoch 131, Batch_no 51, 41% \t train_loss: 0.50 took: 2.30s\n",
            "Epoch 131, Batch_no 64, 51% \t train_loss: 0.46 took: 2.23s\n",
            "Epoch 131, Batch_no 77, 61% \t train_loss: 0.41 took: 2.71s\n",
            "Epoch 131, Batch_no 90, 72% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 131, Batch_no 103, 82% \t train_loss: 0.51 took: 2.25s\n",
            "Epoch 131, Batch_no 116, 92% \t train_loss: 0.42 took: 2.50s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 132, Batch_no 12, 10% \t train_loss: 0.42 took: 3.62s\n",
            "Epoch 132, Batch_no 25, 20% \t train_loss: 0.38 took: 2.29s\n",
            "Epoch 132, Batch_no 38, 30% \t train_loss: 0.46 took: 2.53s\n",
            "Epoch 132, Batch_no 51, 41% \t train_loss: 0.47 took: 2.12s\n",
            "Epoch 132, Batch_no 64, 51% \t train_loss: 0.54 took: 2.67s\n",
            "Epoch 132, Batch_no 77, 61% \t train_loss: 0.39 took: 2.25s\n",
            "Epoch 132, Batch_no 90, 72% \t train_loss: 0.45 took: 2.38s\n",
            "Epoch 132, Batch_no 103, 82% \t train_loss: 0.39 took: 2.44s\n",
            "Epoch 132, Batch_no 116, 92% \t train_loss: 0.49 took: 2.43s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 133, Batch_no 12, 10% \t train_loss: 0.50 took: 3.24s\n",
            "Epoch 133, Batch_no 25, 20% \t train_loss: 0.40 took: 2.39s\n",
            "Epoch 133, Batch_no 38, 30% \t train_loss: 0.48 took: 2.19s\n",
            "Epoch 133, Batch_no 51, 41% \t train_loss: 0.55 took: 2.06s\n",
            "Epoch 133, Batch_no 64, 51% \t train_loss: 0.57 took: 2.95s\n",
            "Epoch 133, Batch_no 77, 61% \t train_loss: 0.46 took: 2.12s\n",
            "Epoch 133, Batch_no 90, 72% \t train_loss: 0.54 took: 2.45s\n",
            "Epoch 133, Batch_no 103, 82% \t train_loss: 0.55 took: 2.80s\n",
            "Epoch 133, Batch_no 116, 92% \t train_loss: 0.40 took: 2.64s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 134, Batch_no 12, 10% \t train_loss: 0.53 took: 2.99s\n",
            "Epoch 134, Batch_no 25, 20% \t train_loss: 0.44 took: 2.52s\n",
            "Epoch 134, Batch_no 38, 30% \t train_loss: 0.36 took: 2.57s\n",
            "Epoch 134, Batch_no 51, 41% \t train_loss: 0.47 took: 2.31s\n",
            "Epoch 134, Batch_no 64, 51% \t train_loss: 0.46 took: 2.35s\n",
            "Epoch 134, Batch_no 77, 61% \t train_loss: 0.44 took: 2.64s\n",
            "Epoch 134, Batch_no 90, 72% \t train_loss: 0.42 took: 2.34s\n",
            "Epoch 134, Batch_no 103, 82% \t train_loss: 0.51 took: 2.28s\n",
            "Epoch 134, Batch_no 116, 92% \t train_loss: 0.54 took: 2.46s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.30\n",
            "Epoch 135, Batch_no 12, 10% \t train_loss: 0.50 took: 3.22s\n",
            "Epoch 135, Batch_no 25, 20% \t train_loss: 0.49 took: 2.35s\n",
            "Epoch 135, Batch_no 38, 30% \t train_loss: 0.56 took: 2.40s\n",
            "Epoch 135, Batch_no 51, 41% \t train_loss: 0.43 took: 2.54s\n",
            "Epoch 135, Batch_no 64, 51% \t train_loss: 0.46 took: 2.31s\n",
            "Epoch 135, Batch_no 77, 61% \t train_loss: 0.50 took: 2.17s\n",
            "Epoch 135, Batch_no 90, 72% \t train_loss: 0.46 took: 2.50s\n",
            "Epoch 135, Batch_no 103, 82% \t train_loss: 0.41 took: 2.48s\n",
            "Epoch 135, Batch_no 116, 92% \t train_loss: 0.51 took: 2.71s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 136, Batch_no 12, 10% \t train_loss: 0.41 took: 2.99s\n",
            "Epoch 136, Batch_no 25, 20% \t train_loss: 0.44 took: 2.64s\n",
            "Epoch 136, Batch_no 38, 30% \t train_loss: 0.37 took: 2.45s\n",
            "Epoch 136, Batch_no 51, 41% \t train_loss: 0.45 took: 2.43s\n",
            "Epoch 136, Batch_no 64, 51% \t train_loss: 0.51 took: 2.29s\n",
            "Epoch 136, Batch_no 77, 61% \t train_loss: 0.42 took: 2.35s\n",
            "Epoch 136, Batch_no 90, 72% \t train_loss: 0.45 took: 2.62s\n",
            "Epoch 136, Batch_no 103, 82% \t train_loss: 0.39 took: 2.66s\n",
            "Epoch 136, Batch_no 116, 92% \t train_loss: 0.52 took: 2.14s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 137, Batch_no 12, 10% \t train_loss: 0.42 took: 3.18s\n",
            "Epoch 137, Batch_no 25, 20% \t train_loss: 0.50 took: 2.54s\n",
            "Epoch 137, Batch_no 38, 30% \t train_loss: 0.42 took: 1.94s\n",
            "Epoch 137, Batch_no 51, 41% \t train_loss: 0.44 took: 2.60s\n",
            "Epoch 137, Batch_no 64, 51% \t train_loss: 0.46 took: 2.39s\n",
            "Epoch 137, Batch_no 77, 61% \t train_loss: 0.37 took: 2.54s\n",
            "Epoch 137, Batch_no 90, 72% \t train_loss: 0.46 took: 2.85s\n",
            "Epoch 137, Batch_no 103, 82% \t train_loss: 0.42 took: 2.13s\n",
            "Epoch 137, Batch_no 116, 92% \t train_loss: 0.48 took: 2.19s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 138, Batch_no 12, 10% \t train_loss: 0.46 took: 3.47s\n",
            "Epoch 138, Batch_no 25, 20% \t train_loss: 0.40 took: 2.37s\n",
            "Epoch 138, Batch_no 38, 30% \t train_loss: 0.43 took: 2.07s\n",
            "Epoch 138, Batch_no 51, 41% \t train_loss: 0.36 took: 2.51s\n",
            "Epoch 138, Batch_no 64, 51% \t train_loss: 0.51 took: 2.43s\n",
            "Epoch 138, Batch_no 77, 61% \t train_loss: 0.51 took: 2.55s\n",
            "Epoch 138, Batch_no 90, 72% \t train_loss: 0.46 took: 2.48s\n",
            "Epoch 138, Batch_no 103, 82% \t train_loss: 0.49 took: 2.60s\n",
            "Epoch 138, Batch_no 116, 92% \t train_loss: 0.51 took: 2.53s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 139, Batch_no 12, 10% \t train_loss: 0.43 took: 2.96s\n",
            "Epoch 139, Batch_no 25, 20% \t train_loss: 0.36 took: 2.97s\n",
            "Epoch 139, Batch_no 38, 30% \t train_loss: 0.53 took: 2.04s\n",
            "Epoch 139, Batch_no 51, 41% \t train_loss: 0.50 took: 2.19s\n",
            "Epoch 139, Batch_no 64, 51% \t train_loss: 0.43 took: 3.00s\n",
            "Epoch 139, Batch_no 77, 61% \t train_loss: 0.46 took: 2.34s\n",
            "Epoch 139, Batch_no 90, 72% \t train_loss: 0.49 took: 2.22s\n",
            "Epoch 139, Batch_no 103, 82% \t train_loss: 0.40 took: 2.26s\n",
            "Epoch 139, Batch_no 116, 92% \t train_loss: 0.41 took: 2.64s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 140, Batch_no 12, 10% \t train_loss: 0.49 took: 3.09s\n",
            "Epoch 140, Batch_no 25, 20% \t train_loss: 0.44 took: 2.39s\n",
            "Epoch 140, Batch_no 38, 30% \t train_loss: 0.43 took: 2.07s\n",
            "Epoch 140, Batch_no 51, 41% \t train_loss: 0.42 took: 2.45s\n",
            "Epoch 140, Batch_no 64, 51% \t train_loss: 0.44 took: 2.98s\n",
            "Epoch 140, Batch_no 77, 61% \t train_loss: 0.46 took: 2.08s\n",
            "Epoch 140, Batch_no 90, 72% \t train_loss: 0.57 took: 2.36s\n",
            "Epoch 140, Batch_no 103, 82% \t train_loss: 0.45 took: 2.57s\n",
            "Epoch 140, Batch_no 116, 92% \t train_loss: 0.43 took: 3.07s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 141, Batch_no 12, 10% \t train_loss: 0.47 took: 3.33s\n",
            "Epoch 141, Batch_no 25, 20% \t train_loss: 0.43 took: 2.58s\n",
            "Epoch 141, Batch_no 38, 30% \t train_loss: 0.38 took: 2.42s\n",
            "Epoch 141, Batch_no 51, 41% \t train_loss: 0.39 took: 2.36s\n",
            "Epoch 141, Batch_no 64, 51% \t train_loss: 0.50 took: 2.81s\n",
            "Epoch 141, Batch_no 77, 61% \t train_loss: 0.45 took: 2.07s\n",
            "Epoch 141, Batch_no 90, 72% \t train_loss: 0.41 took: 1.86s\n",
            "Epoch 141, Batch_no 103, 82% \t train_loss: 0.43 took: 2.33s\n",
            "Epoch 141, Batch_no 116, 92% \t train_loss: 0.49 took: 3.23s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 142, Batch_no 12, 10% \t train_loss: 0.41 took: 3.41s\n",
            "Epoch 142, Batch_no 25, 20% \t train_loss: 0.50 took: 2.49s\n",
            "Epoch 142, Batch_no 38, 30% \t train_loss: 0.46 took: 2.25s\n",
            "Epoch 142, Batch_no 51, 41% \t train_loss: 0.39 took: 2.31s\n",
            "Epoch 142, Batch_no 64, 51% \t train_loss: 0.38 took: 2.22s\n",
            "Epoch 142, Batch_no 77, 61% \t train_loss: 0.42 took: 2.90s\n",
            "Epoch 142, Batch_no 90, 72% \t train_loss: 0.42 took: 2.09s\n",
            "Epoch 142, Batch_no 103, 82% \t train_loss: 0.58 took: 2.81s\n",
            "Epoch 142, Batch_no 116, 92% \t train_loss: 0.36 took: 2.06s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 143, Batch_no 12, 10% \t train_loss: 0.50 took: 3.25s\n",
            "Epoch 143, Batch_no 25, 20% \t train_loss: 0.48 took: 2.43s\n",
            "Epoch 143, Batch_no 38, 30% \t train_loss: 0.48 took: 2.43s\n",
            "Epoch 143, Batch_no 51, 41% \t train_loss: 0.39 took: 2.21s\n",
            "Epoch 143, Batch_no 64, 51% \t train_loss: 0.40 took: 2.37s\n",
            "Epoch 143, Batch_no 77, 61% \t train_loss: 0.38 took: 2.43s\n",
            "Epoch 143, Batch_no 90, 72% \t train_loss: 0.42 took: 2.24s\n",
            "Epoch 143, Batch_no 103, 82% \t train_loss: 0.47 took: 2.70s\n",
            "Epoch 143, Batch_no 116, 92% \t train_loss: 0.49 took: 2.47s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.29\n",
            "Epoch 144, Batch_no 12, 10% \t train_loss: 0.47 took: 3.48s\n",
            "Epoch 144, Batch_no 25, 20% \t train_loss: 0.51 took: 2.01s\n",
            "Epoch 144, Batch_no 38, 30% \t train_loss: 0.52 took: 2.14s\n",
            "Epoch 144, Batch_no 51, 41% \t train_loss: 0.54 took: 2.37s\n",
            "Epoch 144, Batch_no 64, 51% \t train_loss: 0.47 took: 2.39s\n",
            "Epoch 144, Batch_no 77, 61% \t train_loss: 0.48 took: 3.15s\n",
            "Epoch 144, Batch_no 90, 72% \t train_loss: 0.48 took: 2.28s\n",
            "Epoch 144, Batch_no 103, 82% \t train_loss: 0.51 took: 2.08s\n",
            "Epoch 144, Batch_no 116, 92% \t train_loss: 0.41 took: 2.55s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 145, Batch_no 12, 10% \t train_loss: 0.47 took: 3.23s\n",
            "Epoch 145, Batch_no 25, 20% \t train_loss: 0.47 took: 2.73s\n",
            "Epoch 145, Batch_no 38, 30% \t train_loss: 0.39 took: 2.25s\n",
            "Epoch 145, Batch_no 51, 41% \t train_loss: 0.45 took: 2.35s\n",
            "Epoch 145, Batch_no 64, 51% \t train_loss: 0.54 took: 2.85s\n",
            "Epoch 145, Batch_no 77, 61% \t train_loss: 0.63 took: 2.20s\n",
            "Epoch 145, Batch_no 90, 72% \t train_loss: 0.50 took: 2.45s\n",
            "Epoch 145, Batch_no 103, 82% \t train_loss: 0.46 took: 2.32s\n",
            "Epoch 145, Batch_no 116, 92% \t train_loss: 0.37 took: 2.57s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 146, Batch_no 12, 10% \t train_loss: 0.44 took: 3.15s\n",
            "Epoch 146, Batch_no 25, 20% \t train_loss: 0.48 took: 2.29s\n",
            "Epoch 146, Batch_no 38, 30% \t train_loss: 0.49 took: 2.74s\n",
            "Epoch 146, Batch_no 51, 41% \t train_loss: 0.42 took: 2.32s\n",
            "Epoch 146, Batch_no 64, 51% \t train_loss: 0.46 took: 2.58s\n",
            "Epoch 146, Batch_no 77, 61% \t train_loss: 0.51 took: 2.72s\n",
            "Epoch 146, Batch_no 90, 72% \t train_loss: 0.48 took: 2.35s\n",
            "Epoch 146, Batch_no 103, 82% \t train_loss: 0.46 took: 2.47s\n",
            "Epoch 146, Batch_no 116, 92% \t train_loss: 0.40 took: 2.63s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 147, Batch_no 12, 10% \t train_loss: 0.37 took: 3.04s\n",
            "Epoch 147, Batch_no 25, 20% \t train_loss: 0.51 took: 2.58s\n",
            "Epoch 147, Batch_no 38, 30% \t train_loss: 0.47 took: 2.32s\n",
            "Epoch 147, Batch_no 51, 41% \t train_loss: 0.46 took: 2.40s\n",
            "Epoch 147, Batch_no 64, 51% \t train_loss: 0.53 took: 2.22s\n",
            "Epoch 147, Batch_no 77, 61% \t train_loss: 0.39 took: 3.10s\n",
            "Epoch 147, Batch_no 90, 72% \t train_loss: 0.48 took: 2.44s\n",
            "Epoch 147, Batch_no 103, 82% \t train_loss: 0.66 took: 2.18s\n",
            "Epoch 147, Batch_no 116, 92% \t train_loss: 0.46 took: 2.14s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 148, Batch_no 12, 10% \t train_loss: 0.51 took: 3.18s\n",
            "Epoch 148, Batch_no 25, 20% \t train_loss: 0.53 took: 2.40s\n",
            "Epoch 148, Batch_no 38, 30% \t train_loss: 0.42 took: 2.32s\n",
            "Epoch 148, Batch_no 51, 41% \t train_loss: 0.45 took: 2.15s\n",
            "Epoch 148, Batch_no 64, 51% \t train_loss: 0.51 took: 2.73s\n",
            "Epoch 148, Batch_no 77, 61% \t train_loss: 0.49 took: 2.40s\n",
            "Epoch 148, Batch_no 90, 72% \t train_loss: 0.43 took: 2.13s\n",
            "Epoch 148, Batch_no 103, 82% \t train_loss: 0.43 took: 2.01s\n",
            "Epoch 148, Batch_no 116, 92% \t train_loss: 0.53 took: 3.16s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 149, Batch_no 12, 10% \t train_loss: 0.47 took: 3.36s\n",
            "Epoch 149, Batch_no 25, 20% \t train_loss: 0.38 took: 2.35s\n",
            "Epoch 149, Batch_no 38, 30% \t train_loss: 0.50 took: 2.35s\n",
            "Epoch 149, Batch_no 51, 41% \t train_loss: 0.55 took: 2.28s\n",
            "Epoch 149, Batch_no 64, 51% \t train_loss: 0.42 took: 2.75s\n",
            "Epoch 149, Batch_no 77, 61% \t train_loss: 0.53 took: 2.32s\n",
            "Epoch 149, Batch_no 90, 72% \t train_loss: 0.35 took: 2.32s\n",
            "Epoch 149, Batch_no 103, 82% \t train_loss: 0.35 took: 2.28s\n",
            "Epoch 149, Batch_no 116, 92% \t train_loss: 0.43 took: 2.68s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 150, Batch_no 12, 10% \t train_loss: 0.57 took: 2.85s\n",
            "Epoch 150, Batch_no 25, 20% \t train_loss: 0.48 took: 2.77s\n",
            "Epoch 150, Batch_no 38, 30% \t train_loss: 0.41 took: 2.24s\n",
            "Epoch 150, Batch_no 51, 41% \t train_loss: 0.40 took: 2.08s\n",
            "Epoch 150, Batch_no 64, 51% \t train_loss: 0.44 took: 2.36s\n",
            "Epoch 150, Batch_no 77, 61% \t train_loss: 0.51 took: 2.73s\n",
            "Epoch 150, Batch_no 90, 72% \t train_loss: 0.54 took: 2.03s\n",
            "Epoch 150, Batch_no 103, 82% \t train_loss: 0.45 took: 2.71s\n",
            "Epoch 150, Batch_no 116, 92% \t train_loss: 0.38 took: 2.33s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 151, Batch_no 12, 10% \t train_loss: 0.42 took: 3.18s\n",
            "Epoch 151, Batch_no 25, 20% \t train_loss: 0.47 took: 2.15s\n",
            "Epoch 151, Batch_no 38, 30% \t train_loss: 0.53 took: 2.71s\n",
            "Epoch 151, Batch_no 51, 41% \t train_loss: 0.43 took: 2.01s\n",
            "Epoch 151, Batch_no 64, 51% \t train_loss: 0.43 took: 2.95s\n",
            "Epoch 151, Batch_no 77, 61% \t train_loss: 0.42 took: 2.15s\n",
            "Epoch 151, Batch_no 90, 72% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 151, Batch_no 103, 82% \t train_loss: 0.47 took: 2.40s\n",
            "Epoch 151, Batch_no 116, 92% \t train_loss: 0.53 took: 2.97s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 152, Batch_no 12, 10% \t train_loss: 0.45 took: 3.35s\n",
            "Epoch 152, Batch_no 25, 20% \t train_loss: 0.43 took: 2.37s\n",
            "Epoch 152, Batch_no 38, 30% \t train_loss: 0.42 took: 2.40s\n",
            "Epoch 152, Batch_no 51, 41% \t train_loss: 0.47 took: 2.43s\n",
            "Epoch 152, Batch_no 64, 51% \t train_loss: 0.53 took: 2.76s\n",
            "Epoch 152, Batch_no 77, 61% \t train_loss: 0.46 took: 2.24s\n",
            "Epoch 152, Batch_no 90, 72% \t train_loss: 0.40 took: 2.44s\n",
            "Epoch 152, Batch_no 103, 82% \t train_loss: 0.54 took: 2.11s\n",
            "Epoch 152, Batch_no 116, 92% \t train_loss: 0.49 took: 2.43s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 86 %\n",
            "Validation loss = 0.30\n",
            "Epoch 153, Batch_no 12, 10% \t train_loss: 0.44 took: 3.13s\n",
            "Epoch 153, Batch_no 25, 20% \t train_loss: 0.48 took: 2.24s\n",
            "Epoch 153, Batch_no 38, 30% \t train_loss: 0.44 took: 2.11s\n",
            "Epoch 153, Batch_no 51, 41% \t train_loss: 0.46 took: 2.46s\n",
            "Epoch 153, Batch_no 64, 51% \t train_loss: 0.51 took: 2.40s\n",
            "Epoch 153, Batch_no 77, 61% \t train_loss: 0.43 took: 2.60s\n",
            "Epoch 153, Batch_no 90, 72% \t train_loss: 0.39 took: 2.27s\n",
            "Epoch 153, Batch_no 103, 82% \t train_loss: 0.47 took: 2.93s\n",
            "Epoch 153, Batch_no 116, 92% \t train_loss: 0.54 took: 2.49s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 154, Batch_no 12, 10% \t train_loss: 0.49 took: 3.02s\n",
            "Epoch 154, Batch_no 25, 20% \t train_loss: 0.39 took: 2.31s\n",
            "Epoch 154, Batch_no 38, 30% \t train_loss: 0.46 took: 2.93s\n",
            "Epoch 154, Batch_no 51, 41% \t train_loss: 0.32 took: 2.55s\n",
            "Epoch 154, Batch_no 64, 51% \t train_loss: 0.41 took: 2.46s\n",
            "Epoch 154, Batch_no 77, 61% \t train_loss: 0.55 took: 2.40s\n",
            "Epoch 154, Batch_no 90, 72% \t train_loss: 0.40 took: 2.79s\n",
            "Epoch 154, Batch_no 103, 82% \t train_loss: 0.52 took: 2.29s\n",
            "Epoch 154, Batch_no 116, 92% \t train_loss: 0.56 took: 2.14s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 155, Batch_no 12, 10% \t train_loss: 0.42 took: 3.07s\n",
            "Epoch 155, Batch_no 25, 20% \t train_loss: 0.41 took: 2.71s\n",
            "Epoch 155, Batch_no 38, 30% \t train_loss: 0.46 took: 2.28s\n",
            "Epoch 155, Batch_no 51, 41% \t train_loss: 0.41 took: 2.31s\n",
            "Epoch 155, Batch_no 64, 51% \t train_loss: 0.51 took: 2.19s\n",
            "Epoch 155, Batch_no 77, 61% \t train_loss: 0.56 took: 2.93s\n",
            "Epoch 155, Batch_no 90, 72% \t train_loss: 0.52 took: 2.34s\n",
            "Epoch 155, Batch_no 103, 82% \t train_loss: 0.47 took: 2.29s\n",
            "Epoch 155, Batch_no 116, 92% \t train_loss: 0.40 took: 2.26s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 156, Batch_no 12, 10% \t train_loss: 0.47 took: 2.99s\n",
            "Epoch 156, Batch_no 25, 20% \t train_loss: 0.47 took: 2.54s\n",
            "Epoch 156, Batch_no 38, 30% \t train_loss: 0.48 took: 2.32s\n",
            "Epoch 156, Batch_no 51, 41% \t train_loss: 0.37 took: 2.28s\n",
            "Epoch 156, Batch_no 64, 51% \t train_loss: 0.47 took: 2.68s\n",
            "Epoch 156, Batch_no 77, 61% \t train_loss: 0.42 took: 2.38s\n",
            "Epoch 156, Batch_no 90, 72% \t train_loss: 0.42 took: 2.47s\n",
            "Epoch 156, Batch_no 103, 82% \t train_loss: 0.58 took: 2.26s\n",
            "Epoch 156, Batch_no 116, 92% \t train_loss: 0.38 took: 2.49s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 157, Batch_no 12, 10% \t train_loss: 0.53 took: 3.16s\n",
            "Epoch 157, Batch_no 25, 20% \t train_loss: 0.43 took: 2.18s\n",
            "Epoch 157, Batch_no 38, 30% \t train_loss: 0.48 took: 2.59s\n",
            "Epoch 157, Batch_no 51, 41% \t train_loss: 0.48 took: 2.26s\n",
            "Epoch 157, Batch_no 64, 51% \t train_loss: 0.55 took: 2.22s\n",
            "Epoch 157, Batch_no 77, 61% \t train_loss: 0.44 took: 2.70s\n",
            "Epoch 157, Batch_no 90, 72% \t train_loss: 0.45 took: 2.37s\n",
            "Epoch 157, Batch_no 103, 82% \t train_loss: 0.50 took: 2.09s\n",
            "Epoch 157, Batch_no 116, 92% \t train_loss: 0.42 took: 2.42s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 158, Batch_no 12, 10% \t train_loss: 0.41 took: 3.11s\n",
            "Epoch 158, Batch_no 25, 20% \t train_loss: 0.44 took: 2.70s\n",
            "Epoch 158, Batch_no 38, 30% \t train_loss: 0.46 took: 2.27s\n",
            "Epoch 158, Batch_no 51, 41% \t train_loss: 0.50 took: 2.35s\n",
            "Epoch 158, Batch_no 64, 51% \t train_loss: 0.53 took: 2.09s\n",
            "Epoch 158, Batch_no 77, 61% \t train_loss: 0.44 took: 3.00s\n",
            "Epoch 158, Batch_no 90, 72% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 158, Batch_no 103, 82% \t train_loss: 0.48 took: 2.37s\n",
            "Epoch 158, Batch_no 116, 92% \t train_loss: 0.52 took: 2.04s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 159, Batch_no 12, 10% \t train_loss: 0.47 took: 3.03s\n",
            "Epoch 159, Batch_no 25, 20% \t train_loss: 0.52 took: 2.24s\n",
            "Epoch 159, Batch_no 38, 30% \t train_loss: 0.50 took: 2.47s\n",
            "Epoch 159, Batch_no 51, 41% \t train_loss: 0.43 took: 2.56s\n",
            "Epoch 159, Batch_no 64, 51% \t train_loss: 0.52 took: 2.47s\n",
            "Epoch 159, Batch_no 77, 61% \t train_loss: 0.47 took: 2.32s\n",
            "Epoch 159, Batch_no 90, 72% \t train_loss: 0.42 took: 2.29s\n",
            "Epoch 159, Batch_no 103, 82% \t train_loss: 0.39 took: 2.23s\n",
            "Epoch 159, Batch_no 116, 92% \t train_loss: 0.49 took: 3.08s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 160, Batch_no 12, 10% \t train_loss: 0.46 took: 2.63s\n",
            "Epoch 160, Batch_no 25, 20% \t train_loss: 0.49 took: 2.35s\n",
            "Epoch 160, Batch_no 38, 30% \t train_loss: 0.37 took: 2.41s\n",
            "Epoch 160, Batch_no 51, 41% \t train_loss: 0.42 took: 2.78s\n",
            "Epoch 160, Batch_no 64, 51% \t train_loss: 0.47 took: 2.46s\n",
            "Epoch 160, Batch_no 77, 61% \t train_loss: 0.46 took: 2.59s\n",
            "Epoch 160, Batch_no 90, 72% \t train_loss: 0.54 took: 2.48s\n",
            "Epoch 160, Batch_no 103, 82% \t train_loss: 0.42 took: 2.48s\n",
            "Epoch 160, Batch_no 116, 92% \t train_loss: 0.52 took: 2.08s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 161, Batch_no 12, 10% \t train_loss: 0.38 took: 3.09s\n",
            "Epoch 161, Batch_no 25, 20% \t train_loss: 0.36 took: 2.31s\n",
            "Epoch 161, Batch_no 38, 30% \t train_loss: 0.63 took: 2.29s\n",
            "Epoch 161, Batch_no 51, 41% \t train_loss: 0.53 took: 2.67s\n",
            "Epoch 161, Batch_no 64, 51% \t train_loss: 0.50 took: 2.20s\n",
            "Epoch 161, Batch_no 77, 61% \t train_loss: 0.48 took: 2.88s\n",
            "Epoch 161, Batch_no 90, 72% \t train_loss: 0.53 took: 2.31s\n",
            "Epoch 161, Batch_no 103, 82% \t train_loss: 0.58 took: 2.31s\n",
            "Epoch 161, Batch_no 116, 92% \t train_loss: 0.43 took: 2.28s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 162, Batch_no 12, 10% \t train_loss: 0.44 took: 3.20s\n",
            "Epoch 162, Batch_no 25, 20% \t train_loss: 0.44 took: 2.19s\n",
            "Epoch 162, Batch_no 38, 30% \t train_loss: 0.43 took: 2.50s\n",
            "Epoch 162, Batch_no 51, 41% \t train_loss: 0.49 took: 2.20s\n",
            "Epoch 162, Batch_no 64, 51% \t train_loss: 0.63 took: 2.93s\n",
            "Epoch 162, Batch_no 77, 61% \t train_loss: 0.56 took: 2.10s\n",
            "Epoch 162, Batch_no 90, 72% \t train_loss: 0.38 took: 2.34s\n",
            "Epoch 162, Batch_no 103, 82% \t train_loss: 0.47 took: 2.39s\n",
            "Epoch 162, Batch_no 116, 92% \t train_loss: 0.44 took: 2.93s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 163, Batch_no 12, 10% \t train_loss: 0.37 took: 3.32s\n",
            "Epoch 163, Batch_no 25, 20% \t train_loss: 0.42 took: 2.20s\n",
            "Epoch 163, Batch_no 38, 30% \t train_loss: 0.41 took: 2.88s\n",
            "Epoch 163, Batch_no 51, 41% \t train_loss: 0.47 took: 2.09s\n",
            "Epoch 163, Batch_no 64, 51% \t train_loss: 0.49 took: 2.40s\n",
            "Epoch 163, Batch_no 77, 61% \t train_loss: 0.41 took: 2.07s\n",
            "Epoch 163, Batch_no 90, 72% \t train_loss: 0.48 took: 2.56s\n",
            "Epoch 163, Batch_no 103, 82% \t train_loss: 0.41 took: 2.27s\n",
            "Epoch 163, Batch_no 116, 92% \t train_loss: 0.45 took: 2.59s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 164, Batch_no 12, 10% \t train_loss: 0.47 took: 3.54s\n",
            "Epoch 164, Batch_no 25, 20% \t train_loss: 0.46 took: 2.07s\n",
            "Epoch 164, Batch_no 38, 30% \t train_loss: 0.53 took: 2.43s\n",
            "Epoch 164, Batch_no 51, 41% \t train_loss: 0.47 took: 2.03s\n",
            "Epoch 164, Batch_no 64, 51% \t train_loss: 0.39 took: 2.93s\n",
            "Epoch 164, Batch_no 77, 61% \t train_loss: 0.51 took: 2.25s\n",
            "Epoch 164, Batch_no 90, 72% \t train_loss: 0.45 took: 2.47s\n",
            "Epoch 164, Batch_no 103, 82% \t train_loss: 0.38 took: 2.46s\n",
            "Epoch 164, Batch_no 116, 92% \t train_loss: 0.43 took: 2.26s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 165, Batch_no 12, 10% \t train_loss: 0.40 took: 3.26s\n",
            "Epoch 165, Batch_no 25, 20% \t train_loss: 0.43 took: 2.14s\n",
            "Epoch 165, Batch_no 38, 30% \t train_loss: 0.66 took: 2.78s\n",
            "Epoch 165, Batch_no 51, 41% \t train_loss: 0.52 took: 1.96s\n",
            "Epoch 165, Batch_no 64, 51% \t train_loss: 0.39 took: 2.19s\n",
            "Epoch 165, Batch_no 77, 61% \t train_loss: 0.46 took: 2.81s\n",
            "Epoch 165, Batch_no 90, 72% \t train_loss: 0.46 took: 2.65s\n",
            "Epoch 165, Batch_no 103, 82% \t train_loss: 0.44 took: 2.32s\n",
            "Epoch 165, Batch_no 116, 92% \t train_loss: 0.56 took: 2.33s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 166, Batch_no 12, 10% \t train_loss: 0.44 took: 3.00s\n",
            "Epoch 166, Batch_no 25, 20% \t train_loss: 0.47 took: 2.35s\n",
            "Epoch 166, Batch_no 38, 30% \t train_loss: 0.38 took: 2.41s\n",
            "Epoch 166, Batch_no 51, 41% \t train_loss: 0.46 took: 2.55s\n",
            "Epoch 166, Batch_no 64, 51% \t train_loss: 0.43 took: 2.37s\n",
            "Epoch 166, Batch_no 77, 61% \t train_loss: 0.52 took: 2.55s\n",
            "Epoch 166, Batch_no 90, 72% \t train_loss: 0.52 took: 1.95s\n",
            "Epoch 166, Batch_no 103, 82% \t train_loss: 0.50 took: 2.98s\n",
            "Epoch 166, Batch_no 116, 92% \t train_loss: 0.45 took: 2.33s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 167, Batch_no 12, 10% \t train_loss: 0.51 took: 3.00s\n",
            "Epoch 167, Batch_no 25, 20% \t train_loss: 0.47 took: 2.98s\n",
            "Epoch 167, Batch_no 38, 30% \t train_loss: 0.46 took: 2.11s\n",
            "Epoch 167, Batch_no 51, 41% \t train_loss: 0.43 took: 2.25s\n",
            "Epoch 167, Batch_no 64, 51% \t train_loss: 0.48 took: 2.35s\n",
            "Epoch 167, Batch_no 77, 61% \t train_loss: 0.50 took: 2.78s\n",
            "Epoch 167, Batch_no 90, 72% \t train_loss: 0.41 took: 2.07s\n",
            "Epoch 167, Batch_no 103, 82% \t train_loss: 0.39 took: 2.21s\n",
            "Epoch 167, Batch_no 116, 92% \t train_loss: 0.42 took: 2.68s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.30\n",
            "Epoch 168, Batch_no 12, 10% \t train_loss: 0.56 took: 2.83s\n",
            "Epoch 168, Batch_no 25, 20% \t train_loss: 0.52 took: 2.72s\n",
            "Epoch 168, Batch_no 38, 30% \t train_loss: 0.42 took: 2.07s\n",
            "Epoch 168, Batch_no 51, 41% \t train_loss: 0.46 took: 2.29s\n",
            "Epoch 168, Batch_no 64, 51% \t train_loss: 0.46 took: 2.30s\n",
            "Epoch 168, Batch_no 77, 61% \t train_loss: 0.49 took: 2.59s\n",
            "Epoch 168, Batch_no 90, 72% \t train_loss: 0.41 took: 2.88s\n",
            "Epoch 168, Batch_no 103, 82% \t train_loss: 0.47 took: 2.47s\n",
            "Epoch 168, Batch_no 116, 92% \t train_loss: 0.47 took: 2.43s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 169, Batch_no 12, 10% \t train_loss: 0.46 took: 3.08s\n",
            "Epoch 169, Batch_no 25, 20% \t train_loss: 0.37 took: 2.44s\n",
            "Epoch 169, Batch_no 38, 30% \t train_loss: 0.51 took: 2.21s\n",
            "Epoch 169, Batch_no 51, 41% \t train_loss: 0.43 took: 2.07s\n",
            "Epoch 169, Batch_no 64, 51% \t train_loss: 0.54 took: 3.05s\n",
            "Epoch 169, Batch_no 77, 61% \t train_loss: 0.52 took: 2.67s\n",
            "Epoch 169, Batch_no 90, 72% \t train_loss: 0.44 took: 2.05s\n",
            "Epoch 169, Batch_no 103, 82% \t train_loss: 0.54 took: 1.96s\n",
            "Epoch 169, Batch_no 116, 92% \t train_loss: 0.47 took: 3.17s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 170, Batch_no 12, 10% \t train_loss: 0.49 took: 3.14s\n",
            "Epoch 170, Batch_no 25, 20% \t train_loss: 0.43 took: 2.41s\n",
            "Epoch 170, Batch_no 38, 30% \t train_loss: 0.55 took: 2.47s\n",
            "Epoch 170, Batch_no 51, 41% \t train_loss: 0.48 took: 2.17s\n",
            "Epoch 170, Batch_no 64, 51% \t train_loss: 0.48 took: 2.36s\n",
            "Epoch 170, Batch_no 77, 61% \t train_loss: 0.40 took: 2.33s\n",
            "Epoch 170, Batch_no 90, 72% \t train_loss: 0.37 took: 2.26s\n",
            "Epoch 170, Batch_no 103, 82% \t train_loss: 0.37 took: 2.84s\n",
            "Epoch 170, Batch_no 116, 92% \t train_loss: 0.44 took: 2.50s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 171, Batch_no 12, 10% \t train_loss: 0.38 took: 3.11s\n",
            "Epoch 171, Batch_no 25, 20% \t train_loss: 0.56 took: 1.97s\n",
            "Epoch 171, Batch_no 38, 30% \t train_loss: 0.45 took: 2.50s\n",
            "Epoch 171, Batch_no 51, 41% \t train_loss: 0.53 took: 2.94s\n",
            "Epoch 171, Batch_no 64, 51% \t train_loss: 0.47 took: 1.92s\n",
            "Epoch 171, Batch_no 77, 61% \t train_loss: 0.43 took: 2.44s\n",
            "Epoch 171, Batch_no 90, 72% \t train_loss: 0.44 took: 2.74s\n",
            "Epoch 171, Batch_no 103, 82% \t train_loss: 0.50 took: 1.96s\n",
            "Epoch 171, Batch_no 116, 92% \t train_loss: 0.45 took: 2.61s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 172, Batch_no 12, 10% \t train_loss: 0.40 took: 2.97s\n",
            "Epoch 172, Batch_no 25, 20% \t train_loss: 0.51 took: 2.26s\n",
            "Epoch 172, Batch_no 38, 30% \t train_loss: 0.40 took: 2.55s\n",
            "Epoch 172, Batch_no 51, 41% \t train_loss: 0.46 took: 2.04s\n",
            "Epoch 172, Batch_no 64, 51% \t train_loss: 0.45 took: 2.99s\n",
            "Epoch 172, Batch_no 77, 61% \t train_loss: 0.41 took: 2.30s\n",
            "Epoch 172, Batch_no 90, 72% \t train_loss: 0.44 took: 2.36s\n",
            "Epoch 172, Batch_no 103, 82% \t train_loss: 0.42 took: 2.44s\n",
            "Epoch 172, Batch_no 116, 92% \t train_loss: 0.41 took: 2.25s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 173, Batch_no 12, 10% \t train_loss: 0.44 took: 3.02s\n",
            "Epoch 173, Batch_no 25, 20% \t train_loss: 0.54 took: 2.40s\n",
            "Epoch 173, Batch_no 38, 30% \t train_loss: 0.47 took: 2.74s\n",
            "Epoch 173, Batch_no 51, 41% \t train_loss: 0.45 took: 2.21s\n",
            "Epoch 173, Batch_no 64, 51% \t train_loss: 0.55 took: 2.58s\n",
            "Epoch 173, Batch_no 77, 61% \t train_loss: 0.42 took: 2.18s\n",
            "Epoch 173, Batch_no 90, 72% \t train_loss: 0.42 took: 2.12s\n",
            "Epoch 173, Batch_no 103, 82% \t train_loss: 0.43 took: 2.19s\n",
            "Epoch 173, Batch_no 116, 92% \t train_loss: 0.38 took: 2.78s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 174, Batch_no 12, 10% \t train_loss: 0.36 took: 2.87s\n",
            "Epoch 174, Batch_no 25, 20% \t train_loss: 0.54 took: 2.45s\n",
            "Epoch 174, Batch_no 38, 30% \t train_loss: 0.56 took: 2.23s\n",
            "Epoch 174, Batch_no 51, 41% \t train_loss: 0.51 took: 2.29s\n",
            "Epoch 174, Batch_no 64, 51% \t train_loss: 0.43 took: 3.12s\n",
            "Epoch 174, Batch_no 77, 61% \t train_loss: 0.48 took: 2.31s\n",
            "Epoch 174, Batch_no 90, 72% \t train_loss: 0.48 took: 2.45s\n",
            "Epoch 174, Batch_no 103, 82% \t train_loss: 0.43 took: 2.14s\n",
            "Epoch 174, Batch_no 116, 92% \t train_loss: 0.49 took: 2.99s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.30\n",
            "Epoch 175, Batch_no 12, 10% \t train_loss: 0.56 took: 2.74s\n",
            "Epoch 175, Batch_no 25, 20% \t train_loss: 0.35 took: 2.77s\n",
            "Epoch 175, Batch_no 38, 30% \t train_loss: 0.31 took: 2.15s\n",
            "Epoch 175, Batch_no 51, 41% \t train_loss: 0.55 took: 2.30s\n",
            "Epoch 175, Batch_no 64, 51% \t train_loss: 0.50 took: 2.19s\n",
            "Epoch 175, Batch_no 77, 61% \t train_loss: 0.45 took: 2.88s\n",
            "Epoch 175, Batch_no 90, 72% \t train_loss: 0.38 took: 2.26s\n",
            "Epoch 175, Batch_no 103, 82% \t train_loss: 0.39 took: 2.32s\n",
            "Epoch 175, Batch_no 116, 92% \t train_loss: 0.56 took: 2.21s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 176, Batch_no 12, 10% \t train_loss: 0.54 took: 3.10s\n",
            "Epoch 176, Batch_no 25, 20% \t train_loss: 0.43 took: 2.35s\n",
            "Epoch 176, Batch_no 38, 30% \t train_loss: 0.43 took: 2.30s\n",
            "Epoch 176, Batch_no 51, 41% \t train_loss: 0.36 took: 2.65s\n",
            "Epoch 176, Batch_no 64, 51% \t train_loss: 0.40 took: 2.55s\n",
            "Epoch 176, Batch_no 77, 61% \t train_loss: 0.37 took: 2.70s\n",
            "Epoch 176, Batch_no 90, 72% \t train_loss: 0.49 took: 2.36s\n",
            "Epoch 176, Batch_no 103, 82% \t train_loss: 0.41 took: 2.09s\n",
            "Epoch 176, Batch_no 116, 92% \t train_loss: 0.49 took: 2.19s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 177, Batch_no 12, 10% \t train_loss: 0.44 took: 3.21s\n",
            "Epoch 177, Batch_no 25, 20% \t train_loss: 0.48 took: 2.27s\n",
            "Epoch 177, Batch_no 38, 30% \t train_loss: 0.42 took: 2.60s\n",
            "Epoch 177, Batch_no 51, 41% \t train_loss: 0.47 took: 2.50s\n",
            "Epoch 177, Batch_no 64, 51% \t train_loss: 0.39 took: 2.39s\n",
            "Epoch 177, Batch_no 77, 61% \t train_loss: 0.51 took: 2.89s\n",
            "Epoch 177, Batch_no 90, 72% \t train_loss: 0.43 took: 2.19s\n",
            "Epoch 177, Batch_no 103, 82% \t train_loss: 0.44 took: 2.21s\n",
            "Epoch 177, Batch_no 116, 92% \t train_loss: 0.53 took: 2.17s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 178, Batch_no 12, 10% \t train_loss: 0.45 took: 2.85s\n",
            "Epoch 178, Batch_no 25, 20% \t train_loss: 0.50 took: 2.55s\n",
            "Epoch 178, Batch_no 38, 30% \t train_loss: 0.50 took: 2.34s\n",
            "Epoch 178, Batch_no 51, 41% \t train_loss: 0.51 took: 2.60s\n",
            "Epoch 178, Batch_no 64, 51% \t train_loss: 0.39 took: 2.12s\n",
            "Epoch 178, Batch_no 77, 61% \t train_loss: 0.46 took: 2.35s\n",
            "Epoch 178, Batch_no 90, 72% \t train_loss: 0.49 took: 2.93s\n",
            "Epoch 178, Batch_no 103, 82% \t train_loss: 0.32 took: 2.35s\n",
            "Epoch 178, Batch_no 116, 92% \t train_loss: 0.39 took: 2.20s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 179, Batch_no 12, 10% \t train_loss: 0.53 took: 2.96s\n",
            "Epoch 179, Batch_no 25, 20% \t train_loss: 0.47 took: 2.42s\n",
            "Epoch 179, Batch_no 38, 30% \t train_loss: 0.40 took: 2.36s\n",
            "Epoch 179, Batch_no 51, 41% \t train_loss: 0.40 took: 2.43s\n",
            "Epoch 179, Batch_no 64, 51% \t train_loss: 0.43 took: 2.49s\n",
            "Epoch 179, Batch_no 77, 61% \t train_loss: 0.39 took: 2.59s\n",
            "Epoch 179, Batch_no 90, 72% \t train_loss: 0.55 took: 2.14s\n",
            "Epoch 179, Batch_no 103, 82% \t train_loss: 0.40 took: 2.31s\n",
            "Epoch 179, Batch_no 116, 92% \t train_loss: 0.48 took: 2.53s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 180, Batch_no 12, 10% \t train_loss: 0.50 took: 3.14s\n",
            "Epoch 180, Batch_no 25, 20% \t train_loss: 0.50 took: 2.38s\n",
            "Epoch 180, Batch_no 38, 30% \t train_loss: 0.38 took: 2.18s\n",
            "Epoch 180, Batch_no 51, 41% \t train_loss: 0.51 took: 2.35s\n",
            "Epoch 180, Batch_no 64, 51% \t train_loss: 0.48 took: 2.75s\n",
            "Epoch 180, Batch_no 77, 61% \t train_loss: 0.57 took: 2.21s\n",
            "Epoch 180, Batch_no 90, 72% \t train_loss: 0.35 took: 2.32s\n",
            "Epoch 180, Batch_no 103, 82% \t train_loss: 0.46 took: 3.13s\n",
            "Epoch 180, Batch_no 116, 92% \t train_loss: 0.44 took: 2.18s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 181, Batch_no 12, 10% \t train_loss: 0.42 took: 3.20s\n",
            "Epoch 181, Batch_no 25, 20% \t train_loss: 0.44 took: 2.54s\n",
            "Epoch 181, Batch_no 38, 30% \t train_loss: 0.55 took: 2.09s\n",
            "Epoch 181, Batch_no 51, 41% \t train_loss: 0.46 took: 2.15s\n",
            "Epoch 181, Batch_no 64, 51% \t train_loss: 0.39 took: 2.95s\n",
            "Epoch 181, Batch_no 77, 61% \t train_loss: 0.41 took: 2.17s\n",
            "Epoch 181, Batch_no 90, 72% \t train_loss: 0.40 took: 2.26s\n",
            "Epoch 181, Batch_no 103, 82% \t train_loss: 0.50 took: 2.38s\n",
            "Epoch 181, Batch_no 116, 92% \t train_loss: 0.46 took: 2.86s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 182, Batch_no 12, 10% \t train_loss: 0.45 took: 3.25s\n",
            "Epoch 182, Batch_no 25, 20% \t train_loss: 0.43 took: 2.48s\n",
            "Epoch 182, Batch_no 38, 30% \t train_loss: 0.44 took: 2.56s\n",
            "Epoch 182, Batch_no 51, 41% \t train_loss: 0.43 took: 1.99s\n",
            "Epoch 182, Batch_no 64, 51% \t train_loss: 0.39 took: 2.34s\n",
            "Epoch 182, Batch_no 77, 61% \t train_loss: 0.39 took: 2.55s\n",
            "Epoch 182, Batch_no 90, 72% \t train_loss: 0.37 took: 2.25s\n",
            "Epoch 182, Batch_no 103, 82% \t train_loss: 0.38 took: 2.16s\n",
            "Epoch 182, Batch_no 116, 92% \t train_loss: 0.57 took: 2.22s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 183, Batch_no 12, 10% \t train_loss: 0.46 took: 3.08s\n",
            "Epoch 183, Batch_no 25, 20% \t train_loss: 0.39 took: 2.50s\n",
            "Epoch 183, Batch_no 38, 30% \t train_loss: 0.44 took: 2.42s\n",
            "Epoch 183, Batch_no 51, 41% \t train_loss: 0.46 took: 2.15s\n",
            "Epoch 183, Batch_no 64, 51% \t train_loss: 0.50 took: 2.79s\n",
            "Epoch 183, Batch_no 77, 61% \t train_loss: 0.43 took: 2.67s\n",
            "Epoch 183, Batch_no 90, 72% \t train_loss: 0.47 took: 2.24s\n",
            "Epoch 183, Batch_no 103, 82% \t train_loss: 0.44 took: 2.12s\n",
            "Epoch 183, Batch_no 116, 92% \t train_loss: 0.43 took: 2.32s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.30\n",
            "Epoch 184, Batch_no 12, 10% \t train_loss: 0.43 took: 2.93s\n",
            "Epoch 184, Batch_no 25, 20% \t train_loss: 0.48 took: 2.53s\n",
            "Epoch 184, Batch_no 38, 30% \t train_loss: 0.36 took: 2.04s\n",
            "Epoch 184, Batch_no 51, 41% \t train_loss: 0.36 took: 2.48s\n",
            "Epoch 184, Batch_no 64, 51% \t train_loss: 0.47 took: 2.68s\n",
            "Epoch 184, Batch_no 77, 61% \t train_loss: 0.49 took: 2.41s\n",
            "Epoch 184, Batch_no 90, 72% \t train_loss: 0.48 took: 2.49s\n",
            "Epoch 184, Batch_no 103, 82% \t train_loss: 0.45 took: 2.34s\n",
            "Epoch 184, Batch_no 116, 92% \t train_loss: 0.48 took: 2.18s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 185, Batch_no 12, 10% \t train_loss: 0.38 took: 3.34s\n",
            "Epoch 185, Batch_no 25, 20% \t train_loss: 0.53 took: 2.54s\n",
            "Epoch 185, Batch_no 38, 30% \t train_loss: 0.46 took: 2.54s\n",
            "Epoch 185, Batch_no 51, 41% \t train_loss: 0.47 took: 2.16s\n",
            "Epoch 185, Batch_no 64, 51% \t train_loss: 0.46 took: 2.63s\n",
            "Epoch 185, Batch_no 77, 61% \t train_loss: 0.49 took: 2.21s\n",
            "Epoch 185, Batch_no 90, 72% \t train_loss: 0.53 took: 2.27s\n",
            "Epoch 185, Batch_no 103, 82% \t train_loss: 0.48 took: 2.11s\n",
            "Epoch 185, Batch_no 116, 92% \t train_loss: 0.50 took: 2.73s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 186, Batch_no 12, 10% \t train_loss: 0.51 took: 2.77s\n",
            "Epoch 186, Batch_no 25, 20% \t train_loss: 0.49 took: 2.93s\n",
            "Epoch 186, Batch_no 38, 30% \t train_loss: 0.42 took: 2.60s\n",
            "Epoch 186, Batch_no 51, 41% \t train_loss: 0.38 took: 2.12s\n",
            "Epoch 186, Batch_no 64, 51% \t train_loss: 0.45 took: 2.44s\n",
            "Epoch 186, Batch_no 77, 61% \t train_loss: 0.50 took: 2.87s\n",
            "Epoch 186, Batch_no 90, 72% \t train_loss: 0.50 took: 2.20s\n",
            "Epoch 186, Batch_no 103, 82% \t train_loss: 0.56 took: 2.30s\n",
            "Epoch 186, Batch_no 116, 92% \t train_loss: 0.38 took: 2.11s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 187, Batch_no 12, 10% \t train_loss: 0.38 took: 2.80s\n",
            "Epoch 187, Batch_no 25, 20% \t train_loss: 0.46 took: 2.30s\n",
            "Epoch 187, Batch_no 38, 30% \t train_loss: 0.48 took: 2.74s\n",
            "Epoch 187, Batch_no 51, 41% \t train_loss: 0.48 took: 2.02s\n",
            "Epoch 187, Batch_no 64, 51% \t train_loss: 0.51 took: 2.38s\n",
            "Epoch 187, Batch_no 77, 61% \t train_loss: 0.48 took: 2.32s\n",
            "Epoch 187, Batch_no 90, 72% \t train_loss: 0.43 took: 2.79s\n",
            "Epoch 187, Batch_no 103, 82% \t train_loss: 0.51 took: 2.35s\n",
            "Epoch 187, Batch_no 116, 92% \t train_loss: 0.46 took: 2.41s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 188, Batch_no 12, 10% \t train_loss: 0.52 took: 3.58s\n",
            "Epoch 188, Batch_no 25, 20% \t train_loss: 0.44 took: 2.55s\n",
            "Epoch 188, Batch_no 38, 30% \t train_loss: 0.35 took: 2.06s\n",
            "Epoch 188, Batch_no 51, 41% \t train_loss: 0.55 took: 2.25s\n",
            "Epoch 188, Batch_no 64, 51% \t train_loss: 0.48 took: 2.45s\n",
            "Epoch 188, Batch_no 77, 61% \t train_loss: 0.53 took: 2.42s\n",
            "Epoch 188, Batch_no 90, 72% \t train_loss: 0.38 took: 2.02s\n",
            "Epoch 188, Batch_no 103, 82% \t train_loss: 0.41 took: 3.01s\n",
            "Epoch 188, Batch_no 116, 92% \t train_loss: 0.43 took: 1.82s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 189, Batch_no 12, 10% \t train_loss: 0.39 took: 2.89s\n",
            "Epoch 189, Batch_no 25, 20% \t train_loss: 0.53 took: 2.43s\n",
            "Epoch 189, Batch_no 38, 30% \t train_loss: 0.42 took: 2.73s\n",
            "Epoch 189, Batch_no 51, 41% \t train_loss: 0.48 took: 2.46s\n",
            "Epoch 189, Batch_no 64, 51% \t train_loss: 0.43 took: 2.40s\n",
            "Epoch 189, Batch_no 77, 61% \t train_loss: 0.41 took: 2.16s\n",
            "Epoch 189, Batch_no 90, 72% \t train_loss: 0.47 took: 2.77s\n",
            "Epoch 189, Batch_no 103, 82% \t train_loss: 0.51 took: 2.09s\n",
            "Epoch 189, Batch_no 116, 92% \t train_loss: 0.46 took: 2.27s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 190, Batch_no 12, 10% \t train_loss: 0.46 took: 2.98s\n",
            "Epoch 190, Batch_no 25, 20% \t train_loss: 0.36 took: 2.54s\n",
            "Epoch 190, Batch_no 38, 30% \t train_loss: 0.37 took: 2.71s\n",
            "Epoch 190, Batch_no 51, 41% \t train_loss: 0.53 took: 2.26s\n",
            "Epoch 190, Batch_no 64, 51% \t train_loss: 0.43 took: 2.09s\n",
            "Epoch 190, Batch_no 77, 61% \t train_loss: 0.42 took: 2.71s\n",
            "Epoch 190, Batch_no 90, 72% \t train_loss: 0.46 took: 2.04s\n",
            "Epoch 190, Batch_no 103, 82% \t train_loss: 0.52 took: 2.28s\n",
            "Epoch 190, Batch_no 116, 92% \t train_loss: 0.50 took: 2.38s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 191, Batch_no 12, 10% \t train_loss: 0.41 took: 3.06s\n",
            "Epoch 191, Batch_no 25, 20% \t train_loss: 0.49 took: 2.49s\n",
            "Epoch 191, Batch_no 38, 30% \t train_loss: 0.44 took: 2.57s\n",
            "Epoch 191, Batch_no 51, 41% \t train_loss: 0.43 took: 2.15s\n",
            "Epoch 191, Batch_no 64, 51% \t train_loss: 0.47 took: 2.56s\n",
            "Epoch 191, Batch_no 77, 61% \t train_loss: 0.48 took: 2.04s\n",
            "Epoch 191, Batch_no 90, 72% \t train_loss: 0.44 took: 3.14s\n",
            "Epoch 191, Batch_no 103, 82% \t train_loss: 0.47 took: 1.73s\n",
            "Epoch 191, Batch_no 116, 92% \t train_loss: 0.39 took: 2.63s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 192, Batch_no 12, 10% \t train_loss: 0.45 took: 2.92s\n",
            "Epoch 192, Batch_no 25, 20% \t train_loss: 0.50 took: 2.69s\n",
            "Epoch 192, Batch_no 38, 30% \t train_loss: 0.44 took: 2.43s\n",
            "Epoch 192, Batch_no 51, 41% \t train_loss: 0.45 took: 2.25s\n",
            "Epoch 192, Batch_no 64, 51% \t train_loss: 0.44 took: 2.24s\n",
            "Epoch 192, Batch_no 77, 61% \t train_loss: 0.48 took: 2.99s\n",
            "Epoch 192, Batch_no 90, 72% \t train_loss: 0.44 took: 2.02s\n",
            "Epoch 192, Batch_no 103, 82% \t train_loss: 0.38 took: 2.09s\n",
            "Epoch 192, Batch_no 116, 92% \t train_loss: 0.39 took: 2.41s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 89 %\n",
            "Validation loss = 0.26\n",
            "Epoch 193, Batch_no 12, 10% \t train_loss: 0.41 took: 3.18s\n",
            "Epoch 193, Batch_no 25, 20% \t train_loss: 0.46 took: 2.52s\n",
            "Epoch 193, Batch_no 38, 30% \t train_loss: 0.37 took: 2.36s\n",
            "Epoch 193, Batch_no 51, 41% \t train_loss: 0.44 took: 2.27s\n",
            "Epoch 193, Batch_no 64, 51% \t train_loss: 0.43 took: 2.91s\n",
            "Epoch 193, Batch_no 77, 61% \t train_loss: 0.41 took: 1.92s\n",
            "Epoch 193, Batch_no 90, 72% \t train_loss: 0.38 took: 2.08s\n",
            "Epoch 193, Batch_no 103, 82% \t train_loss: 0.39 took: 2.42s\n",
            "Epoch 193, Batch_no 116, 92% \t train_loss: 0.47 took: 3.16s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 194, Batch_no 12, 10% \t train_loss: 0.44 took: 3.24s\n",
            "Epoch 194, Batch_no 25, 20% \t train_loss: 0.54 took: 2.44s\n",
            "Epoch 194, Batch_no 38, 30% \t train_loss: 0.48 took: 2.05s\n",
            "Epoch 194, Batch_no 51, 41% \t train_loss: 0.39 took: 2.23s\n",
            "Epoch 194, Batch_no 64, 51% \t train_loss: 0.44 took: 2.36s\n",
            "Epoch 194, Batch_no 77, 61% \t train_loss: 0.45 took: 2.36s\n",
            "Epoch 194, Batch_no 90, 72% \t train_loss: 0.45 took: 2.53s\n",
            "Epoch 194, Batch_no 103, 82% \t train_loss: 0.34 took: 2.52s\n",
            "Epoch 194, Batch_no 116, 92% \t train_loss: 0.41 took: 2.46s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.30\n",
            "Epoch 195, Batch_no 12, 10% \t train_loss: 0.44 took: 2.92s\n",
            "Epoch 195, Batch_no 25, 20% \t train_loss: 0.58 took: 2.57s\n",
            "Epoch 195, Batch_no 38, 30% \t train_loss: 0.53 took: 2.52s\n",
            "Epoch 195, Batch_no 51, 41% \t train_loss: 0.54 took: 2.28s\n",
            "Epoch 195, Batch_no 64, 51% \t train_loss: 0.37 took: 2.59s\n",
            "Epoch 195, Batch_no 77, 61% \t train_loss: 0.49 took: 2.18s\n",
            "Epoch 195, Batch_no 90, 72% \t train_loss: 0.43 took: 2.73s\n",
            "Epoch 195, Batch_no 103, 82% \t train_loss: 0.50 took: 2.32s\n",
            "Epoch 195, Batch_no 116, 92% \t train_loss: 0.50 took: 2.08s\n",
            "Training accuracy: 81 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 196, Batch_no 12, 10% \t train_loss: 0.45 took: 3.34s\n",
            "Epoch 196, Batch_no 25, 20% \t train_loss: 0.36 took: 2.40s\n",
            "Epoch 196, Batch_no 38, 30% \t train_loss: 0.56 took: 2.38s\n",
            "Epoch 196, Batch_no 51, 41% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 196, Batch_no 64, 51% \t train_loss: 0.49 took: 2.36s\n",
            "Epoch 196, Batch_no 77, 61% \t train_loss: 0.53 took: 1.99s\n",
            "Epoch 196, Batch_no 90, 72% \t train_loss: 0.46 took: 3.32s\n",
            "Epoch 196, Batch_no 103, 82% \t train_loss: 0.43 took: 2.04s\n",
            "Epoch 196, Batch_no 116, 92% \t train_loss: 0.41 took: 1.97s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.26\n",
            "Epoch 197, Batch_no 12, 10% \t train_loss: 0.46 took: 2.91s\n",
            "Epoch 197, Batch_no 25, 20% \t train_loss: 0.53 took: 2.95s\n",
            "Epoch 197, Batch_no 38, 30% \t train_loss: 0.51 took: 2.15s\n",
            "Epoch 197, Batch_no 51, 41% \t train_loss: 0.45 took: 1.99s\n",
            "Epoch 197, Batch_no 64, 51% \t train_loss: 0.40 took: 2.25s\n",
            "Epoch 197, Batch_no 77, 61% \t train_loss: 0.46 took: 2.52s\n",
            "Epoch 197, Batch_no 90, 72% \t train_loss: 0.41 took: 2.60s\n",
            "Epoch 197, Batch_no 103, 82% \t train_loss: 0.43 took: 2.87s\n",
            "Epoch 197, Batch_no 116, 92% \t train_loss: 0.51 took: 2.11s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 198, Batch_no 12, 10% \t train_loss: 0.47 took: 3.63s\n",
            "Epoch 198, Batch_no 25, 20% \t train_loss: 0.40 took: 2.34s\n",
            "Epoch 198, Batch_no 38, 30% \t train_loss: 0.49 took: 2.23s\n",
            "Epoch 198, Batch_no 51, 41% \t train_loss: 0.47 took: 2.06s\n",
            "Epoch 198, Batch_no 64, 51% \t train_loss: 0.55 took: 2.65s\n",
            "Epoch 198, Batch_no 77, 61% \t train_loss: 0.56 took: 2.60s\n",
            "Epoch 198, Batch_no 90, 72% \t train_loss: 0.40 took: 2.09s\n",
            "Epoch 198, Batch_no 103, 82% \t train_loss: 0.53 took: 2.22s\n",
            "Epoch 198, Batch_no 116, 92% \t train_loss: 0.41 took: 2.31s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 199, Batch_no 12, 10% \t train_loss: 0.54 took: 2.91s\n",
            "Epoch 199, Batch_no 25, 20% \t train_loss: 0.43 took: 2.33s\n",
            "Epoch 199, Batch_no 38, 30% \t train_loss: 0.36 took: 2.50s\n",
            "Epoch 199, Batch_no 51, 41% \t train_loss: 0.36 took: 2.21s\n",
            "Epoch 199, Batch_no 64, 51% \t train_loss: 0.46 took: 2.31s\n",
            "Epoch 199, Batch_no 77, 61% \t train_loss: 0.56 took: 2.96s\n",
            "Epoch 199, Batch_no 90, 72% \t train_loss: 0.45 took: 2.41s\n",
            "Epoch 199, Batch_no 103, 82% \t train_loss: 0.43 took: 2.30s\n",
            "Epoch 199, Batch_no 116, 92% \t train_loss: 0.46 took: 2.44s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.28\n",
            "Epoch 200, Batch_no 12, 10% \t train_loss: 0.47 took: 3.02s\n",
            "Epoch 200, Batch_no 25, 20% \t train_loss: 0.49 took: 2.22s\n",
            "Epoch 200, Batch_no 38, 30% \t train_loss: 0.47 took: 2.14s\n",
            "Epoch 200, Batch_no 51, 41% \t train_loss: 0.52 took: 2.54s\n",
            "Epoch 200, Batch_no 64, 51% \t train_loss: 0.47 took: 2.41s\n",
            "Epoch 200, Batch_no 77, 61% \t train_loss: 0.47 took: 2.77s\n",
            "Epoch 200, Batch_no 90, 72% \t train_loss: 0.38 took: 2.63s\n",
            "Epoch 200, Batch_no 103, 82% \t train_loss: 0.51 took: 2.14s\n",
            "Epoch 200, Batch_no 116, 92% \t train_loss: 0.50 took: 2.16s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.26\n",
            "Epoch 201, Batch_no 12, 10% \t train_loss: 0.47 took: 3.47s\n",
            "Epoch 201, Batch_no 25, 20% \t train_loss: 0.46 took: 2.03s\n",
            "Epoch 201, Batch_no 38, 30% \t train_loss: 0.40 took: 2.38s\n",
            "Epoch 201, Batch_no 51, 41% \t train_loss: 0.39 took: 2.12s\n",
            "Epoch 201, Batch_no 64, 51% \t train_loss: 0.45 took: 3.18s\n",
            "Epoch 201, Batch_no 77, 61% \t train_loss: 0.44 took: 1.93s\n",
            "Epoch 201, Batch_no 90, 72% \t train_loss: 0.45 took: 2.35s\n",
            "Epoch 201, Batch_no 103, 82% \t train_loss: 0.40 took: 2.28s\n",
            "Epoch 201, Batch_no 116, 92% \t train_loss: 0.46 took: 2.86s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 202, Batch_no 12, 10% \t train_loss: 0.52 took: 3.30s\n",
            "Epoch 202, Batch_no 25, 20% \t train_loss: 0.43 took: 2.58s\n",
            "Epoch 202, Batch_no 38, 30% \t train_loss: 0.51 took: 2.16s\n",
            "Epoch 202, Batch_no 51, 41% \t train_loss: 0.46 took: 1.91s\n",
            "Epoch 202, Batch_no 64, 51% \t train_loss: 0.39 took: 2.36s\n",
            "Epoch 202, Batch_no 77, 61% \t train_loss: 0.47 took: 2.81s\n",
            "Epoch 202, Batch_no 90, 72% \t train_loss: 0.42 took: 2.30s\n",
            "Epoch 202, Batch_no 103, 82% \t train_loss: 0.51 took: 2.18s\n",
            "Epoch 202, Batch_no 116, 92% \t train_loss: 0.43 took: 2.41s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.29\n",
            "Epoch 203, Batch_no 12, 10% \t train_loss: 0.50 took: 3.16s\n",
            "Epoch 203, Batch_no 25, 20% \t train_loss: 0.55 took: 2.36s\n",
            "Epoch 203, Batch_no 38, 30% \t train_loss: 0.37 took: 2.29s\n",
            "Epoch 203, Batch_no 51, 41% \t train_loss: 0.44 took: 2.41s\n",
            "Epoch 203, Batch_no 64, 51% \t train_loss: 0.46 took: 2.08s\n",
            "Epoch 203, Batch_no 77, 61% \t train_loss: 0.42 took: 2.70s\n",
            "Epoch 203, Batch_no 90, 72% \t train_loss: 0.36 took: 2.37s\n",
            "Epoch 203, Batch_no 103, 82% \t train_loss: 0.43 took: 2.22s\n",
            "Epoch 203, Batch_no 116, 92% \t train_loss: 0.41 took: 2.47s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 87 %\n",
            "Validation loss = 0.31\n",
            "Epoch 204, Batch_no 12, 10% \t train_loss: 0.41 took: 3.69s\n",
            "Epoch 204, Batch_no 25, 20% \t train_loss: 0.45 took: 2.20s\n",
            "Epoch 204, Batch_no 38, 30% \t train_loss: 0.64 took: 2.16s\n",
            "Epoch 204, Batch_no 51, 41% \t train_loss: 0.45 took: 2.42s\n",
            "Epoch 204, Batch_no 64, 51% \t train_loss: 0.42 took: 2.79s\n",
            "Epoch 204, Batch_no 77, 61% \t train_loss: 0.47 took: 2.43s\n",
            "Epoch 204, Batch_no 90, 72% \t train_loss: 0.43 took: 2.28s\n",
            "Epoch 204, Batch_no 103, 82% \t train_loss: 0.38 took: 2.29s\n",
            "Epoch 204, Batch_no 116, 92% \t train_loss: 0.54 took: 2.73s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 205, Batch_no 12, 10% \t train_loss: 0.49 took: 2.85s\n",
            "Epoch 205, Batch_no 25, 20% \t train_loss: 0.44 took: 2.45s\n",
            "Epoch 205, Batch_no 38, 30% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 205, Batch_no 51, 41% \t train_loss: 0.42 took: 2.39s\n",
            "Epoch 205, Batch_no 64, 51% \t train_loss: 0.46 took: 2.26s\n",
            "Epoch 205, Batch_no 77, 61% \t train_loss: 0.51 took: 2.44s\n",
            "Epoch 205, Batch_no 90, 72% \t train_loss: 0.40 took: 2.39s\n",
            "Epoch 205, Batch_no 103, 82% \t train_loss: 0.42 took: 2.91s\n",
            "Epoch 205, Batch_no 116, 92% \t train_loss: 0.38 took: 2.32s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 206, Batch_no 12, 10% \t train_loss: 0.46 took: 2.95s\n",
            "Epoch 206, Batch_no 25, 20% \t train_loss: 0.54 took: 2.74s\n",
            "Epoch 206, Batch_no 38, 30% \t train_loss: 0.45 took: 1.97s\n",
            "Epoch 206, Batch_no 51, 41% \t train_loss: 0.48 took: 2.59s\n",
            "Epoch 206, Batch_no 64, 51% \t train_loss: 0.49 took: 2.19s\n",
            "Epoch 206, Batch_no 77, 61% \t train_loss: 0.40 took: 2.80s\n",
            "Epoch 206, Batch_no 90, 72% \t train_loss: 0.42 took: 2.35s\n",
            "Epoch 206, Batch_no 103, 82% \t train_loss: 0.43 took: 2.46s\n",
            "Epoch 206, Batch_no 116, 92% \t train_loss: 0.43 took: 2.19s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 207, Batch_no 12, 10% \t train_loss: 0.45 took: 3.24s\n",
            "Epoch 207, Batch_no 25, 20% \t train_loss: 0.41 took: 2.32s\n",
            "Epoch 207, Batch_no 38, 30% \t train_loss: 0.48 took: 2.47s\n",
            "Epoch 207, Batch_no 51, 41% \t train_loss: 0.35 took: 2.14s\n",
            "Epoch 207, Batch_no 64, 51% \t train_loss: 0.42 took: 2.86s\n",
            "Epoch 207, Batch_no 77, 61% \t train_loss: 0.47 took: 2.44s\n",
            "Epoch 207, Batch_no 90, 72% \t train_loss: 0.51 took: 1.99s\n",
            "Epoch 207, Batch_no 103, 82% \t train_loss: 0.55 took: 2.43s\n",
            "Epoch 207, Batch_no 116, 92% \t train_loss: 0.51 took: 2.79s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.29\n",
            "Epoch 208, Batch_no 12, 10% \t train_loss: 0.46 took: 3.27s\n",
            "Epoch 208, Batch_no 25, 20% \t train_loss: 0.43 took: 2.35s\n",
            "Epoch 208, Batch_no 38, 30% \t train_loss: 0.44 took: 2.36s\n",
            "Epoch 208, Batch_no 51, 41% \t train_loss: 0.43 took: 2.16s\n",
            "Epoch 208, Batch_no 64, 51% \t train_loss: 0.45 took: 2.21s\n",
            "Epoch 208, Batch_no 77, 61% \t train_loss: 0.38 took: 2.69s\n",
            "Epoch 208, Batch_no 90, 72% \t train_loss: 0.47 took: 2.30s\n",
            "Epoch 208, Batch_no 103, 82% \t train_loss: 0.37 took: 2.09s\n",
            "Epoch 208, Batch_no 116, 92% \t train_loss: 0.46 took: 2.65s\n",
            "Training accuracy: 83 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.27\n",
            "Epoch 209, Batch_no 12, 10% \t train_loss: 0.50 took: 3.40s\n",
            "Epoch 209, Batch_no 25, 20% \t train_loss: 0.51 took: 2.45s\n",
            "Epoch 209, Batch_no 38, 30% \t train_loss: 0.38 took: 2.01s\n",
            "Epoch 209, Batch_no 51, 41% \t train_loss: 0.44 took: 2.44s\n",
            "Epoch 209, Batch_no 64, 51% \t train_loss: 0.42 took: 2.25s\n",
            "Epoch 209, Batch_no 77, 61% \t train_loss: 0.39 took: 2.50s\n",
            "Epoch 209, Batch_no 90, 72% \t train_loss: 0.42 took: 2.16s\n",
            "Epoch 209, Batch_no 103, 82% \t train_loss: 0.48 took: 2.80s\n",
            "Epoch 209, Batch_no 116, 92% \t train_loss: 0.43 took: 1.97s\n",
            "Training accuracy: 84 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Epoch 210, Batch_no 12, 10% \t train_loss: 0.47 took: 2.78s\n",
            "Epoch 210, Batch_no 25, 20% \t train_loss: 0.46 took: 2.56s\n",
            "Epoch 210, Batch_no 38, 30% \t train_loss: 0.45 took: 3.12s\n",
            "Epoch 210, Batch_no 51, 41% \t train_loss: 0.35 took: 2.09s\n",
            "Epoch 210, Batch_no 64, 51% \t train_loss: 0.47 took: 2.18s\n",
            "Epoch 210, Batch_no 77, 61% \t train_loss: 0.42 took: 2.27s\n",
            "Epoch 210, Batch_no 90, 72% \t train_loss: 0.49 took: 2.86s\n",
            "Epoch 210, Batch_no 103, 82% \t train_loss: 0.50 took: 2.56s\n",
            "Epoch 210, Batch_no 116, 92% \t train_loss: 0.56 took: 2.12s\n",
            "Training accuracy: 82 %\n",
            "Validation accuracy: 88 %\n",
            "Validation loss = 0.28\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Confusion matrix:\n",
            "tensor([[155.,   3.,   0.,   4.,   1.],\n",
            "        [ 30., 125.,   1.,   5.,   4.],\n",
            "        [  0.,   1.,  53.,   0.,   1.],\n",
            "        [  4.,   1.,   0., 151.,   2.],\n",
            "        [  5.,   6.,   0.,  11., 111.]])\n",
            "Training finished, took 6066.07s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLuAU433FfKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model to memory\n",
        "restore_point = torch.load('./drive/My Drive/Papsmear_dataset/cpu_simple_testnet_normalized.pth')\n",
        "fresh_net = simple_testnet(num_classes=5)\n",
        "fresh_net.load_state_dict(restore_point)\n",
        "fresh_net.eval()\n",
        "\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(fresh_net, example)\n",
        "traced_script_module.save('./drive/My Drive/Papsmear_dataset/cpu_simple_testnet_normalized_loaded.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ-4p0o3Ze1a",
        "colab_type": "code",
        "outputId": "8c783d90-e160-43ac-f502-907a9e09ab3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_list, label='Training loss')\n",
        "plt.plot(test_list, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe9da4696d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 372,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeJDZN1rs71J",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9T5kCwBhjz",
        "colab_type": "code",
        "outputId": "4f08b256-64d9-4b75-affc-744a40da81de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "network.to(device)\n",
        "summary(network, batch_size=1, input_size=(3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2          [1, 16, 112, 112]              32\n",
            "             ReLU6-3          [1, 16, 112, 112]               0\n",
            "         h_sigmoid-4          [1, 16, 112, 112]               0\n",
            "           h_swish-5          [1, 16, 112, 112]               0\n",
            "            Conv2d-6          [1, 16, 112, 112]             144\n",
            "       BatchNorm2d-7          [1, 16, 112, 112]              32\n",
            "              ReLU-8          [1, 16, 112, 112]               0\n",
            "          Identity-9          [1, 16, 112, 112]               0\n",
            "           Conv2d-10          [1, 16, 112, 112]             256\n",
            "      BatchNorm2d-11          [1, 16, 112, 112]              32\n",
            " InvertedResidual-12          [1, 16, 112, 112]               0\n",
            "           Conv2d-13          [1, 64, 112, 112]           1,024\n",
            "      BatchNorm2d-14          [1, 64, 112, 112]             128\n",
            "             ReLU-15          [1, 64, 112, 112]               0\n",
            "           Conv2d-16            [1, 64, 56, 56]             576\n",
            "      BatchNorm2d-17            [1, 64, 56, 56]             128\n",
            "         Identity-18            [1, 64, 56, 56]               0\n",
            "             ReLU-19            [1, 64, 56, 56]               0\n",
            "           Conv2d-20            [1, 24, 56, 56]           1,536\n",
            "      BatchNorm2d-21            [1, 24, 56, 56]              48\n",
            " InvertedResidual-22            [1, 24, 56, 56]               0\n",
            "           Conv2d-23            [1, 72, 56, 56]           1,728\n",
            "      BatchNorm2d-24            [1, 72, 56, 56]             144\n",
            "             ReLU-25            [1, 72, 56, 56]               0\n",
            "           Conv2d-26            [1, 72, 56, 56]             648\n",
            "      BatchNorm2d-27            [1, 72, 56, 56]             144\n",
            "         Identity-28            [1, 72, 56, 56]               0\n",
            "             ReLU-29            [1, 72, 56, 56]               0\n",
            "           Conv2d-30            [1, 24, 56, 56]           1,728\n",
            "      BatchNorm2d-31            [1, 24, 56, 56]              48\n",
            " InvertedResidual-32            [1, 24, 56, 56]               0\n",
            "           Conv2d-33            [1, 72, 56, 56]           1,728\n",
            "      BatchNorm2d-34            [1, 72, 56, 56]             144\n",
            "             ReLU-35            [1, 72, 56, 56]               0\n",
            "           Conv2d-36            [1, 72, 28, 28]           1,800\n",
            "      BatchNorm2d-37            [1, 72, 28, 28]             144\n",
            "AdaptiveAvgPool2d-38              [1, 72, 1, 1]               0\n",
            "           Linear-39                    [1, 24]           1,752\n",
            "             ReLU-40                    [1, 24]               0\n",
            "           Linear-41                    [1, 72]           1,800\n",
            "            ReLU6-42                    [1, 72]               0\n",
            "        h_sigmoid-43                    [1, 72]               0\n",
            "          SELayer-44            [1, 72, 28, 28]               0\n",
            "             ReLU-45            [1, 72, 28, 28]               0\n",
            "           Conv2d-46            [1, 40, 28, 28]           2,880\n",
            "      BatchNorm2d-47            [1, 40, 28, 28]              80\n",
            " InvertedResidual-48            [1, 40, 28, 28]               0\n",
            "           Conv2d-49           [1, 120, 28, 28]           4,800\n",
            "      BatchNorm2d-50           [1, 120, 28, 28]             240\n",
            "             ReLU-51           [1, 120, 28, 28]               0\n",
            "           Conv2d-52           [1, 120, 28, 28]           3,000\n",
            "      BatchNorm2d-53           [1, 120, 28, 28]             240\n",
            "AdaptiveAvgPool2d-54             [1, 120, 1, 1]               0\n",
            "           Linear-55                    [1, 32]           3,872\n",
            "             ReLU-56                    [1, 32]               0\n",
            "           Linear-57                   [1, 120]           3,960\n",
            "            ReLU6-58                   [1, 120]               0\n",
            "        h_sigmoid-59                   [1, 120]               0\n",
            "          SELayer-60           [1, 120, 28, 28]               0\n",
            "             ReLU-61           [1, 120, 28, 28]               0\n",
            "           Conv2d-62            [1, 40, 28, 28]           4,800\n",
            "      BatchNorm2d-63            [1, 40, 28, 28]              80\n",
            " InvertedResidual-64            [1, 40, 28, 28]               0\n",
            "           Conv2d-65           [1, 120, 28, 28]           4,800\n",
            "      BatchNorm2d-66           [1, 120, 28, 28]             240\n",
            "             ReLU-67           [1, 120, 28, 28]               0\n",
            "           Conv2d-68           [1, 120, 28, 28]           3,000\n",
            "      BatchNorm2d-69           [1, 120, 28, 28]             240\n",
            "AdaptiveAvgPool2d-70             [1, 120, 1, 1]               0\n",
            "           Linear-71                    [1, 32]           3,872\n",
            "             ReLU-72                    [1, 32]               0\n",
            "           Linear-73                   [1, 120]           3,960\n",
            "            ReLU6-74                   [1, 120]               0\n",
            "        h_sigmoid-75                   [1, 120]               0\n",
            "          SELayer-76           [1, 120, 28, 28]               0\n",
            "             ReLU-77           [1, 120, 28, 28]               0\n",
            "           Conv2d-78            [1, 40, 28, 28]           4,800\n",
            "      BatchNorm2d-79            [1, 40, 28, 28]              80\n",
            " InvertedResidual-80            [1, 40, 28, 28]               0\n",
            "           Conv2d-81           [1, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-82           [1, 240, 28, 28]             480\n",
            "            ReLU6-83           [1, 240, 28, 28]               0\n",
            "        h_sigmoid-84           [1, 240, 28, 28]               0\n",
            "          h_swish-85           [1, 240, 28, 28]               0\n",
            "           Conv2d-86           [1, 240, 14, 14]           2,160\n",
            "      BatchNorm2d-87           [1, 240, 14, 14]             480\n",
            "         Identity-88           [1, 240, 14, 14]               0\n",
            "            ReLU6-89           [1, 240, 14, 14]               0\n",
            "        h_sigmoid-90           [1, 240, 14, 14]               0\n",
            "          h_swish-91           [1, 240, 14, 14]               0\n",
            "           Conv2d-92            [1, 80, 14, 14]          19,200\n",
            "      BatchNorm2d-93            [1, 80, 14, 14]             160\n",
            " InvertedResidual-94            [1, 80, 14, 14]               0\n",
            "           Conv2d-95           [1, 200, 14, 14]          16,000\n",
            "      BatchNorm2d-96           [1, 200, 14, 14]             400\n",
            "            ReLU6-97           [1, 200, 14, 14]               0\n",
            "        h_sigmoid-98           [1, 200, 14, 14]               0\n",
            "          h_swish-99           [1, 200, 14, 14]               0\n",
            "          Conv2d-100           [1, 200, 14, 14]           1,800\n",
            "     BatchNorm2d-101           [1, 200, 14, 14]             400\n",
            "        Identity-102           [1, 200, 14, 14]               0\n",
            "           ReLU6-103           [1, 200, 14, 14]               0\n",
            "       h_sigmoid-104           [1, 200, 14, 14]               0\n",
            "         h_swish-105           [1, 200, 14, 14]               0\n",
            "          Conv2d-106            [1, 80, 14, 14]          16,000\n",
            "     BatchNorm2d-107            [1, 80, 14, 14]             160\n",
            "InvertedResidual-108            [1, 80, 14, 14]               0\n",
            "          Conv2d-109           [1, 184, 14, 14]          14,720\n",
            "     BatchNorm2d-110           [1, 184, 14, 14]             368\n",
            "           ReLU6-111           [1, 184, 14, 14]               0\n",
            "       h_sigmoid-112           [1, 184, 14, 14]               0\n",
            "         h_swish-113           [1, 184, 14, 14]               0\n",
            "          Conv2d-114           [1, 184, 14, 14]           1,656\n",
            "     BatchNorm2d-115           [1, 184, 14, 14]             368\n",
            "        Identity-116           [1, 184, 14, 14]               0\n",
            "           ReLU6-117           [1, 184, 14, 14]               0\n",
            "       h_sigmoid-118           [1, 184, 14, 14]               0\n",
            "         h_swish-119           [1, 184, 14, 14]               0\n",
            "          Conv2d-120            [1, 80, 14, 14]          14,720\n",
            "     BatchNorm2d-121            [1, 80, 14, 14]             160\n",
            "InvertedResidual-122            [1, 80, 14, 14]               0\n",
            "          Conv2d-123           [1, 184, 14, 14]          14,720\n",
            "     BatchNorm2d-124           [1, 184, 14, 14]             368\n",
            "           ReLU6-125           [1, 184, 14, 14]               0\n",
            "       h_sigmoid-126           [1, 184, 14, 14]               0\n",
            "         h_swish-127           [1, 184, 14, 14]               0\n",
            "          Conv2d-128           [1, 184, 14, 14]           1,656\n",
            "     BatchNorm2d-129           [1, 184, 14, 14]             368\n",
            "        Identity-130           [1, 184, 14, 14]               0\n",
            "           ReLU6-131           [1, 184, 14, 14]               0\n",
            "       h_sigmoid-132           [1, 184, 14, 14]               0\n",
            "         h_swish-133           [1, 184, 14, 14]               0\n",
            "          Conv2d-134            [1, 80, 14, 14]          14,720\n",
            "     BatchNorm2d-135            [1, 80, 14, 14]             160\n",
            "InvertedResidual-136            [1, 80, 14, 14]               0\n",
            "          Conv2d-137           [1, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-138           [1, 480, 14, 14]             960\n",
            "           ReLU6-139           [1, 480, 14, 14]               0\n",
            "       h_sigmoid-140           [1, 480, 14, 14]               0\n",
            "         h_swish-141           [1, 480, 14, 14]               0\n",
            "          Conv2d-142           [1, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-143           [1, 480, 14, 14]             960\n",
            "AdaptiveAvgPool2d-144             [1, 480, 1, 1]               0\n",
            "          Linear-145                   [1, 120]          57,720\n",
            "            ReLU-146                   [1, 120]               0\n",
            "          Linear-147                   [1, 480]          58,080\n",
            "           ReLU6-148                   [1, 480]               0\n",
            "       h_sigmoid-149                   [1, 480]               0\n",
            "         SELayer-150           [1, 480, 14, 14]               0\n",
            "           ReLU6-151           [1, 480, 14, 14]               0\n",
            "       h_sigmoid-152           [1, 480, 14, 14]               0\n",
            "         h_swish-153           [1, 480, 14, 14]               0\n",
            "          Conv2d-154           [1, 112, 14, 14]          53,760\n",
            "     BatchNorm2d-155           [1, 112, 14, 14]             224\n",
            "InvertedResidual-156           [1, 112, 14, 14]               0\n",
            "          Conv2d-157           [1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-158           [1, 672, 14, 14]           1,344\n",
            "           ReLU6-159           [1, 672, 14, 14]               0\n",
            "       h_sigmoid-160           [1, 672, 14, 14]               0\n",
            "         h_swish-161           [1, 672, 14, 14]               0\n",
            "          Conv2d-162           [1, 672, 14, 14]           6,048\n",
            "     BatchNorm2d-163           [1, 672, 14, 14]           1,344\n",
            "AdaptiveAvgPool2d-164             [1, 672, 1, 1]               0\n",
            "          Linear-165                   [1, 168]         113,064\n",
            "            ReLU-166                   [1, 168]               0\n",
            "          Linear-167                   [1, 672]         113,568\n",
            "           ReLU6-168                   [1, 672]               0\n",
            "       h_sigmoid-169                   [1, 672]               0\n",
            "         SELayer-170           [1, 672, 14, 14]               0\n",
            "           ReLU6-171           [1, 672, 14, 14]               0\n",
            "       h_sigmoid-172           [1, 672, 14, 14]               0\n",
            "         h_swish-173           [1, 672, 14, 14]               0\n",
            "          Conv2d-174           [1, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-175           [1, 112, 14, 14]             224\n",
            "InvertedResidual-176           [1, 112, 14, 14]               0\n",
            "          Conv2d-177           [1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-178           [1, 672, 14, 14]           1,344\n",
            "           ReLU6-179           [1, 672, 14, 14]               0\n",
            "       h_sigmoid-180           [1, 672, 14, 14]               0\n",
            "         h_swish-181           [1, 672, 14, 14]               0\n",
            "          Conv2d-182             [1, 672, 7, 7]          16,800\n",
            "     BatchNorm2d-183             [1, 672, 7, 7]           1,344\n",
            "AdaptiveAvgPool2d-184             [1, 672, 1, 1]               0\n",
            "          Linear-185                   [1, 168]         113,064\n",
            "            ReLU-186                   [1, 168]               0\n",
            "          Linear-187                   [1, 672]         113,568\n",
            "           ReLU6-188                   [1, 672]               0\n",
            "       h_sigmoid-189                   [1, 672]               0\n",
            "         SELayer-190             [1, 672, 7, 7]               0\n",
            "           ReLU6-191             [1, 672, 7, 7]               0\n",
            "       h_sigmoid-192             [1, 672, 7, 7]               0\n",
            "         h_swish-193             [1, 672, 7, 7]               0\n",
            "          Conv2d-194             [1, 160, 7, 7]         107,520\n",
            "     BatchNorm2d-195             [1, 160, 7, 7]             320\n",
            "InvertedResidual-196             [1, 160, 7, 7]               0\n",
            "          Conv2d-197             [1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-198             [1, 960, 7, 7]           1,920\n",
            "           ReLU6-199             [1, 960, 7, 7]               0\n",
            "       h_sigmoid-200             [1, 960, 7, 7]               0\n",
            "         h_swish-201             [1, 960, 7, 7]               0\n",
            "          Conv2d-202             [1, 960, 7, 7]          24,000\n",
            "     BatchNorm2d-203             [1, 960, 7, 7]           1,920\n",
            "AdaptiveAvgPool2d-204             [1, 960, 1, 1]               0\n",
            "          Linear-205                   [1, 240]         230,640\n",
            "            ReLU-206                   [1, 240]               0\n",
            "          Linear-207                   [1, 960]         231,360\n",
            "           ReLU6-208                   [1, 960]               0\n",
            "       h_sigmoid-209                   [1, 960]               0\n",
            "         SELayer-210             [1, 960, 7, 7]               0\n",
            "           ReLU6-211             [1, 960, 7, 7]               0\n",
            "       h_sigmoid-212             [1, 960, 7, 7]               0\n",
            "         h_swish-213             [1, 960, 7, 7]               0\n",
            "          Conv2d-214             [1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-215             [1, 160, 7, 7]             320\n",
            "InvertedResidual-216             [1, 160, 7, 7]               0\n",
            "          Conv2d-217             [1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-218             [1, 960, 7, 7]           1,920\n",
            "           ReLU6-219             [1, 960, 7, 7]               0\n",
            "       h_sigmoid-220             [1, 960, 7, 7]               0\n",
            "         h_swish-221             [1, 960, 7, 7]               0\n",
            "          Conv2d-222             [1, 960, 7, 7]          24,000\n",
            "     BatchNorm2d-223             [1, 960, 7, 7]           1,920\n",
            "AdaptiveAvgPool2d-224             [1, 960, 1, 1]               0\n",
            "          Linear-225                   [1, 240]         230,640\n",
            "            ReLU-226                   [1, 240]               0\n",
            "          Linear-227                   [1, 960]         231,360\n",
            "           ReLU6-228                   [1, 960]               0\n",
            "       h_sigmoid-229                   [1, 960]               0\n",
            "         SELayer-230             [1, 960, 7, 7]               0\n",
            "           ReLU6-231             [1, 960, 7, 7]               0\n",
            "       h_sigmoid-232             [1, 960, 7, 7]               0\n",
            "         h_swish-233             [1, 960, 7, 7]               0\n",
            "          Conv2d-234             [1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-235             [1, 160, 7, 7]             320\n",
            "InvertedResidual-236             [1, 160, 7, 7]               0\n",
            "          Conv2d-237             [1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-238             [1, 960, 7, 7]           1,920\n",
            "           ReLU6-239             [1, 960, 7, 7]               0\n",
            "       h_sigmoid-240             [1, 960, 7, 7]               0\n",
            "         h_swish-241             [1, 960, 7, 7]               0\n",
            "AdaptiveAvgPool2d-242             [1, 960, 1, 1]               0\n",
            "          Linear-243                  [1, 1280]       1,230,080\n",
            "           ReLU6-244                  [1, 1280]               0\n",
            "       h_sigmoid-245                  [1, 1280]               0\n",
            "         h_swish-246                  [1, 1280]               0\n",
            "         Dropout-247                  [1, 1280]               0\n",
            "          Linear-248                     [1, 5]           6,405\n",
            "================================================================\n",
            "Total params: 4,208,437\n",
            "Trainable params: 4,208,437\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 134.49\n",
            "Params size (MB): 16.05\n",
            "Estimated Total Size (MB): 151.12\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}